<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 9 Basic statistics with R | Applied Biological Data Analysis: Statistics and R for Biologists</title>
  <meta name="description" content="Helping biologists to become more informed users of R and statistical methods." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 9 Basic statistics with R | Applied Biological Data Analysis: Statistics and R for Biologists" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://greenquanteco.github.io/book/lab_logo_02.jpg" />
  <meta property="og:description" content="Helping biologists to become more informed users of R and statistical methods." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 9 Basic statistics with R | Applied Biological Data Analysis: Statistics and R for Biologists" />
  
  <meta name="twitter:description" content="Helping biologists to become more informed users of R and statistical methods." />
  <meta name="twitter:image" content="https://greenquanteco.github.io/book/lab_logo_02.jpg" />

<meta name="author" content="Dr. Nick Green, Kennesaw State University" />


<meta name="date" content="2024-08-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="fav.ico" type="image/x-icon" />
<link rel="prev" href="mod-08.html"/>
<link rel="next" href="mod-10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied biological data analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#work-in-progress-warning"><i class="fa fa-check"></i>Work in progress warning!</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license-and-permissions"><i class="fa fa-check"></i>License and permissions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objectives"><i class="fa fa-check"></i>Course objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-requirements"><i class="fa fa-check"></i>Course requirements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recommended-reading"><i class="fa fa-check"></i>Recommended reading</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-organization"><i class="fa fa-check"></i>Course organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mod-01.html"><a href="mod-01.html"><i class="fa fa-check"></i><b>1</b> Statistics in modern biology</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-01.html"><a href="mod-01.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="mod-01.html"><a href="mod-01.html#statistics-in-modern-biology"><i class="fa fa-check"></i><b>1.2</b> Statistics in modern biology</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mod-01.html"><a href="mod-01.html#the-scientific-method"><i class="fa fa-check"></i><b>1.2.1</b> The scientific method</a></li>
<li class="chapter" data-level="1.2.2" data-path="mod-01.html"><a href="mod-01.html#example-data-analysis"><i class="fa fa-check"></i><b>1.2.2</b> Example data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="mod-01.html"><a href="mod-01.html#misuses-of-statistics"><i class="fa fa-check"></i><b>1.3</b> Misuses of statistics</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="mod-01.html"><a href="mod-01.html#proving-the-trivial-and-meaningless-hypotheses"><i class="fa fa-check"></i><b>1.3.1</b> “Proving” the trivial and meaningless hypotheses</a></li>
<li class="chapter" data-level="1.3.2" data-path="mod-01.html"><a href="mod-01.html#inappropriate-methods"><i class="fa fa-check"></i><b>1.3.2</b> Inappropriate methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="mod-01.html"><a href="mod-01.html#p-hacking-and-data-dredging"><i class="fa fa-check"></i><b>1.3.3</b> <em>P</em>-hacking and data dredging</a></li>
<li class="chapter" data-level="1.3.4" data-path="mod-01.html"><a href="mod-01.html#inadequate-sample-sizes-and-pseudoreplication"><i class="fa fa-check"></i><b>1.3.4</b> Inadequate sample sizes and pseudoreplication</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="mod-01.html"><a href="mod-01.html#mod-01-nhst"><i class="fa fa-check"></i><b>1.4</b> <em>P</em>-values and null hypothesis significance testing (NHST)</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="mod-01.html"><a href="mod-01.html#definition"><i class="fa fa-check"></i><b>1.4.1</b> Definition</a></li>
<li class="chapter" data-level="1.4.2" data-path="mod-01.html"><a href="mod-01.html#aside-another-way-to-frame-null-hypotheses"><i class="fa fa-check"></i><b>1.4.2</b> Aside: Another way to frame null hypotheses</a></li>
<li class="chapter" data-level="1.4.3" data-path="mod-01.html"><a href="mod-01.html#history-and-status-of-p-values"><i class="fa fa-check"></i><b>1.4.3</b> History and status of <em>P</em>-values</a></li>
<li class="chapter" data-level="1.4.4" data-path="mod-01.html"><a href="mod-01.html#where-p-values-come-from"><i class="fa fa-check"></i><b>1.4.4</b> Where <em>P</em>-values come from</a></li>
<li class="chapter" data-level="1.4.5" data-path="mod-01.html"><a href="mod-01.html#what-p-values-mean-and-do-not-mean"><i class="fa fa-check"></i><b>1.4.5</b> What <em>P</em>-values mean and do not mean</a></li>
<li class="chapter" data-level="1.4.6" data-path="mod-01.html"><a href="mod-01.html#do-you-need-a-p-value"><i class="fa fa-check"></i><b>1.4.6</b> Do you need a <em>P</em>-value?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="mod-01.html"><a href="mod-01.html#alternatives-to-nhst"><i class="fa fa-check"></i><b>1.5</b> Alternatives to NHST</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="mod-01.html"><a href="mod-01.html#mod-01-bayes"><i class="fa fa-check"></i><b>1.5.1</b> Bayesian inference</a></li>
<li class="chapter" data-level="1.5.2" data-path="mod-01.html"><a href="mod-01.html#information-theoretic-methods"><i class="fa fa-check"></i><b>1.5.2</b> Information-theoretic methods</a></li>
<li class="chapter" data-level="1.5.3" data-path="mod-01.html"><a href="mod-01.html#machine-learning"><i class="fa fa-check"></i><b>1.5.3</b> Machine learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mod-02.html"><a href="mod-02.html"><i class="fa fa-check"></i><b>2</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-02.html"><a href="mod-02.html#getting-started-with-r"><i class="fa fa-check"></i><b>2.1</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="mod-02.html"><a href="mod-02.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="mod-02.html"><a href="mod-02.html#advantages-of-r"><i class="fa fa-check"></i><b>2.1.2</b> Advantages of R</a></li>
<li class="chapter" data-level="2.1.3" data-path="mod-02.html"><a href="mod-02.html#disadvantages-of-r"><i class="fa fa-check"></i><b>2.1.3</b> Disadvantages of R</a></li>
<li class="chapter" data-level="2.1.4" data-path="mod-02.html"><a href="mod-02.html#base-r-and-vs.-tidyverse"><i class="fa fa-check"></i><b>2.1.4</b> Base R and (vs.?) <code>tidyverse</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="mod-02.html"><a href="mod-02.html#download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>2.2</b> Download and install R (and RStudio)</a></li>
<li class="chapter" data-level="2.3" data-path="mod-02.html"><a href="mod-02.html#using-r"><i class="fa fa-check"></i><b>2.3</b> Using R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="mod-02.html"><a href="mod-02.html#using-the-base-r-gui"><i class="fa fa-check"></i><b>2.3.1</b> Using the base R GUI</a></li>
<li class="chapter" data-level="2.3.2" data-path="mod-02.html"><a href="mod-02.html#using-r-in-rstudio"><i class="fa fa-check"></i><b>2.3.2</b> Using R in RStudio</a></li>
<li class="chapter" data-level="2.3.3" data-path="mod-02.html"><a href="mod-02.html#using-r-with-other-programs"><i class="fa fa-check"></i><b>2.3.3</b> Using R with other programs</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="mod-02.html"><a href="mod-02.html#mod-02-first"><i class="fa fa-check"></i><b>2.4</b> A first R session</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="mod-02.html"><a href="mod-02.html#import-data"><i class="fa fa-check"></i><b>2.4.1</b> Import data</a></li>
<li class="chapter" data-level="2.4.2" data-path="mod-02.html"><a href="mod-02.html#explore-and-visualize-data"><i class="fa fa-check"></i><b>2.4.2</b> Explore and visualize data</a></li>
<li class="chapter" data-level="2.4.3" data-path="mod-02.html"><a href="mod-02.html#transform-data"><i class="fa fa-check"></i><b>2.4.3</b> Transform data</a></li>
<li class="chapter" data-level="2.4.4" data-path="mod-02.html"><a href="mod-02.html#analyze-data"><i class="fa fa-check"></i><b>2.4.4</b> Analyze data</a></li>
<li class="chapter" data-level="2.4.5" data-path="mod-02.html"><a href="mod-02.html#mod-01-elkfig"><i class="fa fa-check"></i><b>2.4.5</b> Write out results</a></li>
<li class="chapter" data-level="2.4.6" data-path="mod-02.html"><a href="mod-02.html#save-your-work"><i class="fa fa-check"></i><b>2.4.6</b> Save your work?</a></li>
<li class="chapter" data-level="2.4.7" data-path="mod-02.html"><a href="mod-02.html#whats-next"><i class="fa fa-check"></i><b>2.4.7</b> What’s next?</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="mod-02.html"><a href="mod-02.html#write-and-execute-commands-in-the-r-console"><i class="fa fa-check"></i><b>2.5</b> Write and execute commands in the R console</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="mod-02.html"><a href="mod-02.html#r-commandsbasics"><i class="fa fa-check"></i><b>2.5.1</b> R commands–basics</a></li>
<li class="chapter" data-level="2.5.2" data-path="mod-02.html"><a href="mod-02.html#elements-of-r-code"><i class="fa fa-check"></i><b>2.5.2</b> Elements of R code</a></li>
<li class="chapter" data-level="2.5.3" data-path="mod-02.html"><a href="mod-02.html#the-r-workspace"><i class="fa fa-check"></i><b>2.5.3</b> The R workspace</a></li>
<li class="chapter" data-level="2.5.4" data-path="mod-02.html"><a href="mod-02.html#r-code-basics-assignment-and-operators"><i class="fa fa-check"></i><b>2.5.4</b> R code basics: assignment and operators</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="mod-02.html"><a href="mod-02.html#mod-02-struct"><i class="fa fa-check"></i><b>2.6</b> Basic R data structures</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="mod-02.html"><a href="mod-02.html#vectors"><i class="fa fa-check"></i><b>2.6.1</b> Vectors</a></li>
<li class="chapter" data-level="2.6.2" data-path="mod-02.html"><a href="mod-02.html#data-frames"><i class="fa fa-check"></i><b>2.6.2</b> Data frames</a></li>
<li class="chapter" data-level="2.6.3" data-path="mod-02.html"><a href="mod-02.html#matrices-and-arrays"><i class="fa fa-check"></i><b>2.6.3</b> Matrices and arrays</a></li>
<li class="chapter" data-level="2.6.4" data-path="mod-02.html"><a href="mod-02.html#lists"><i class="fa fa-check"></i><b>2.6.4</b> Lists</a></li>
<li class="chapter" data-level="2.6.5" data-path="mod-02.html"><a href="mod-02.html#s3-s4-and-other-paradigms"><i class="fa fa-check"></i><b>2.6.5</b> S3, S4, and other paradigms</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="mod-02.html"><a href="mod-02.html#r-data-types"><i class="fa fa-check"></i><b>2.7</b> R data types</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="mod-02.html"><a href="mod-02.html#character-type"><i class="fa fa-check"></i><b>2.7.1</b> Character type</a></li>
<li class="chapter" data-level="2.7.2" data-path="mod-02.html"><a href="mod-02.html#numeric-type"><i class="fa fa-check"></i><b>2.7.2</b> Numeric type</a></li>
<li class="chapter" data-level="2.7.3" data-path="mod-02.html"><a href="mod-02.html#integer-type"><i class="fa fa-check"></i><b>2.7.3</b> Integer type</a></li>
<li class="chapter" data-level="2.7.4" data-path="mod-02.html"><a href="mod-02.html#logical-type"><i class="fa fa-check"></i><b>2.7.4</b> Logical type</a></li>
<li class="chapter" data-level="2.7.5" data-path="mod-02.html"><a href="mod-02.html#special-values"><i class="fa fa-check"></i><b>2.7.5</b> Special values</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="mod-02.html"><a href="mod-02.html#manage-r-code-as-scripts-.r-files"><i class="fa fa-check"></i><b>2.8</b> Manage R code as scripts (.r files)</a></li>
<li class="chapter" data-level="2.9" data-path="mod-02.html"><a href="mod-02.html#manage-and-use-r-packages"><i class="fa fa-check"></i><b>2.9</b> Manage and use R packages</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="mod-02.html"><a href="mod-02.html#your-r-library"><i class="fa fa-check"></i><b>2.9.1</b> Your R library</a></li>
<li class="chapter" data-level="2.9.2" data-path="mod-02.html"><a href="mod-02.html#installing-packages-using-the-r-gui"><i class="fa fa-check"></i><b>2.9.2</b> Installing packages using the R GUI</a></li>
<li class="chapter" data-level="2.9.3" data-path="mod-02.html"><a href="mod-02.html#installing-packages-in-rstudio"><i class="fa fa-check"></i><b>2.9.3</b> Installing packages in RStudio</a></li>
<li class="chapter" data-level="2.9.4" data-path="mod-02.html"><a href="mod-02.html#installing-packages-using-the-r-console"><i class="fa fa-check"></i><b>2.9.4</b> Installing packages using the R console</a></li>
<li class="chapter" data-level="2.9.5" data-path="mod-02.html"><a href="mod-02.html#working-with-packages-in-r"><i class="fa fa-check"></i><b>2.9.5</b> Working with packages in R</a></li>
<li class="chapter" data-level="2.9.6" data-path="mod-02.html"><a href="mod-02.html#package-dependencies"><i class="fa fa-check"></i><b>2.9.6</b> Package dependencies</a></li>
<li class="chapter" data-level="2.9.7" data-path="mod-02.html"><a href="mod-02.html#citing-packages"><i class="fa fa-check"></i><b>2.9.7</b> Citing packages</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="mod-02.html"><a href="mod-02.html#r-documentation"><i class="fa fa-check"></i><b>2.10</b> R documentation</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="mod-02.html"><a href="mod-02.html#documentation-help-files"><i class="fa fa-check"></i><b>2.10.1</b> Documentation (help) files</a></li>
<li class="chapter" data-level="2.10.2" data-path="mod-02.html"><a href="mod-02.html#r-vignettes"><i class="fa fa-check"></i><b>2.10.2</b> R vignettes</a></li>
<li class="chapter" data-level="2.10.3" data-path="mod-02.html"><a href="mod-02.html#official-r-project-resources"><i class="fa fa-check"></i><b>2.10.3</b> Official R Project resources</a></li>
<li class="chapter" data-level="2.10.4" data-path="mod-02.html"><a href="mod-02.html#unofficial-online-resources"><i class="fa fa-check"></i><b>2.10.4</b> Unofficial online resources</a></li>
<li class="chapter" data-level="2.10.5" data-path="mod-02.html"><a href="mod-02.html#r-books"><i class="fa fa-check"></i><b>2.10.5</b> R books</a></li>
<li class="chapter" data-level="2.10.6" data-path="mod-02.html"><a href="mod-02.html#two-reminders"><i class="fa fa-check"></i><b>2.10.6</b> Two reminders</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-03.html"><a href="mod-03.html"><i class="fa fa-check"></i><b>3</b> Data manipulation with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-03.html"><a href="mod-03.html#mod-03-inout"><i class="fa fa-check"></i><b>3.1</b> Data import and export</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mod-03.html"><a href="mod-03.html#importing-data-preliminaries"><i class="fa fa-check"></i><b>3.1.1</b> Importing data: preliminaries</a></li>
<li class="chapter" data-level="3.1.2" data-path="mod-03.html"><a href="mod-03.html#importing-data-from-text-files-with-read.csv-and-read.table"><i class="fa fa-check"></i><b>3.1.2</b> Importing data from text files with <code>read.csv()</code> and <code>read.table()</code></a></li>
<li class="chapter" data-level="3.1.3" data-path="mod-03.html"><a href="mod-03.html#importing-data-from-saved-workspaces"><i class="fa fa-check"></i><b>3.1.3</b> Importing data from saved workspaces</a></li>
<li class="chapter" data-level="3.1.4" data-path="mod-03.html"><a href="mod-03.html#importing-data-special-cases"><i class="fa fa-check"></i><b>3.1.4</b> Importing data: special cases:</a></li>
<li class="chapter" data-level="3.1.5" data-path="mod-03.html"><a href="mod-03.html#export-data-from-r"><i class="fa fa-check"></i><b>3.1.5</b> Export data from R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mod-03.html"><a href="mod-03.html#making-values-in-r"><i class="fa fa-check"></i><b>3.2</b> Making values in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mod-03.html"><a href="mod-03.html#producing-arbitrary-values-with-c"><i class="fa fa-check"></i><b>3.2.1</b> Producing arbitrary values with <code>c()</code></a></li>
<li class="chapter" data-level="3.2.2" data-path="mod-03.html"><a href="mod-03.html#generating-regular-values"><i class="fa fa-check"></i><b>3.2.2</b> Generating regular values</a></li>
<li class="chapter" data-level="3.2.3" data-path="mod-03.html"><a href="mod-03.html#generating-random-values"><i class="fa fa-check"></i><b>3.2.3</b> Generating random values</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mod-03.html"><a href="mod-03.html#selecting-data-with"><i class="fa fa-check"></i><b>3.3</b> Selecting data with <code>[]</code></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mod-03.html"><a href="mod-03.html#mod-03-brackets"><i class="fa fa-check"></i><b>3.3.1</b> Basics of brackets</a></li>
<li class="chapter" data-level="3.3.2" data-path="mod-03.html"><a href="mod-03.html#extracting-and-selecting-data-with-logical-tests"><i class="fa fa-check"></i><b>3.3.2</b> Extracting and selecting data with logical tests</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mod-03.html"><a href="mod-03.html#managing-dates-and-characters"><i class="fa fa-check"></i><b>3.4</b> Managing dates and characters</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mod-03.html"><a href="mod-03.html#temporal-data-and-dates"><i class="fa fa-check"></i><b>3.4.1</b> Temporal data and dates</a></li>
<li class="chapter" data-level="3.4.2" data-path="mod-03.html"><a href="mod-03.html#character-data-text"><i class="fa fa-check"></i><b>3.4.2</b> Character data (text)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="mod-03.html"><a href="mod-03.html#mod-03-dataframe"><i class="fa fa-check"></i><b>3.5</b> Data frame management</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="mod-03.html"><a href="mod-03.html#data-frame-structure"><i class="fa fa-check"></i><b>3.5.1</b> Data frame structure</a></li>
<li class="chapter" data-level="3.5.2" data-path="mod-03.html"><a href="mod-03.html#common-data-frame-operations"><i class="fa fa-check"></i><b>3.5.2</b> Common data frame operations</a></li>
<li class="chapter" data-level="3.5.3" data-path="mod-03.html"><a href="mod-03.html#other-data-frame-operations"><i class="fa fa-check"></i><b>3.5.3</b> Other data frame operations</a></li>
<li class="chapter" data-level="3.5.4" data-path="mod-03.html"><a href="mod-03.html#mod-03-reshape"><i class="fa fa-check"></i><b>3.5.4</b> Reshaping data frames</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-04.html"><a href="mod-04.html"><i class="fa fa-check"></i><b>4</b> Exploratory data analysis 1: Data description and summarization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mod-04.html"><a href="mod-04.html#motivation"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="mod-04.html"><a href="mod-04.html#descriptive-and-summary-statistics"><i class="fa fa-check"></i><b>4.2</b> Descriptive and summary statistics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="mod-04.html"><a href="mod-04.html#basic-summary-statistics"><i class="fa fa-check"></i><b>4.2.1</b> Basic summary statistics</a></li>
<li class="chapter" data-level="4.2.2" data-path="mod-04.html"><a href="mod-04.html#mod-04-agg"><i class="fa fa-check"></i><b>4.2.2</b> Summarizing by groups (AKA: pivot tables)</a></li>
<li class="chapter" data-level="4.2.3" data-path="mod-04.html"><a href="mod-04.html#summarizing-data-with-the-apply-family"><i class="fa fa-check"></i><b>4.2.3</b> Summarizing data with the <code>apply()</code> family</a></li>
<li class="chapter" data-level="4.2.4" data-path="mod-04.html"><a href="mod-04.html#mod-04-tab"><i class="fa fa-check"></i><b>4.2.4</b> Tabulation (frequency tables)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="mod-04.html"><a href="mod-04.html#mod-04-vis"><i class="fa fa-check"></i><b>4.3</b> Visualizing single variables</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="mod-04.html"><a href="mod-04.html#boxplots-aka-box-and-whisker-plots"><i class="fa fa-check"></i><b>4.3.1</b> Boxplots (aka: box-and-whisker plots)</a></li>
<li class="chapter" data-level="4.3.2" data-path="mod-04.html"><a href="mod-04.html#histograms"><i class="fa fa-check"></i><b>4.3.2</b> Histograms</a></li>
<li class="chapter" data-level="4.3.3" data-path="mod-04.html"><a href="mod-04.html#mod04-pdfplots"><i class="fa fa-check"></i><b>4.3.3</b> Kernel density plots</a></li>
<li class="chapter" data-level="4.3.4" data-path="mod-04.html"><a href="mod-04.html#empirical-cumulative-density-function-ecdf-plots"><i class="fa fa-check"></i><b>4.3.4</b> Empirical cumulative density function (ECDF) plots</a></li>
<li class="chapter" data-level="4.3.5" data-path="mod-04.html"><a href="mod-04.html#quantile-quantile-qq-plots"><i class="fa fa-check"></i><b>4.3.5</b> Quantile-quantile (QQ) plots</a></li>
<li class="chapter" data-level="4.3.6" data-path="mod-04.html"><a href="mod-04.html#how-should-i-plot-my-data"><i class="fa fa-check"></i><b>4.3.6</b> How should I plot my data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mod-05.html"><a href="mod-05.html"><i class="fa fa-check"></i><b>5</b> Exploratory data analysis 2: Probability distributions for biology</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mod-05.html"><a href="mod-05.html#mod-05-dists1"><i class="fa fa-check"></i><b>5.1</b> Statistical distributions</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="mod-05.html"><a href="mod-05.html#probability-distributions-in-r"><i class="fa fa-check"></i><b>5.1.1</b> Probability distributions in R</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="mod-05.html"><a href="mod-05.html#mod-05-discrete"><i class="fa fa-check"></i><b>5.2</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="mod-05.html"><a href="mod-05.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.2.1</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="5.2.2" data-path="mod-05.html"><a href="mod-05.html#mod-05-binom"><i class="fa fa-check"></i><b>5.2.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="5.2.3" data-path="mod-05.html"><a href="mod-05.html#poisson-distribution"><i class="fa fa-check"></i><b>5.2.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="5.2.4" data-path="mod-05.html"><a href="mod-05.html#mod-05-negbin"><i class="fa fa-check"></i><b>5.2.4</b> Negative binomial distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="mod-05.html"><a href="mod-05.html#mod-05-geom"><i class="fa fa-check"></i><b>5.2.5</b> Geometric distribution</a></li>
<li class="chapter" data-level="5.2.6" data-path="mod-05.html"><a href="mod-05.html#beta-binomial-distribution"><i class="fa fa-check"></i><b>5.2.6</b> Beta-binomial distribution</a></li>
<li class="chapter" data-level="5.2.7" data-path="mod-05.html"><a href="mod-05.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.2.7</b> Multinomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mod-05.html"><a href="mod-05.html#mod-05-condists"><i class="fa fa-check"></i><b>5.3</b> Continuous distributions</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mod-05.html"><a href="mod-05.html#uniform-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Uniform distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="mod-05.html"><a href="mod-05.html#mod-05-norm"><i class="fa fa-check"></i><b>5.3.2</b> Normal distribution</a></li>
<li class="chapter" data-level="5.3.3" data-path="mod-05.html"><a href="mod-05.html#lognormal-distribution"><i class="fa fa-check"></i><b>5.3.3</b> Lognormal distribution</a></li>
<li class="chapter" data-level="5.3.4" data-path="mod-05.html"><a href="mod-05.html#mod-05-gamma"><i class="fa fa-check"></i><b>5.3.4</b> Gamma distribution</a></li>
<li class="chapter" data-level="5.3.5" data-path="mod-05.html"><a href="mod-05.html#beta-distribution"><i class="fa fa-check"></i><b>5.3.5</b> Beta distribution</a></li>
<li class="chapter" data-level="5.3.6" data-path="mod-05.html"><a href="mod-05.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.3.6</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="5.3.7" data-path="mod-05.html"><a href="mod-05.html#mod-05-expo"><i class="fa fa-check"></i><b>5.3.7</b> Exponential distribution</a></li>
<li class="chapter" data-level="5.3.8" data-path="mod-05.html"><a href="mod-05.html#triangular-distribution"><i class="fa fa-check"></i><b>5.3.8</b> Triangular distribution</a></li>
<li class="chapter" data-level="5.3.9" data-path="mod-05.html"><a href="mod-05.html#distributions-summary"><i class="fa fa-check"></i><b>5.3.9</b> Distributions summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="mod-05.html"><a href="mod-05.html#mod-05-dists2"><i class="fa fa-check"></i><b>5.4</b> Fitting and testing distributions</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mod-05.html"><a href="mod-05.html#estimating-distributional-parameters"><i class="fa fa-check"></i><b>5.4.1</b> Estimating distributional parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mod-05.html"><a href="mod-05.html#graphical-methods-for-examining-distributions"><i class="fa fa-check"></i><b>5.5</b> Graphical methods for examining distributions</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="mod-05.html"><a href="mod-05.html#comparing-distributions-with-pdf-or-pmf-plots"><i class="fa fa-check"></i><b>5.5.1</b> Comparing distributions with PDF or PMF plots</a></li>
<li class="chapter" data-level="5.5.2" data-path="mod-05.html"><a href="mod-05.html#comparing-distributions-using-cdf-and-ecdf"><i class="fa fa-check"></i><b>5.5.2</b> Comparing distributions using CDF and ECDF</a></li>
<li class="chapter" data-level="5.5.3" data-path="mod-05.html"><a href="mod-05.html#comparing-distributions-using-qq-plots"><i class="fa fa-check"></i><b>5.5.3</b> Comparing distributions using QQ plots</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="mod-05.html"><a href="mod-05.html#formal-tests-for-distributions"><i class="fa fa-check"></i><b>5.6</b> Formal tests for distributions</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="mod-05.html"><a href="mod-05.html#tests-for-normality"><i class="fa fa-check"></i><b>5.6.1</b> Tests for normality</a></li>
<li class="chapter" data-level="5.6.2" data-path="mod-05.html"><a href="mod-05.html#kolmogorov-smirnov-tests"><i class="fa fa-check"></i><b>5.6.2</b> Kolmogorov-Smirnov tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="mod-05.html"><a href="mod-05.html#mod-05-trans"><i class="fa fa-check"></i><b>5.7</b> Data transformations</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="mod-05.html"><a href="mod-05.html#why-transform"><i class="fa fa-check"></i><b>5.7.1</b> Why transform?</a></li>
<li class="chapter" data-level="5.7.2" data-path="mod-05.html"><a href="mod-05.html#transforms-vs.-link-functions"><i class="fa fa-check"></i><b>5.7.2</b> Transforms vs. link functions</a></li>
<li class="chapter" data-level="5.7.3" data-path="mod-05.html"><a href="mod-05.html#common-transformations"><i class="fa fa-check"></i><b>5.7.3</b> Common transformations</a></li>
<li class="chapter" data-level="5.7.4" data-path="mod-05.html"><a href="mod-05.html#uncommon-transformations"><i class="fa fa-check"></i><b>5.7.4</b> Uncommon transformations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mod-06.html"><a href="mod-06.html"><i class="fa fa-check"></i><b>6</b> Exploratory data analysis 3: Multivariate data exploration</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mod-06.html"><a href="mod-06.html#scatterplots-for-2-continuous-variables"><i class="fa fa-check"></i><b>6.1</b> Scatterplots for 2 continuous variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mod-06.html"><a href="mod-06.html#r-plotting-basics"><i class="fa fa-check"></i><b>6.1.1</b> R plotting basics</a></li>
<li class="chapter" data-level="6.1.2" data-path="mod-06.html"><a href="mod-06.html#graphical-parameters-and-advanced-plots-with-par"><i class="fa fa-check"></i><b>6.1.2</b> Graphical parameters and advanced plots with <code>par()</code></a></li>
<li class="chapter" data-level="6.1.3" data-path="mod-06.html"><a href="mod-06.html#really-advanced-plotting"><i class="fa fa-check"></i><b>6.1.3</b> Really advanced plotting</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mod-06.html"><a href="mod-06.html#mod-06-smat"><i class="fa fa-check"></i><b>6.2</b> Scatterplot matrices for many variables</a></li>
<li class="chapter" data-level="6.3" data-path="mod-06.html"><a href="mod-06.html#lattice-plots-for-hierarchical-data"><i class="fa fa-check"></i><b>6.3</b> Lattice plots for hierarchical data</a></li>
<li class="chapter" data-level="6.4" data-path="mod-06.html"><a href="mod-06.html#continuous-variables-by-group"><i class="fa fa-check"></i><b>6.4</b> Continuous variables by group</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mod-06.html"><a href="mod-06.html#boxplots"><i class="fa fa-check"></i><b>6.4.1</b> Boxplots</a></li>
<li class="chapter" data-level="6.4.2" data-path="mod-06.html"><a href="mod-06.html#barplots"><i class="fa fa-check"></i><b>6.4.2</b> Barplots</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mod-06.html"><a href="mod-06.html#ordination-brief-introduction"><i class="fa fa-check"></i><b>6.5</b> Ordination (brief introduction)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="mod-06.html"><a href="mod-06.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>6.5.1</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="6.5.2" data-path="mod-06.html"><a href="mod-06.html#nonmetric-multidimensional-scaling-nmds"><i class="fa fa-check"></i><b>6.5.2</b> Nonmetric multidimensional scaling (NMDS)</a></li>
<li class="chapter" data-level="6.5.3" data-path="mod-06.html"><a href="mod-06.html#plotting-ordinations"><i class="fa fa-check"></i><b>6.5.3</b> Plotting ordinations</a></li>
<li class="chapter" data-level="6.5.4" data-path="mod-06.html"><a href="mod-06.html#ordination-wrap-up-for-now"><i class="fa fa-check"></i><b>6.5.4</b> Ordination wrap-up (for now)</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="mod-06.html"><a href="mod-06.html#principles-for-good-data-graphics"><i class="fa fa-check"></i><b>6.6</b> Principles for good data graphics</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="mod-06.html"><a href="mod-06.html#maximize-the-data-to-ink-ratio"><i class="fa fa-check"></i><b>6.6.1</b> Maximize the data-to-ink ratio</a></li>
<li class="chapter" data-level="6.6.2" data-path="mod-06.html"><a href="mod-06.html#show-variability"><i class="fa fa-check"></i><b>6.6.2</b> Show variability</a></li>
<li class="chapter" data-level="6.6.3" data-path="mod-06.html"><a href="mod-06.html#use-labels-not-symbols"><i class="fa fa-check"></i><b>6.6.3</b> Use labels, not symbols</a></li>
<li class="chapter" data-level="6.6.4" data-path="mod-06.html"><a href="mod-06.html#be-consistent"><i class="fa fa-check"></i><b>6.6.4</b> Be consistent</a></li>
<li class="chapter" data-level="6.6.5" data-path="mod-06.html"><a href="mod-06.html#set-yourself-up-for-success"><i class="fa fa-check"></i><b>6.6.5</b> Set yourself up for success</a></li>
<li class="chapter" data-level="6.6.6" data-path="mod-06.html"><a href="mod-06.html#consider-legibility"><i class="fa fa-check"></i><b>6.6.6</b> Consider legibility</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mod-07.html"><a href="mod-07.html"><i class="fa fa-check"></i><b>7</b> Common statistical problems</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mod-07.html"><a href="mod-07.html#outliers-and-erroneous-values"><i class="fa fa-check"></i><b>7.1</b> Outliers and erroneous values</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="mod-07.html"><a href="mod-07.html#detecting-outliers"><i class="fa fa-check"></i><b>7.1.1</b> Detecting outliers</a></li>
<li class="chapter" data-level="7.1.2" data-path="mod-07.html"><a href="mod-07.html#dealing-with-outliers"><i class="fa fa-check"></i><b>7.1.2</b> Dealing with outliers</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mod-07.html"><a href="mod-07.html#autocorrelation"><i class="fa fa-check"></i><b>7.2</b> Autocorrelation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="mod-07.html"><a href="mod-07.html#graphical-methods-to-identify-autocorrelation"><i class="fa fa-check"></i><b>7.2.1</b> Graphical methods to identify autocorrelation</a></li>
<li class="chapter" data-level="7.2.2" data-path="mod-07.html"><a href="mod-07.html#mantel-tests-for-autocorrelation"><i class="fa fa-check"></i><b>7.2.2</b> Mantel tests for autocorrelation</a></li>
<li class="chapter" data-level="7.2.3" data-path="mod-07.html"><a href="mod-07.html#dealing-with-autocorrelation"><i class="fa fa-check"></i><b>7.2.3</b> Dealing with autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mod-07.html"><a href="mod-07.html#mod-07-multicol"><i class="fa fa-check"></i><b>7.3</b> Collinearity</a></li>
<li class="chapter" data-level="7.4" data-path="mod-07.html"><a href="mod-07.html#missing-data"><i class="fa fa-check"></i><b>7.4</b> Missing data</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mod-07.html"><a href="mod-07.html#option-1-ignore-missing-data"><i class="fa fa-check"></i><b>7.4.1</b> Option 1: Ignore missing data</a></li>
<li class="chapter" data-level="7.4.2" data-path="mod-07.html"><a href="mod-07.html#option-2-interpolate-or-impute-missing-data"><i class="fa fa-check"></i><b>7.4.2</b> Option 2: Interpolate or impute missing data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mod-08.html"><a href="mod-08.html"><i class="fa fa-check"></i><b>8</b> Planning your analysis (what test?)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mod-08.html"><a href="mod-08.html#how-to-use-this-guide"><i class="fa fa-check"></i><b>8.1</b> How to use this guide</a></li>
<li class="chapter" data-level="8.2" data-path="mod-08.html"><a href="mod-08.html#what-question-are-you-trying-to-answer"><i class="fa fa-check"></i><b>8.2</b> What question are you trying to answer?</a></li>
<li class="chapter" data-level="8.3" data-path="mod-08.html"><a href="mod-08.html#testing-for-a-difference-in-mean-or-location"><i class="fa fa-check"></i><b>8.3</b> Testing for a difference in mean or location</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="mod-08.html"><a href="mod-08.html#additional-considerations"><i class="fa fa-check"></i><b>8.3.1</b> Additional considerations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mod-08.html"><a href="mod-08.html#testing-for-a-continuous-relationship-between-two-or-more-variables"><i class="fa fa-check"></i><b>8.4</b> Testing for a continuous relationship between two or more variables</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mod-08.html"><a href="mod-08.html#additional-considerations-1"><i class="fa fa-check"></i><b>8.4.1</b> Additional considerations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mod-08.html"><a href="mod-08.html#independence-between-categories"><i class="fa fa-check"></i><b>8.5</b> Independence between categories</a></li>
<li class="chapter" data-level="8.6" data-path="mod-08.html"><a href="mod-08.html#classifying-observations"><i class="fa fa-check"></i><b>8.6</b> Classifying observations</a></li>
<li class="chapter" data-level="8.7" data-path="mod-08.html"><a href="mod-08.html#conclusions"><i class="fa fa-check"></i><b>8.7</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mod-09.html"><a href="mod-09.html"><i class="fa fa-check"></i><b>9</b> Basic statistics with R</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mod-09.html"><a href="mod-09.html#mod-09-datasets"><i class="fa fa-check"></i><b>9.1</b> Datasets and variables</a></li>
<li class="chapter" data-level="9.2" data-path="mod-09.html"><a href="mod-09.html#mod-09-frequentist"><i class="fa fa-check"></i><b>9.2</b> Frequentist statistics</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="mod-09.html"><a href="mod-09.html#mod-09-pvalues"><i class="fa fa-check"></i><b>9.2.1</b> <em>P</em>-values and NHST</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="mod-09.html"><a href="mod-09.html#mod-09-whattest"><i class="fa fa-check"></i><b>9.3</b> What kind of test do you need?</a></li>
<li class="chapter" data-level="9.4" data-path="mod-09.html"><a href="mod-09.html#mod-09-tests-for-loc"><i class="fa fa-check"></i><b>9.4</b> Tests for differences in mean or location</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="mod-09.html"><a href="mod-09.html#tests-for-1-group"><i class="fa fa-check"></i><b>9.4.1</b> Tests for 1 group</a></li>
<li class="chapter" data-level="9.4.2" data-path="mod-09.html"><a href="mod-09.html#tests-comparing-two-groups"><i class="fa fa-check"></i><b>9.4.2</b> Tests comparing two groups</a></li>
<li class="chapter" data-level="9.4.3" data-path="mod-09.html"><a href="mod-09.html#aside-whats-with-all-these-t-tests"><i class="fa fa-check"></i><b>9.4.3</b> Aside: what’s with all these <em>t</em>-tests?</a></li>
<li class="chapter" data-level="9.4.4" data-path="mod-09.html"><a href="mod-09.html#comparing-3-or-more-groups"><i class="fa fa-check"></i><b>9.4.4</b> Comparing 3 or more groups</a></li>
<li class="chapter" data-level="9.4.5" data-path="mod-09.html"><a href="mod-09.html#ordered-factors-and-polynomial-patterns-in-grouped-data"><i class="fa fa-check"></i><b>9.4.5</b> Ordered factors and polynomial patterns in grouped data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mod-09.html"><a href="mod-09.html#tests-for-patterns-in-continuous-data"><i class="fa fa-check"></i><b>9.5</b> Tests for patterns in continuous data</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="mod-09.html"><a href="mod-09.html#mod-09-correlation"><i class="fa fa-check"></i><b>9.5.1</b> Correlation</a></li>
<li class="chapter" data-level="9.5.2" data-path="mod-09.html"><a href="mod-09.html#mod-09-linear-mods"><i class="fa fa-check"></i><b>9.5.2</b> Linear models</a></li>
<li class="chapter" data-level="9.5.3" data-path="mod-09.html"><a href="mod-09.html#linear-models-wrap-up-for-now"><i class="fa fa-check"></i><b>9.5.3</b> Linear models wrap-up (for now)</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="mod-09.html"><a href="mod-09.html#mod-09-contingency"><i class="fa fa-check"></i><b>9.6</b> Tests for proportions and contingency</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="mod-09.html"><a href="mod-09.html#mod-09-ctables"><i class="fa fa-check"></i><b>9.6.1</b> Contingency tables</a></li>
<li class="chapter" data-level="9.6.2" data-path="mod-09.html"><a href="mod-09.html#tests-on-contingency-tables"><i class="fa fa-check"></i><b>9.6.2</b> Tests on contingency tables</a></li>
<li class="chapter" data-level="9.6.3" data-path="mod-09.html"><a href="mod-09.html#mod-09-oddsratios"><i class="fa fa-check"></i><b>9.6.3</b> Contingency tests and odds ratios</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="mod-09.html"><a href="mod-09.html#mod-09-classification"><i class="fa fa-check"></i><b>9.7</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mod-10.html"><a href="mod-10.html"><i class="fa fa-check"></i><b>10</b> Generalized linear models (GLM)</a>
<ul>
<li class="chapter" data-level="10.1" data-path="mod-10.html"><a href="mod-10.html#mod-10-lm"><i class="fa fa-check"></i><b>10.1</b> Prelude with linear models</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="mod-10.html"><a href="mod-10.html#assumptions-of-linear-models"><i class="fa fa-check"></i><b>10.1.1</b> Assumptions of linear models</a></li>
<li class="chapter" data-level="10.1.2" data-path="mod-10.html"><a href="mod-10.html#linear-regression-in-r"><i class="fa fa-check"></i><b>10.1.2</b> Linear regression in R</a></li>
<li class="chapter" data-level="10.1.3" data-path="mod-10.html"><a href="mod-10.html#multiple-linear-regression"><i class="fa fa-check"></i><b>10.1.3</b> Multiple linear regression</a></li>
<li class="chapter" data-level="10.1.4" data-path="mod-10.html"><a href="mod-10.html#mod-10-anova"><i class="fa fa-check"></i><b>10.1.4</b> ANOVA and ANCOVA with <code>lm()</code></a></li>
<li class="chapter" data-level="10.1.5" data-path="mod-10.html"><a href="mod-10.html#variations-on-linear-models"><i class="fa fa-check"></i><b>10.1.5</b> Variations on linear models</a></li>
<li class="chapter" data-level="10.1.6" data-path="mod-10.html"><a href="mod-10.html#example-linear-regression-workflow"><i class="fa fa-check"></i><b>10.1.6</b> Example linear regression workflow</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="mod-10.html"><a href="mod-10.html#mod-10-basic"><i class="fa fa-check"></i><b>10.2</b> GLM basics</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="mod-10.html"><a href="mod-10.html#example-glms"><i class="fa fa-check"></i><b>10.2.1</b> Example GLMS</a></li>
<li class="chapter" data-level="10.2.2" data-path="mod-10.html"><a href="mod-10.html#glm-families"><i class="fa fa-check"></i><b>10.2.2</b> GLM families</a></li>
<li class="chapter" data-level="10.2.3" data-path="mod-10.html"><a href="mod-10.html#glm-link-functions"><i class="fa fa-check"></i><b>10.2.3</b> GLM link functions</a></li>
<li class="chapter" data-level="10.2.4" data-path="mod-10.html"><a href="mod-10.html#deviance-and-other-glm-diagnostics"><i class="fa fa-check"></i><b>10.2.4</b> Deviance and other GLM diagnostics</a></li>
<li class="chapter" data-level="10.2.5" data-path="mod-10.html"><a href="mod-10.html#to-pseudo-r2-or-not-to-pseudo-r2"><i class="fa fa-check"></i><b>10.2.5</b> To pseudo-<em>R</em><sup>2</sup> or not to pseudo-<em>R</em><sup>2</sup>?</a></li>
<li class="chapter" data-level="10.2.6" data-path="mod-10.html"><a href="mod-10.html#common-glms"><i class="fa fa-check"></i><b>10.2.6</b> Common GLMs</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="mod-10.html"><a href="mod-10.html#mod-10-loglin"><i class="fa fa-check"></i><b>10.3</b> Log-linear models</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="mod-10.html"><a href="mod-10.html#mod-10-loglin-examp"><i class="fa fa-check"></i><b>10.3.1</b> Example with simulated data</a></li>
<li class="chapter" data-level="10.3.2" data-path="mod-10.html"><a href="mod-10.html#example-with-real-data"><i class="fa fa-check"></i><b>10.3.2</b> Example with real data</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="mod-10.html"><a href="mod-10.html#mod-10-poisson"><i class="fa fa-check"></i><b>10.4</b> Poisson GLM for counts</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="mod-10.html"><a href="mod-10.html#example-with-simulated-data"><i class="fa fa-check"></i><b>10.4.1</b> Example with simulated data</a></li>
<li class="chapter" data-level="10.4.2" data-path="mod-10.html"><a href="mod-10.html#example-with-real-data-1"><i class="fa fa-check"></i><b>10.4.2</b> Example with real data</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="mod-10.html"><a href="mod-10.html#mod-10-quasi"><i class="fa fa-check"></i><b>10.5</b> Quasi-Poisson and negative binomial GLM</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="mod-10.html"><a href="mod-10.html#example-with-simulated-data-1"><i class="fa fa-check"></i><b>10.5.1</b> Example with simulated data</a></li>
<li class="chapter" data-level="10.5.2" data-path="mod-10.html"><a href="mod-10.html#example-with-real-data-2"><i class="fa fa-check"></i><b>10.5.2</b> Example with real data</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="mod-10.html"><a href="mod-10.html#mod-10-logistic"><i class="fa fa-check"></i><b>10.6</b> Logistic regression for binary outcomes</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="mod-10.html"><a href="mod-10.html#example-with-simulated-data-2"><i class="fa fa-check"></i><b>10.6.1</b> Example with simulated data</a></li>
<li class="chapter" data-level="10.6.2" data-path="mod-10.html"><a href="mod-10.html#example-with-real-data-3"><i class="fa fa-check"></i><b>10.6.2</b> Example with real data</a></li>
<li class="chapter" data-level="10.6.3" data-path="mod-10.html"><a href="mod-10.html#mod-10-auc"><i class="fa fa-check"></i><b>10.6.3</b> Logistic GLM diagnostics: AUC and ROC</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="mod-10.html"><a href="mod-10.html#mod-10-binom"><i class="fa fa-check"></i><b>10.7</b> Binomial GLM for proportional data</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="mod-10.html"><a href="mod-10.html#binomial-glm"><i class="fa fa-check"></i><b>10.7.1</b> Binomial GLM</a></li>
<li class="chapter" data-level="10.7.2" data-path="mod-10.html"><a href="mod-10.html#example-with-simulated-data-3"><i class="fa fa-check"></i><b>10.7.2</b> Example with simulated data</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="mod-10.html"><a href="mod-10.html#mod-10-gamma"><i class="fa fa-check"></i><b>10.8</b> Gamma models for overdispersed data</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="mod-10.html"><a href="mod-10.html#example-with-simulated-data-4"><i class="fa fa-check"></i><b>10.8.1</b> Example with simulated data</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="mod-10.html"><a href="mod-10.html#mod-10-beyond"><i class="fa fa-check"></i><b>10.9</b> Beyond GLM: Overview of GAM and GEE</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="mod-10.html"><a href="mod-10.html#mod-10-gam"><i class="fa fa-check"></i><b>10.9.1</b> Generalized additive models (GAM)</a></li>
<li class="chapter" data-level="10.9.2" data-path="mod-10.html"><a href="mod-10.html#mod-10-gee"><i class="fa fa-check"></i><b>10.9.2</b> Generalized estimating equations (GEE)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mod-11.html"><a href="mod-11.html"><i class="fa fa-check"></i><b>11</b> Nonlinear models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mod-11.html"><a href="mod-11.html#background"><i class="fa fa-check"></i><b>11.1</b> Background</a></li>
<li class="chapter" data-level="11.2" data-path="mod-11.html"><a href="mod-11.html#mod-11-intro"><i class="fa fa-check"></i><b>11.2</b> Nonlinear least squares (NLS)</a></li>
<li class="chapter" data-level="11.3" data-path="mod-11.html"><a href="mod-11.html#mod-11-micmen"><i class="fa fa-check"></i><b>11.3</b> Michaelis-Menten curves</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="mod-11.html"><a href="mod-11.html#mod-11-micmen-sim"><i class="fa fa-check"></i><b>11.3.1</b> Example with simulated data</a></li>
<li class="chapter" data-level="11.3.2" data-path="mod-11.html"><a href="mod-11.html#example-with-real-data-4"><i class="fa fa-check"></i><b>11.3.2</b> Example with real data</a></li>
<li class="chapter" data-level="11.3.3" data-path="mod-11.html"><a href="mod-11.html#alternative-strategies-for-the-analysis"><i class="fa fa-check"></i><b>11.3.3</b> Alternative strategies for the analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="mod-11.html"><a href="mod-11.html#mod-11-grow"><i class="fa fa-check"></i><b>11.4</b> Biological growth curves</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="mod-11.html"><a href="mod-11.html#gompertz-and-von-bertalanffy-curves"><i class="fa fa-check"></i><b>11.4.1</b> Gompertz and von Bertalanffy curves</a></li>
<li class="chapter" data-level="11.4.2" data-path="mod-11.html"><a href="mod-11.html#example-with-real-data-5"><i class="fa fa-check"></i><b>11.4.2</b> Example with real data</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="mod-11.html"><a href="mod-11.html#dose-response-curves"><i class="fa fa-check"></i><b>11.5</b> Dose response curves</a></li>
<li class="chapter" data-level="11.6" data-path="mod-11.html"><a href="mod-11.html#alternatives-to-nls"><i class="fa fa-check"></i><b>11.6</b> Alternatives to NLS</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="mod-11.html"><a href="mod-11.html#generalized-nonlinear-models"><i class="fa fa-check"></i><b>11.6.1</b> Generalized nonlinear models</a></li>
<li class="chapter" data-level="11.6.2" data-path="mod-11.html"><a href="mod-11.html#quantile-regression"><i class="fa fa-check"></i><b>11.6.2</b> Quantile regression</a></li>
<li class="chapter" data-level="11.6.3" data-path="mod-11.html"><a href="mod-11.html#mod-11-gam"><i class="fa fa-check"></i><b>11.6.3</b> Generalized additive models (GAM)</a></li>
<li class="chapter" data-level="11.6.4" data-path="mod-11.html"><a href="mod-11.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>11.6.4</b> Classification and regression trees (CART)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mod-12.html"><a href="mod-12.html"><i class="fa fa-check"></i><b>12</b> Mixed models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="mod-12.html"><a href="mod-12.html#prelude-glm"><i class="fa fa-check"></i><b>12.1</b> Prelude (GLM)</a></li>
<li class="chapter" data-level="12.2" data-path="mod-12.html"><a href="mod-12.html#mod-12-lmm"><i class="fa fa-check"></i><b>12.2</b> Linear mixed models (LMM)</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="mod-12.html"><a href="mod-12.html#formal-definition-and-example"><i class="fa fa-check"></i><b>12.2.1</b> Formal definition and example</a></li>
<li class="chapter" data-level="12.2.2" data-path="mod-12.html"><a href="mod-12.html#example-with-simulated-data-5"><i class="fa fa-check"></i><b>12.2.2</b> Example with simulated data</a></li>
<li class="chapter" data-level="12.2.3" data-path="mod-12.html"><a href="mod-12.html#p-values-in-lmm"><i class="fa fa-check"></i><b>12.2.3</b> <em>P</em>-values in LMM</a></li>
<li class="chapter" data-level="12.2.4" data-path="mod-12.html"><a href="mod-12.html#specifying-random-effects"><i class="fa fa-check"></i><b>12.2.4</b> Specifying random effects</a></li>
<li class="chapter" data-level="12.2.5" data-path="mod-12.html"><a href="mod-12.html#lmm-example-with-real-data"><i class="fa fa-check"></i><b>12.2.5</b> LMM example with real data</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="mod-12.html"><a href="mod-12.html#mod-12-glmm"><i class="fa fa-check"></i><b>12.3</b> Generalized linear mixed models (GLMM)</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="mod-12.html"><a href="mod-12.html#definition-1"><i class="fa fa-check"></i><b>12.3.1</b> Definition</a></li>
<li class="chapter" data-level="12.3.2" data-path="mod-12.html"><a href="mod-12.html#glmm-on-simulated-data"><i class="fa fa-check"></i><b>12.3.2</b> GLMM on simulated data</a></li>
<li class="chapter" data-level="12.3.3" data-path="mod-12.html"><a href="mod-12.html#glmm-on-real-data"><i class="fa fa-check"></i><b>12.3.3</b> GLMM on real data</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="mod-12.html"><a href="mod-12.html#mod-12-nlme"><i class="fa fa-check"></i><b>12.4</b> Nonlinear mixed models (NLME)</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="mod-12.html"><a href="mod-12.html#definition-and-background"><i class="fa fa-check"></i><b>12.4.1</b> Definition and background</a></li>
<li class="chapter" data-level="12.4.2" data-path="mod-12.html"><a href="mod-12.html#nlme-on-simulated-data"><i class="fa fa-check"></i><b>12.4.2</b> NLME on simulated data</a></li>
<li class="chapter" data-level="12.4.3" data-path="mod-12.html"><a href="mod-12.html#nlme-on-real-data"><i class="fa fa-check"></i><b>12.4.3</b> NLME on real data</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="mod-12.html"><a href="mod-12.html#mod-12-gamm"><i class="fa fa-check"></i><b>12.5</b> (Generalized) additive mixed models (AMM/GAMM)</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="mod-12.html"><a href="mod-12.html#example-gamm-with-real-data"><i class="fa fa-check"></i><b>12.5.1</b> Example GAMM with real data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mod-13.html"><a href="mod-13.html"><i class="fa fa-check"></i><b>13</b> Multivariate data analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="mod-13.html"><a href="mod-13.html#multivariate-data"><i class="fa fa-check"></i><b>13.1</b> Multivariate data</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="mod-13.html"><a href="mod-13.html#univariate-vs.-multivariate-data"><i class="fa fa-check"></i><b>13.1.1</b> Univariate vs. multivariate data</a></li>
<li class="chapter" data-level="13.1.2" data-path="mod-13.html"><a href="mod-13.html#components-of-multivariate-data"><i class="fa fa-check"></i><b>13.1.2</b> Components of multivariate data</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="mod-13.html"><a href="mod-13.html#mod-13-dist"><i class="fa fa-check"></i><b>13.2</b> Distance metrics: biological (dis)similarity</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="mod-13.html"><a href="mod-13.html#euclidean-distance"><i class="fa fa-check"></i><b>13.2.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="13.2.2" data-path="mod-13.html"><a href="mod-13.html#bray-curtis-and-other-distance-metrics"><i class="fa fa-check"></i><b>13.2.2</b> Bray-Curtis and other distance metrics</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="mod-13.html"><a href="mod-13.html#clustering"><i class="fa fa-check"></i><b>13.3</b> Clustering</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="mod-13.html"><a href="mod-13.html#k-means-clustering"><i class="fa fa-check"></i><b>13.3.1</b> <em>K</em>-means clustering</a></li>
<li class="chapter" data-level="13.3.2" data-path="mod-13.html"><a href="mod-13.html#hierarchical-agglomerative-clustering"><i class="fa fa-check"></i><b>13.3.2</b> Hierarchical agglomerative clustering</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="mod-13.html"><a href="mod-13.html#mod-13-sims"><i class="fa fa-check"></i><b>13.4</b> Analyzing dissimilarity</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="mod-13.html"><a href="mod-13.html#mantel-tests-distance-vs.-distance"><i class="fa fa-check"></i><b>13.4.1</b> Mantel tests: distance vs. distance</a></li>
<li class="chapter" data-level="13.4.2" data-path="mod-13.html"><a href="mod-13.html#comparing-dissimilarity-between-groups"><i class="fa fa-check"></i><b>13.4.2</b> Comparing dissimilarity between groups</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="mod-13.html"><a href="mod-13.html#mod-13-ord"><i class="fa fa-check"></i><b>13.5</b> Ordination</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="mod-13.html"><a href="mod-13.html#mod-13-pca"><i class="fa fa-check"></i><b>13.5.1</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="13.5.2" data-path="mod-13.html"><a href="mod-13.html#nmds-and-other-ordination-methods"><i class="fa fa-check"></i><b>13.5.2</b> NMDS and other ordination methods</a></li>
<li class="chapter" data-level="13.5.3" data-path="mod-13.html"><a href="mod-13.html#other-ordination-techniques"><i class="fa fa-check"></i><b>13.5.3</b> Other ordination techniques</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="literature-cited.html"><a href="literature-cited.html"><i class="fa fa-check"></i>Literature Cited</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Biological Data Analysis: Statistics and R for Biologists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-09" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Module 9</span> Basic statistics with R<a href="mod-09.html#mod-09" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this module, we will review some basic statistical methods that most
people learn in an Introductory Statistics or Biostatistics course.
These tests are classics for a reason: they are powerful and
collectively can be applied to many situations. However, as we shall
see, these tests have a lot of assumptions and conditions that must be
met in order to use them…and those conditions are not always easy to
meet.</p>
<p>First, we’ll review the basics of how biological data are stored, then
what a frequentist statistical test really is–including what <em>P</em>-values
are and what they represent. Then we’ll review the classic frequentist
tests in 4 categories:</p>
<ol style="list-style-type: decimal">
<li><p>Tests for differences in mean or location.</p></li>
<li><p>Tests for patterns in continuous data.</p></li>
<li><p>Tests for proportions and contingency (aka: tests for independence)</p></li>
<li><p>Classification.</p></li>
</ol>
<p>Most biological problems fall into one of those four categories. Most
introductory statistical courses cover the first 3 categories pretty
well. Classification can be more complicated. When your problem does
not, you’ll need to move on to one of the more advanced analyses we’ll
cover later this semester.</p>
<div id="mod-09-datasets" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Datasets and variables<a href="mod-09.html#mod-09-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When biologists collect data in an experiment, they create a
<strong>dataset</strong>. This often takes the form of a table. Different software
packages may refer to this as a spreadsheet, data frame, or matrix.
Whatever you call them, a typical biological dataset usually looks like
this:</p>
<p><img src="09_01.jpg" width="414" /></p>
<ul>
<li><p>Each row of the dataset is called an <strong>observation</strong>, or sometimes
<strong>record</strong>. Each entity about which data is collected corresponds to
1 row of the dataset. For example, the dataset for a study of limb
morphology in bats might have one row for each species.</p>
<ul>
<li>If someone asks what your “sample size” or “number of samples”
is, the answer is likely the number of rows in your dataset
(minus any blanks or missing values, of course).</li>
</ul></li>
<li><p>Each column is called a <strong>variable</strong>, or sometimes a <strong>feature</strong>.
Each variable contains 1 and <u><strong><em>only</em></strong></u> 1 kind of
value. For example, a cell entry like “12.2 cm” is not legitimate,
because it contains 2 kinds of values: a measurement (12.2) and a
unit (cm). If the units need to be recorded, they should be in a
separate column.</p></li>
</ul>
<p>In R, datasets are almost always stored as objects called <strong>data
frames</strong>. Data frames have rows and columns, and function a bit like a
spreadsheet. Unlike a <strong>matrix</strong>, rows and columns are not
interchangeable: a row of a data frame is a data frame with 1 row, while
a column is really a vector (i.e., not a data frame).</p>
<p><img src="09_02.jpg" /></p>
<p>Variables in a dataset come in three varieties. Understanding these is
crucial to being able to think about the structure of your dataset and
what kind of statistical test you need.</p>
<ul>
<li><p><strong>Response:</strong> outcomes that you want to explain or predict using
other variables. AKA: dependent variables.</p></li>
<li><p><strong>Predictor:</strong> variables that are used to model, or predict,
response variables. AKA: independent or explanatory variables.</p></li>
<li><p><strong>Key:</strong> values which are used to relate 2 or more datasets to each
other. Fundamental in database design.</p></li>
</ul>
<p>In this course, every analysis will have have at least 1 response
variable and at least 1 predictor variable. Understanding the nature of
your response variable and predictor variable(s) is key to choosing an
appropriate analysis.</p>
<p>Variables can be either <strong>categorical</strong> or <strong>numeric</strong>:</p>
<table style="width:54%;">
<colgroup>
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Definition</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>C
ategorical</td>
<td>Defines
group
m
embership;
can be
coded as
numbers
(
integers),
dummy
variables,
or text</td>
<td><p>“Control”
vs. ”
treated”</p>
<p>“group 1”
vs. “group
2”</p>
<p>”
herbivore”
vs.
”
carnivore”</p></td>
</tr>
<tr class="even">
<td>Numeric</td>
<td>Numbers
(don’t
overthink
it!)</td>
<td>1, 2,
<span class="math inline">\(\pi\)</span>,
3.33, etc.</td>
</tr>
</tbody>
</table>
<p>When categorical variables are used as explanatory variables, they are
called <strong>factors</strong>. Each unique value of a factor is called a <strong>level</strong>.
For example, the factor “treatment” might have two levels, “control” and
“exposed”. Or, the factor “diet” might have 3 levels: “herbivore”,
“omnivore”, and “carnivore”.</p>
<p>Numeric variables can be <strong>continuous</strong> or <strong>discrete</strong>:</p>
<table style="width:50%;">
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>D
efinition</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>C
ontinuous</td>
<td>Can take
on any
real
value
within
its
domain,
limited
only by
the
precision
of m
e
asurement</td>
<td>1.11, 3,
4.82,
<span class="math inline">\(\pi\)</span>,
9.999,
etc.</td>
</tr>
<tr class="even">
<td>Discrete</td>
<td>Can take
on only
integer
values.
Often
r
estricted
to be n o
n
-negative
(<span class="math inline">\(\geq0\)</span>)
or even
strictly
positive
(&gt;0).</td>
<td>1, 2, 16,
42, etc.</td>
</tr>
</tbody>
</table>
<p>Some kinds of numeric variables need to be treated with care, because
they have a particular domain and set of possible values. Most of these
should probably not be modeled (i.e., treated as the response variable)
using the basic statistical methods in this module. We’ll explore models
in Module 10 for these variables.</p>
<p><strong>Type of data:</strong> counts</p>
<p><strong>Domain:</strong></p>
<p><strong>How to model:</strong></p>
<p><strong>Type of data:</strong> counts</p>
<p><strong>Domain:</strong> non-negative integers</p>
<p><strong>How to model:</strong> GLM or GLMM</p>
<p><strong>Type of data:</strong> proportions</p>
<p><strong>Domain:</strong> The closed interval [0,1], or possibly the open interval (0,
1) depending on the context.</p>
<p><strong>How to model:</strong> GLM or GLMM; maybe a <span class="math inline">\(\chi^2\)</span> tests or other test for
independence.</p>
<p><strong>Type of data:</strong> measurements (mass, length, volume, etc.)</p>
<p><strong>Domain:</strong> Positive real numbers. These values cannot be negative!</p>
<p><strong>How to model:</strong> Log-transform before analysis, or use GLM(M) with a
log link function.</p>
<p><strong>Type of data:</strong> Censored or truncated data.</p>
<p><strong>Domain:</strong> Usually real numbers with a well-defined upper or lower
limit, usually related to limits of detection or measurement.</p>
<p><strong>How to model:</strong> Special methods such as tobit models (beyond the scope
of this course…for now).</p>
<p><strong>Type of data:</strong> Binomial or multinomial outcomes.</p>
<p><strong>Domain:</strong> Outcome is one of a set of possibel outcomes (e.g., 0 or 1;
a, b, or c, etc.). Although usually coded as numbers, these cannot be
treated as numbers.</p>
<p><strong>How to model:</strong> GLM or GLMM; CART, maybe other methods outside the
scope of this course.</p>
</div>
<div id="mod-09-frequentist" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Frequentist statistics<a href="mod-09.html#mod-09-frequentist" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Classical statistics, such as those taught in introductory statistics or
biostatistics courses, are also <strong>frequentist</strong> statistics. This means
that these methods rely on probability distributions (also called
frequency distributions) of derived values called <strong>test statistics</strong>,
and make assumptions about the applicability of those test statistics.
These assumptions can be quite strong, and possibly restrictive, so you
need to be very careful when using frequentist tests.</p>
<p><strong>If your data do not meet the assumptions of a test, the test will be
invalid.</strong></p>
<p>The basic procedure of any frequentist test is as follows:</p>
<ol style="list-style-type: decimal">
<li>Collect data.</li>
<li>Calculate a summary of the effect size (aka: quantity of interest,
such as difference in means between groups), which scales the effect
size by other considerations such as sample size or variability.
This summary is the <strong>test statistic</strong>.</li>
<li>Compare the test statistic to the distribution of test statistics
expected given the sample size and assuming a true effect size of 0.
The probability of a test statistic that extreme given a true effect
size of 0 is the <strong><em>P</em>-value</strong>.</li>
<li>If the value of the test statistic is rare enough (i.e., the
<em>P</em>-value is small enough), assume that it was not that large due to
chance alone, and reject the null hypothesis of no effect. I.e.,
declare a “significant” test result.</li>
</ol>
<p>Frequentist tests vary wildly in their assumptions, the nature of their
test statistics, and so on, but the procedure above describes most of
them.</p>
<div id="mod-09-pvalues" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> <em>P</em>-values and NHST<a href="mod-09.html#mod-09-pvalues" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When scientists design experiments, they are testing one or more
<strong>hypotheses</strong>: proposed explanations for a phenomenon. Contrary to
popular belief, the statistical analysis of experimental results usually
does not test a researcher’s hypotheses directly. Instead, statistical
analysis of experimental results focuses on comparing experimental
results to the results that might have occurred if there was no effect
of the factor under investigation. This paradigm is called <strong>null
hypothesis significance testing (NHST)</strong> and, for better or worse, is
the bedrock of modern scientific inference.</p>
<p>In other words, traditional statistics sets up the following:</p>
<ul>
<li><p><strong>Null hypothesis (<em>H</em><sub>0</sub>):</strong> default assumption that some effect
(or quantity, or relationship, etc.) is zero and does not affect the
experimental results</p></li>
<li><p><strong>Alternative hypothesis (<em>H</em><sub>a</sub> or <em>H</em><sub>1</sub>):</strong> assumption that the
null hypothesis is false. Usually this is the researcher’s
scientific hypothesis, or proposed explanation for a phenomenon.</p></li>
</ul>
<p>In (slightly) more concrete terms, if researchers want to test the
hypothesis that some factor X has an effect on measurement Y, then they
would run an experiment comparing observations of Y across different
values of X. They would then have the following set up:</p>
<ul>
<li><p><strong>Null hypothesis:</strong> Factor X does not affect Y.</p></li>
<li><p><strong>Alternative hypothesis:</strong> Factor X affects Y.</p></li>
</ul>
<p>Notice that alternative hypothesis is the <strong>logical negation</strong> of the
null hypothesis, not a statement about another variable (e.g., “Factor Z
affects Y”). This is an easy mistake to make and one of the weaknesses
of NHST: the word “hypothesis” in the phrase “null hypothesis” is not
being used in the sense that scientists usually use it. This unfortunate
choice of vocabulary is responsible for generations of confusion among
researchers and statistics learners.</p>
<p>In order to test an “alternative hypothesis”, what researchers usually
do is estimate how likely the data would be if the null hypothesis were
true. If a pattern seen in the data is highly unlikely, this is taken as
evidence that the null hypothesis should be rejected in favor of the
alternative hypothesis. This is not the same as “accepting” or “proving”
the alternative hypothesis. Instead, we “reject” the null hypothesis. In
the other case, we do not “accept” the null hypothesis, we “fail to
reject” it.</p>
<p>The null hypothesis is usually translated to a prediction like one of
the following:</p>
<ul>
<li><p>The mean of Y is the same for all levels of X.</p></li>
<li><p>The mean of Y does not differ between observations exposed to X and
observations not exposed to X.</p></li>
<li><p>Outcome Y is equally likely to occur when X occurs as it is when X
does not occur.</p></li>
<li><p>Rank order of Y values does not differ between levels of X.</p></li>
</ul>
<p>All of these predictions have something in common: <strong>Y is independent of
X</strong>. That’s what the null hypothesis means. To test the predictions of
the null hypothesis, we need a way to calculate what the data might have
looked like if the null hypothesis was correct. Once we calculate that,
we can compare the data to those calculations to see how “unlikely” the
actual data were. Data that are sufficiently unlikely under the
assumption that the null hypothesis is correct—a “significant
difference”—are a kind of evidence that the null hypothesis is false.</p>
<p>Most classical statistical tests calculate a summary statistic called a
<strong><em>P</em>-value</strong> to estimate how likely some observed pattern in the
data would be if the null hypothesis was true. This is the value that
researchers often use to make inferences about their data. The <em>P</em>-value
depends on many factors, as we’ll see, but the most important are:</p>
<ul>
<li><p>The magnitude of some <strong>test statistic</strong>: a summary of the pattern
of interest.</p></li>
<li><p>Sample size, usually expressed as degrees of freedom (DF)</p></li>
<li><p>The variability of the data</p></li>
<li><p>The consistency, sign, and magnitude of the observed pattern</p></li>
</ul>
<p>When I teach statistics to biology majors, I try to avoid the terms
“null hypothesis” and “alternative hypothesis” at first, because most of
my students find them confusing. Instead, I instruct students to ask two
questions about a hypothesis:</p>
<ol style="list-style-type: decimal">
<li><p>What should we observe if the hypothesis is not true?</p></li>
<li><p>What should we observe if the hypothesis is true?</p></li>
</ol>
<p>In order to be scientific hypothesis, both of those questions have to be
answerable. Otherwise, we have no way of distinguishing between a
situation where a hypothesis is supported, or a situation where it is
not, because experimental results could always be due to random chance
(even if the probability of that is very small). All we can really do is
estimate how likely it is that random chance drove the outcome.</p>
<p>When interpreting <em>P</em>-values, there are 3 <u><strong><em>extremely important things
to remember</em></strong></u>:</p>
<ol style="list-style-type: decimal">
<li><p>When <em>P</em> &lt; 0.05, the hypothesis is “supported”. When <em>P</em> ≥ 0.05,
the hypothesis is “not supported”. Notice that the terms are
“supported” or “not supported”. Words like “proven” or “disproven”
are not appropriate when discussing <em>P</em>-values.</p></li>
<li><p>The <em>P</em>-value is <u><strong><em>not</em></strong></u> the probability that your
hypothesis is false, or the probability that a hypothesis is true.
It is a heuristic for assessing how well data support or do not
support a hypothesis.</p></li>
<li><p><em>P</em>-values depend on many factors, including sample size, effect
size, variation in the data, and how well the data fit the
assumptions of the statistical test used to calculate the <em>P</em>-value.
This means that <em>P</em>-values should not be taken at face value and
<u><strong><em>never</em></strong></u> used as the sole criterion by which a
hypothesis is accepted or rejected.</p></li>
</ol>
<p>When a statistical test returns a <em>P</em>-value &lt;0.05, we say that the
results are “statistically significant”. For example, a “statistically
significant difference” in means between two groups, or a “statistically
significant correlation” between two numeric variables.</p>
</div>
</div>
<div id="mod-09-whattest" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> What kind of test do you need?<a href="mod-09.html#mod-09-whattest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you’re not sure what kind of test you need, start here! Note that
this key is meant to guide you to one of the classical tests in this
module. For a more complete guide to choosing your analysis, refer to
<a href="mod-08.html#mod-08">Module 8</a>.</p>
<ol style="list-style-type: decimal">
<li><p>I want to…</p>
<ol style="list-style-type: lower-alpha">
<li><p>Compare a single set of values to a single value: Go to 2.</p></li>
<li><p>Compare two or more groups to see if their values are different:
Go to 3.</p></li>
<li><p>Compare two sets of continuous values (with or without a
grouping variable) to see if they are related: Go to 4.</p></li>
<li><p>Compare the proportions of observations that fall into different
groups or categories. I.e., Analyze a contingency table: Go to</p>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol></li>
</ol></li>
<li><p>Comparing a set of values to a single value. I want to…</p>
<ol style="list-style-type: lower-alpha">
<li><p>Test whether the mean is equal to some value: <strong>one-sample,
2-tailed <em>t</em>-test</strong>. See <a href="mod-09.html#mod-09-1samp-2tail-ttest">here</a>.</p></li>
<li><p>Test whether the mean is greater than some value: <strong>one-sample,
1-tailed <em>t</em>-test, alternative = “greater”</strong>. See
<a href="#mod-09-1samp-ttest">here</a>.</p></li>
<li><p>Test whether the mean is less than some value: <strong>one-sample,
1-tailed <em>t</em>-test, alternative = “less”</strong>. See
<a href="#mod-09-1samp-ttest">here</a>.</p></li>
<li><p>Test whether the mean of a set of values is equal to 0. You
can’t actually do this, but you can get close with an
<strong>equivalence test</strong>. See <a href="mod-09.html#mod-09-equivalence">here</a>.</p></li>
</ol></li>
<li><p>Comparing the means of 2 or more groups. How many groups do you
have?.</p>
<ol style="list-style-type: lower-alpha">
<li><p>2 groups. Go to 6.</p></li>
<li><p>3 or more groups. Go to 7.</p></li>
</ol></li>
<li><p>Compare 2 sets of continuous values to see if they are related. Pick
the choice that best matches your situation:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Test whether two variables are linearly correlated: <strong>linear
correlation</strong>. See <a href="mod-09.html#mod-09-linear-correl">here</a>.</p></li>
<li><p>Test whether two variables are nonlinearly correlated, or
assumptions of linear correlation not met: <strong>nonlinear
correlation (aka: rank correlation)</strong>. See
<a href="mod-09.html#mod-09-nonlinear">here</a>.</p></li>
<li><p>Use one continuous variable to predict another, using a linear
function: <strong>linear regression</strong>. See <a href="mod-09.html#mod-09-lin-reg">here</a>.</p></li>
<li><p>Use one continuous variable and a factor to predict a continuous
variable: <strong>analysis of covariance (ANCOVA)</strong>. See
<a href="mod-09.html#mod-09-ancova">here</a>.</p></li>
</ol></li>
<li><p>Compare proportions of observations across groups; i.e., analyze a
<a href="mod-09.html#mod-09-ctables">contingency table</a>. Pick the choice that best
matches your situation.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Sample size is small: <strong>Fisher’s exact test</strong>. See
<a href="mod-09.html#mod-09-fishers">here</a>.</p></li>
<li><p>Sample size is large: <strong>Chi-squared (</strong><span class="math inline">\(\chi^2\)</span>) test. See
<a href="mod-09.html#mod-09-chisq">here</a>.</p></li>
<li><p>Sample size is really large: <strong><em>G</em></strong>-test. See
<a href="mod-09.html#mod-09-gtest">here</a>.</p></li>
<li><p>I want to predict group membership of observations, not test for
independence: <strong>Logistic regression</strong>, <strong>multinomial
regression</strong>, <strong>classification trees</strong>, or something else more
exotic. Not covered in this module.</p></li>
</ol></li>
<li><p>Compare the means of 2 groups. Pick the choice that best matches
your situation:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Compare the means of 2 groups, assumptions of <em>t</em>-test met:
<strong>two-sample, 2-tailed <em>t</em>-test</strong>. See
<a href="mod-09.html#mod-09-2samp-2tail-ttest">here</a>. This is what most people mean
if they say “<em>t</em>-test” without any qualification.</p></li>
<li><p>Test whether the mean difference in paired observations is 0:
<strong>paired <em>t</em>-test</strong>. See <a href="mod-09.html#mod-09-paired-ttest">here</a>.</p></li>
<li><p>Test whether values in one group are consistently greater or
smaller than in the other group: <strong>Wilcoxon test (AKA:
Mann-Whitney U test)</strong>. See <a href="mod-09.html#mod-09-wilcoxon">here</a>.</p></li>
<li><p>(Uncommon). Test whether difference in means between 2 groups is
greater than some value: <strong>two-sample, 1-tailed <em>t</em>-test,
alternative = “greater”</strong>. See <a href="mod-09.html#mod-09-2samp-1tail">here</a>.</p></li>
<li><p>(Uncommon). Test whether difference in means between 2 groups is
less than some value: <strong>two-sample, 1-tailed <em>t</em>-test,
alternative = “less”</strong>. See <a href="mod-09.html#mod-09-2samp-1tail">here</a>.</p></li>
</ol></li>
<li><p>Compare the means of 3 or more groups.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Assumptions of linear models met: <strong>analysis of variance
(ANOVA)</strong>. See <a href="mod-09.html#mod-09-anova">here</a>.</p></li>
<li><p>Assumptions of linear models not met: either transform to meet
assumptions and use ANOVA, or use <strong>Kruskal-Wallis test</strong>. See
<a href="mod-09.html#mod-09-kwtest">here</a>.</p></li>
</ol></li>
</ol>
</div>
<div id="mod-09-tests-for-loc" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Tests for differences in mean or location<a href="mod-09.html#mod-09-tests-for-loc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="tests-for-1-group" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Tests for 1 group<a href="mod-09.html#tests-for-1-group" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When all of the values come from the same set of observations, we can
ask questions such as:</p>
<ul>
<li><p>Is the mean in this group equal to, or different from, some
arbitrary value?</p></li>
<li><p>Is the mean in this group greater than, or not greater than, some
arbitrary value?</p></li>
</ul>
<p>These are the domain of <strong>one-sample tests</strong>. The most important is the
<strong>one-sample <em>t</em>-test</strong>, which comes in two varieties: one-tailed and
two-tailed. All one-sample tests compare the mean of a set of values to
some quantity (hypothetical mean) of interest, called μ (the Greek
letter “mu”). In a one-tailed test, the null hypothesis is that the
underlying population mean is either ≤ or ≥ μ. In a two-tailed test, the
null hypothesis is that the population mean is = μ.</p>
<p><strong>A note on symbology: sample vs. population parameters</strong></p>
<p>In statistics, we analyze <strong>samples</strong> (randomly-selected subsets of
observed entities) to make inferences about <strong>populations</strong> (the
hypothetical set of all possible entities which could be included in a
sample). For example, if you wanted to know how the population of a
nation of 100 million citizens felt about some political issue, it would
be time-consuming and expensive to ask each citizen what they thought.
Instead, you would sample a randomly selected set of, say, 1000 citizens
and then assume that those results reflected the opinion of the entire
citizenry. For this assumption to work, your sample must be both random
and representative of the entire population.</p>
<p>Similarly, in science we cannot collect a infinite amount of data, or
measure every squirrel in the forest, or any other type of exhaustive
and comprehensive data collection. We instead take representative
samples of all the possible samples–i.e., the population–and rely on
good practices in experimental design to enable us to assume that our
sample represents the population.</p>
<p>The reason I bring this up here is that we need to keep in mind the
difference between sample statistics and population statistics. <strong>Sample
statistics</strong> are calculated from your data. <strong>Population statistics</strong>
are <strong>inferred</strong> from your data using statistical models. This is why
statistical tests are commonly referred to as <strong>inferential
statistics</strong>, and why simple calculations from data are referred to as
<strong>descriptive statistics</strong>.</p>
<p>The most common descriptive statistics are shown below:</p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Sample</th>
<th align="center">Population</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean</td>
<td align="center"><span class="math inline">\(\bar{x}\)</span></td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="even">
<td>Variance</td>
<td align="center"><span class="math inline">\(s^{2}\)</span></td>
<td align="center"><span class="math inline">\(\sigma^{2}\)</span></td>
</tr>
<tr class="odd">
<td>Standard deviation</td>
<td align="center"><span class="math inline">\(s\)</span></td>
<td align="center"><span class="math inline">\(\sigma\)</span></td>
</tr>
</tbody>
</table>
<p>This means that some quantities that look inferential, such as
correlation coefficients, are actually descriptive. So, the old saw that
“correlation does not equal causation” is true in a literal mathematical
sense.</p>
<div id="mod-09-1samp-1-tail" class="section level4 hasAnchor" number="9.4.1.1">
<h4><span class="header-section-number">9.4.1.1</span> One-sample <em>t</em>-test (one-tailed)<a href="mod-09.html#mod-09-1samp-1-tail" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In a one-tailed one-sample test, we are interested in whether the
underlying population mean (<span class="math inline">\(\mu\)</span>) of a set of values is not equal to
some value of interest…either at most some value, or at least some
value.</p>
<p><strong>Question:</strong> Is the mean value less than OR greater than some value
<span class="math inline">\(x\)</span>?</p>
<p><strong>Null hypothesis:</strong> True mean is <span class="math inline">\(=x\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> True mean is <span class="math inline">\(&lt;x\)</span> or true mean is <span class="math inline">\(&gt;x\)</span>.</p>
<p><strong>Example use case:</strong>Fred needs to check whether the mean measurement
error across his 12 technicians is less than 0.5 mm.</p>
<p>Here is an example where we simulate some data in R and run a one-tailed
one-sample <em>t</em>-test.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="mod-09.html#cb889-1" tabindex="-1"></a><span class="co"># simulate some values with nominal mean 5 and sd 3</span></span>
<span id="cb889-2"><a href="mod-09.html#cb889-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb889-3"><a href="mod-09.html#cb889-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb889-4"><a href="mod-09.html#cb889-4" tabindex="-1"></a></span>
<span id="cb889-5"><a href="mod-09.html#cb889-5" tabindex="-1"></a><span class="co"># 1 sample 1 tailed t-test</span></span>
<span id="cb889-6"><a href="mod-09.html#cb889-6" tabindex="-1"></a><span class="co"># Example 1: Is the underlying mean &lt;= 0?</span></span>
<span id="cb889-7"><a href="mod-09.html#cb889-7" tabindex="-1"></a><span class="fu">t.test</span>(a, <span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  a
## t = 8.3142, df = 19, p-value = 4.707e-08
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  4.29664     Inf
## sample estimates:
## mean of x 
##  5.424871</code></pre>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb891-1"><a href="mod-09.html#cb891-1" tabindex="-1"></a><span class="co"># Example 2: Is the underlying mean &lt;= 4?</span></span>
<span id="cb891-2"><a href="mod-09.html#cb891-2" tabindex="-1"></a><span class="fu">t.test</span>(a, <span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="at">mu=</span><span class="dv">4</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  a
## t = 2.1838, df = 19, p-value = 0.02086
## alternative hypothesis: true mean is greater than 4
## 95 percent confidence interval:
##  4.29664     Inf
## sample estimates:
## mean of x 
##  5.424871</code></pre>
<p>Interpretation: In the example above, the researcher can reject the null
hypothesis that the true mean is <span class="math inline">\(\leq4\)</span>, and conclude that the true
mean is <span class="math inline">\(&gt;4\)</span>.</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb893-1"><a href="mod-09.html#cb893-1" tabindex="-1"></a><span class="co"># Example 3: Is the underlying mean &gt;= 5.5?</span></span>
<span id="cb893-2"><a href="mod-09.html#cb893-2" tabindex="-1"></a><span class="fu">t.test</span>(a, <span class="at">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="at">mu=</span><span class="fl">5.5</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  a
## t = -0.11514, df = 19, p-value = 0.4548
## alternative hypothesis: true mean is less than 5.5
## 95 percent confidence interval:
##      -Inf 6.553102
## sample estimates:
## mean of x 
##  5.424871</code></pre>
<p>Interpretation: In example 3, the researcher cannot reject the null
hypothesis that <span class="math inline">\(\mu\geq5.5\)</span>. This makes sense, because the true mean is
about <span class="math inline">\(5.4\pm2.9\)</span> (verify with <code>mean(a);sd(a)</code>).</p>
</div>
<div id="mod-09-1samp-2tail-ttest" class="section level4 hasAnchor" number="9.4.1.2">
<h4><span class="header-section-number">9.4.1.2</span> One-sample <em>t</em>-test (two-tailed)<a href="mod-09.html#mod-09-1samp-2tail-ttest" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In a two-tailed test, we are interested in whether the underlying
population mean of a set of values is <strong>equal</strong> to some value of
interest.</p>
<p><strong>Question:</strong> Is the underlying true mean equal to, or different from,
some value <span class="math inline">\(\mu\)</span>?</p>
<p><strong>Null hypothesis:</strong> True mean <span class="math inline">\(=\mu\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> True mean <span class="math inline">\(\neq\mu\)</span></p>
<p><strong>Example use case:</strong> Wilma needs to test whether the mean pH of her
prepared stock solution is equal to, or different from, the target pH of
7.7.</p>
<p>Here is an example where we simulate some data and conduct a one-sample,
two-tailed <em>t</em>-test.</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb895-1"><a href="mod-09.html#cb895-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb895-2"><a href="mod-09.html#cb895-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb895-3"><a href="mod-09.html#cb895-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb895-4"><a href="mod-09.html#cb895-4" tabindex="-1"></a></span>
<span id="cb895-5"><a href="mod-09.html#cb895-5" tabindex="-1"></a><span class="co"># 1 sample 2-tailed t-test:</span></span>
<span id="cb895-6"><a href="mod-09.html#cb895-6" tabindex="-1"></a><span class="do">## Example 1: mu = 0 (default)</span></span>
<span id="cb895-7"><a href="mod-09.html#cb895-7" tabindex="-1"></a><span class="fu">t.test</span>(a)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  a
## t = 14.445, df = 19, p-value = 1.067e-11
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  2.686402 3.596845
## sample estimates:
## mean of x 
##  3.141624</code></pre>
<p>Interpretation: In the example above, the researcher can reject the null
that the true mean = 0, and conclude that the true mean is significantly
different from 0.</p>
<p>Here’s another example with a different dataset and test.</p>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb897-1"><a href="mod-09.html#cb897-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb897-2"><a href="mod-09.html#cb897-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb897-3"><a href="mod-09.html#cb897-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb897-4"><a href="mod-09.html#cb897-4" tabindex="-1"></a></span>
<span id="cb897-5"><a href="mod-09.html#cb897-5" tabindex="-1"></a><span class="co"># 1 sample 2-tailed t-test:</span></span>
<span id="cb897-6"><a href="mod-09.html#cb897-6" tabindex="-1"></a><span class="do">## Example 2: mu = 5</span></span>
<span id="cb897-7"><a href="mod-09.html#cb897-7" tabindex="-1"></a><span class="fu">t.test</span>(a, <span class="at">mu=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  a
## t = -8.5445, df = 19, p-value = 6.218e-08
## alternative hypothesis: true mean is not equal to 5
## 95 percent confidence interval:
##  2.686402 3.596845
## sample estimates:
## mean of x 
##  3.141624</code></pre>
<p>Interpretation: The researcher can reject the null hypothesis that the
mean = 5, and conclude that the true mean is not 5. Note that this
includes both &lt;5 and &gt;5 as possibilities, but the direction is obvious
from the sample mean.</p>
</div>
</div>
<div id="tests-comparing-two-groups" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Tests comparing two groups<a href="mod-09.html#tests-comparing-two-groups" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="mod-09-2samp-2tail-ttest" class="section level4 hasAnchor" number="9.4.2.1">
<h4><span class="header-section-number">9.4.2.1</span> Student’s <em>t</em>-test (Welch)<a href="mod-09.html#mod-09-2samp-2tail-ttest" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When most people say “<em>t</em>-test”, what they really mean is a two-sample,
two-tailed <em>t</em>-test that tests for a <u><strong>difference in means between two
groups</strong></u>. The original version was introduced as the
“<strong>Student’s <em>t</em>-test</strong>” by William Sealy Gosset in 1908<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a>;
modern software uses a modified version called the <strong>Welch <em>t</em>-test</strong>
(or sometimes the “<strong>Welch-Satterthwaite <em>t</em>-test</strong>”, but this is not
quite correct). The newer version is better at accounting for
non-normality and unequal variances.</p>
<p><strong>Question:</strong> Do two groups have the same underlying population mean?</p>
<p><strong>Null hypothesis:</strong> The true difference in means = 0. I.e.,
<span class="math inline">\(\mu_1=\mu_2\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> True difference in means is <span class="math inline">\(\neq0\)</span>. I.e.,
<span class="math inline">\(\mu_1\neq\mu_2\)</span>.</p>
<p><strong>Example use case:</strong> Barney needs to test whether tomato plants treated
with a new pesticide have a different yield (g tomato / plant) than
plants in the untreated control group.</p>
<p>Below is an example where we simulate some data and run a two-sample,
two-tailed test to compare the means of two groups.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="mod-09.html#cb899-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb899-2"><a href="mod-09.html#cb899-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb899-3"><a href="mod-09.html#cb899-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb899-4"><a href="mod-09.html#cb899-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb899-5"><a href="mod-09.html#cb899-5" tabindex="-1"></a></span>
<span id="cb899-6"><a href="mod-09.html#cb899-6" tabindex="-1"></a><span class="co"># example 1: difference in means</span></span>
<span id="cb899-7"><a href="mod-09.html#cb899-7" tabindex="-1"></a><span class="fu">t.test</span>(a,b)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  a and b
## t = 8.1211, df = 194.18, p-value = 5.241e-14
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  1.696004 2.783990
## sample estimates:
## mean of x mean of y 
##  5.065030  2.825033</code></pre>
<p>Interpretation: The researcher should conclude that there is a
statistically significant difference between the means of groups a and
b. In a manuscript, you should report the test statistic, the DF, and
the <em>p</em>-value. E.g., “There was a significant difference in means
between group a and group b (<span class="math inline">\(t=8.121\)</span>, 194.18 d.f., <span class="math inline">\(p&lt;0.001\)</span>).” Notice
that values are rounded to 2 or 3 decimal places, which is usually
enough for biology. This is especially true for the <em>p</em>-value, which can
be approximated by R down to infinitesimally small values. I always
round those to 0.001 or 0.0001.</p>
<p>A <em>t</em>-test can also be performed using the <strong>formula</strong> interface, which
is an R programming construct for specifying a response variable and its
predictors.</p>
<p><strong>Syntax note:</strong> Formula syntax has the response variable on the left,
then a <code>~</code>, then the predictor or predictors. For predictors, <code>a + b</code>
means an additive model; while <code>a * b</code> specifies an interaction. For a
<em>t</em>-test, there can be only 1 predictor. If you want to test &gt;1
predictor at a time, you need to use ANOVA instead.</p>
<p>Here are some common formula examples:</p>
<p><strong>Formula:</strong> <code>y ~ x</code></p>
<p><strong>Meaning:</strong> <code>y</code> is a function of <code>x</code>.</p>
<p><strong>Formula:</strong> <code>y ~ x1 + x2</code></p>
<p><strong>Meaning:</strong> <code>y</code> is a function of <code>x1</code> and <code>x2</code>, and the effects of <code>x1</code>
and <code>x2</code> are orthogonal (i.e., independent of each other).</p>
<p><strong>Formula:</strong> <code>y ~ x1 * x2</code></p>
<p><strong>Meaning:</strong> <code>y</code> is a function of <code>x1</code> and <code>x2</code>, and the effects of <code>x1</code>
and <code>x2</code> <strong>interact</strong>. This means that one variable alters the effect of
another variable.</p>
<p>The formula interface is used in several contexts in R, including
plotting, modeling, and aggregating.</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb901-1"><a href="mod-09.html#cb901-1" tabindex="-1"></a><span class="co"># pull some data from example dataset iris</span></span>
<span id="cb901-2"><a href="mod-09.html#cb901-2" tabindex="-1"></a><span class="co"># i.e., make a new data frame with only 2 groups</span></span>
<span id="cb901-3"><a href="mod-09.html#cb901-3" tabindex="-1"></a>dx <span class="ot">&lt;-</span> iris[<span class="fu">which</span>(iris<span class="sc">$</span>Species <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;versicolor&quot;</span>)),]</span>
<span id="cb901-4"><a href="mod-09.html#cb901-4" tabindex="-1"></a></span>
<span id="cb901-5"><a href="mod-09.html#cb901-5" tabindex="-1"></a><span class="co"># perform the test.</span></span>
<span id="cb901-6"><a href="mod-09.html#cb901-6" tabindex="-1"></a><span class="fu">t.test</span>(Petal.Length<span class="sc">~</span>Species, <span class="at">data=</span>dx)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Petal.Length by Species
## t = -39.493, df = 62.14, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means between group setosa and group versicolor is not equal to 0
## 95 percent confidence interval:
##  -2.939618 -2.656382
## sample estimates:
##     mean in group setosa mean in group versicolor 
##                    1.462                    4.260</code></pre>
<p>Interpretation: The researcher should conclude that there is a
statistically significant difference between the mean petal length of
<em>Iris setosa</em> and <em>Iris versicolor</em>.</p>
</div>
<div id="mod-09-2samp-1tail" class="section level4 hasAnchor" number="9.4.2.2">
<h4><span class="header-section-number">9.4.2.2</span> 2-sample, 1-tailed <em>t</em>-test<a href="mod-09.html#mod-09-2samp-1tail" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Is the true difference in means between 2 groups greater
than, or less than, some target value <span class="math inline">\(\mu\)</span>?</p>
<p><strong>Null hypothesis:</strong> The true difference in means between 2 groups is
<span class="math inline">\(\mu\)</span>. I.e., <span class="math inline">\(\mu_1-\mu_2=\mu\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> The true difference in means between 2
groups is either greater than <span class="math inline">\(&gt;\mu\)</span> or <span class="math inline">\(&lt;\mu\)</span>. I.e., either
<span class="math inline">\(\mu_1-\mu_2&gt;\mu\)</span> or <span class="math inline">\(\mu_1-\mu_2&lt;\mu\)</span>.</p>
<p><strong>Example use case:</strong> Betty needs to compare two stock solutions and
check that their pH differs by no more than 0.2.</p>
<p>The 2-sample, 1-tailed <em>t</em>-test that can be used in several ways:</p>
<ol style="list-style-type: decimal">
<li>Test whether the mean in one group is greater or less than the mean
in another group.</li>
<li>Test whether the the mean in one group is greater or less than the
mean in another group, and that the difference is at least some
specified value.</li>
</ol>
<p>For example, if you know that the mean of group 1 must be less than the
mean in group 2, and will not even consider that the mean of group 2 is
less than that of group 1, then you could run a 2-sample 1-tailed test.</p>
<div class="sourceCode" id="cb903"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb903-1"><a href="mod-09.html#cb903-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb903-2"><a href="mod-09.html#cb903-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb903-3"><a href="mod-09.html#cb903-3" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">1</span>)</span>
<span id="cb903-4"><a href="mod-09.html#cb903-4" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">12</span>, <span class="dv">1</span>)</span>
<span id="cb903-5"><a href="mod-09.html#cb903-5" tabindex="-1"></a></span>
<span id="cb903-6"><a href="mod-09.html#cb903-6" tabindex="-1"></a><span class="co"># is mean(x1) &gt; mean(x2)?</span></span>
<span id="cb903-7"><a href="mod-09.html#cb903-7" tabindex="-1"></a><span class="fu">t.test</span>(x1, x2, <span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = -8.8728, df = 37.99, p-value = 1
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -2.402592       Inf
## sample estimates:
## mean of x mean of y 
##  9.689982 11.708940</code></pre>
<div class="sourceCode" id="cb905"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb905-1"><a href="mod-09.html#cb905-1" tabindex="-1"></a><span class="co"># is mean(x1) &lt; mean(x2)?</span></span>
<span id="cb905-2"><a href="mod-09.html#cb905-2" tabindex="-1"></a><span class="fu">t.test</span>(x1, x2, <span class="at">alternative=</span><span class="st">&quot;less&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = -8.8728, df = 37.99, p-value = 4.259e-11
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##       -Inf -1.635323
## sample estimates:
## mean of x mean of y 
##  9.689982 11.708940</code></pre>
<div class="sourceCode" id="cb907"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb907-1"><a href="mod-09.html#cb907-1" tabindex="-1"></a><span class="co"># is mean(x1) - mean(x2) at least 1?</span></span>
<span id="cb907-2"><a href="mod-09.html#cb907-2" tabindex="-1"></a><span class="fu">t.test</span>(x1, x2, <span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="at">mu=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = -13.267, df = 37.99, p-value = 1
## alternative hypothesis: true difference in means is greater than 1
## 95 percent confidence interval:
##  -2.402592       Inf
## sample estimates:
## mean of x mean of y 
##  9.689982 11.708940</code></pre>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb909-1"><a href="mod-09.html#cb909-1" tabindex="-1"></a><span class="co"># is mean(x1) - mean(x2) at most 1?</span></span>
<span id="cb909-2"><a href="mod-09.html#cb909-2" tabindex="-1"></a><span class="fu">t.test</span>(x1, x2, <span class="at">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="at">mu=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = -13.267, df = 37.99, p-value = 3.867e-16
## alternative hypothesis: true difference in means is less than 1
## 95 percent confidence interval:
##       -Inf -1.635323
## sample estimates:
## mean of x mean of y 
##  9.689982 11.708940</code></pre>
<p>an uncommon variant used to test whether two groups have a difference in
means that is less than or greater than some specified value. For
example, if you produce two batches of bacteria by inoculation,
incubation, and then serial dilution, you may want to confirm that the
difference in cell count between them is less than some acceptable
value. You could use a 2-sample, 1-tailed test with the “alternative”
hypothesis being that the difference is “greater” than your benchmark.</p>
<div class="sourceCode" id="cb911"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb911-1"><a href="mod-09.html#cb911-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb911-2"><a href="mod-09.html#cb911-2" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">30</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb911-3"><a href="mod-09.html#cb911-3" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">30</span>, <span class="fl">5.5</span>, <span class="dv">2</span>)</span>
<span id="cb911-4"><a href="mod-09.html#cb911-4" tabindex="-1"></a></span>
<span id="cb911-5"><a href="mod-09.html#cb911-5" tabindex="-1"></a><span class="co"># is the true difference in means &lt;1?</span></span>
<span id="cb911-6"><a href="mod-09.html#cb911-6" tabindex="-1"></a><span class="fu">t.test</span>(x1, x2, <span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = -2.6676, df = 50.08, p-value = 0.9949
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -2.048126       Inf
## sample estimates:
## mean of x mean of y 
##  4.438104  5.696001</code></pre>
</div>
<div id="mod-09-paired-ttest" class="section level4 hasAnchor" number="9.4.2.3">
<h4><span class="header-section-number">9.4.2.3</span> Paired <em>t</em>-test<a href="mod-09.html#mod-09-paired-ttest" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Is the mean of the differences of paired observations = 0,
or different from 0?</p>
<p><strong>Null hypothesis:</strong> Mean of pairwise differences = 0.</p>
<p><strong>Alternative hypothesis:</strong> Mean of pairwise differences <span class="math inline">\(\neq0\)</span>.</p>
<p><strong>Example use case:</strong> Pebbles needs to test for a mean change in the
body mass of individual mice before and after a feeding trial. Each
mouse is weighed both before and after the trial.</p>
<p>Many biological experiments involved <strong>paired samples</strong>, where a single
unit or entity was measured twice. For example, a single mouse might be
weighed before and after a feeding trial. In a <strong>paired <em>t</em>-test</strong>, we
test whether or not the mean of differences is 0. This is subtly
different than the test for differences in means evaluated in a
two-sample test. Consequently, a paired <em>t</em>-test is tantamount to a
one-sample test on the pairwise differences. Compare the null hypotheses
tested by the two-sample <em>t</em>-test and paired <em>t</em>-test below:</p>
<p><strong>Two-sample <em>t</em>-test:</strong>
<span class="math inline">\(\frac{\sum_{i=1}^{n_1}x_{1,i}}{n_1}=\frac{\sum_{i=1}^{n_2}x_{2,i}}{n_2}\)</span></p>
<p><strong>Paired <em>t</em>-test:</strong> <span class="math inline">\(\frac{\sum_{i=1}^{n}{x_{1,i}-x_{2,i}}}{n}=0\)</span></p>
<p>Let’s simulate some data, conduct a paired <em>t</em>-test, and convince
ourselves that the paired <em>t</em>-test is equivalent to a one-sample test on
pairwise differences.</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb913-1"><a href="mod-09.html#cb913-1" tabindex="-1"></a><span class="co"># Example paired t-test in R</span></span>
<span id="cb913-2"><a href="mod-09.html#cb913-2" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb913-3"><a href="mod-09.html#cb913-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb913-4"><a href="mod-09.html#cb913-4" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb913-5"><a href="mod-09.html#cb913-5" tabindex="-1"></a><span class="co"># add pair-wise differences</span></span>
<span id="cb913-6"><a href="mod-09.html#cb913-6" tabindex="-1"></a>b <span class="ot">&lt;-</span> a <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">2</span>, <span class="fl">0.2</span>)</span></code></pre></div>
<p>Interpretation: The researcher should conclude that the mean difference
for each subject was significantly different from 0, at -1.98 with a 95%
CI of [-2.02, -1.95].</p>
<p>Note that the paired <em>t</em>-test gives essentially the same result as
conducting a one-sample test on the pair-wise differences.</p>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb914-1"><a href="mod-09.html#cb914-1" tabindex="-1"></a><span class="co"># compare to one sample t-test on differences:</span></span>
<span id="cb914-2"><a href="mod-09.html#cb914-2" tabindex="-1"></a></span>
<span id="cb914-3"><a href="mod-09.html#cb914-3" tabindex="-1"></a>dif <span class="ot">&lt;-</span> b <span class="sc">-</span> a</span>
<span id="cb914-4"><a href="mod-09.html#cb914-4" tabindex="-1"></a><span class="fu">t.test</span>(dif)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  dif
## t = 109.63, df = 99, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  1.946622 2.018385
## sample estimates:
## mean of x 
##  1.982503</code></pre>
</div>
<div id="mod-09-equivalence" class="section level4 hasAnchor" number="9.4.2.4">
<h4><span class="header-section-number">9.4.2.4</span> Equivalence testing<a href="mod-09.html#mod-09-equivalence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>It is not possible to demonstrate statistically that a difference in
means is truly 0. With a 1-sample, 2-tailed test the best you can do is
“fail to reject” the null hypothesis that the mean is 0. However, we can
show that the difference is arbitrarily close to 0. This approach is
called <strong>equivalence testing</strong>, and one method is the <strong>two one-sided
tests</strong> (<strong>TOST</strong>) method. The method works like this:</p>
<p><img src="09_03.jpg" /></p>
<p>Here is a worked TOST equivalence test example in R:</p>
<div class="sourceCode" id="cb916"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb916-1"><a href="mod-09.html#cb916-1" tabindex="-1"></a><span class="co"># simulate data</span></span>
<span id="cb916-2"><a href="mod-09.html#cb916-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb916-3"><a href="mod-09.html#cb916-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb916-4"><a href="mod-09.html#cb916-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb916-5"><a href="mod-09.html#cb916-5" tabindex="-1"></a></span>
<span id="cb916-6"><a href="mod-09.html#cb916-6" tabindex="-1"></a><span class="co"># target tolerance:</span></span>
<span id="cb916-7"><a href="mod-09.html#cb916-7" tabindex="-1"></a><span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="mod-09.html#cb918-1" tabindex="-1"></a><span class="co"># note that total tolerance range is 1, so</span></span>
<span id="cb918-2"><a href="mod-09.html#cb918-2" tabindex="-1"></a><span class="co"># amount for both one-sided tests is 0.5</span></span>
<span id="cb918-3"><a href="mod-09.html#cb918-3" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb918-4"><a href="mod-09.html#cb918-4" tabindex="-1"></a></span>
<span id="cb918-5"><a href="mod-09.html#cb918-5" tabindex="-1"></a><span class="co"># test 1: is difference &gt;-D?</span></span>
<span id="cb918-6"><a href="mod-09.html#cb918-6" tabindex="-1"></a><span class="fu">t.test</span>(a, b, <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span>, <span class="at">mu=</span><span class="sc">-</span>D)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  a and b
## t = 4.4956, df = 194.18, p-value = 5.955e-06
## alternative hypothesis: true difference in means is greater than -0.5
## 95 percent confidence interval:
##  -0.1079329        Inf
## sample estimates:
## mean of x mean of y 
##  5.032515  4.912516</code></pre>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb920-1"><a href="mod-09.html#cb920-1" tabindex="-1"></a><span class="co"># test 2: is difference &lt;D?</span></span>
<span id="cb920-2"><a href="mod-09.html#cb920-2" tabindex="-1"></a><span class="fu">t.test</span>(a, b, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>, <span class="at">mu=</span>D)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  a and b
## t = -2.7554, df = 194.18, p-value = 0.00321
## alternative hypothesis: true difference in means is less than 0.5
## 95 percent confidence interval:
##     -Inf 0.34793
## sample estimates:
## mean of x mean of y 
##  5.032515  4.912516</code></pre>
<p>Interpretation: Test 1 showed that the difference in means is <span class="math inline">\(&gt;-0.5\)</span>.
Test 2 showed that the difference in means is <span class="math inline">\(&lt;0.5\)</span>. So, the difference
in means is in the interval (-0.5, 0.5), which means the difference in
means is at most 0.5 in either direction.</p>
</div>
<div id="mod-09-wilcoxon" class="section level4 hasAnchor" number="9.4.2.5">
<h4><span class="header-section-number">9.4.2.5</span> Wilcoxon rank sum test (aka: Mann-Whitney <em>U</em> test)<a href="mod-09.html#mod-09-wilcoxon" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Are values in one group consistently greater than or less
than those in another group? OR: Is the median rank in one group
different from that in another group?</p>
<p><strong>Null hypothesis:</strong> For randomly selected values <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> from two
groups, <span class="math inline">\(p\left(x&gt;y\right)=p\left(y&gt;x\right)\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> For randomly selected values X and Y from
two groups, <span class="math inline">\(p\left(x&gt;y\right)\neq p\left(y&gt;x\right)\)</span>.</p>
<p><strong>Example use case:</strong> Homer is interested in whether plots where
invasive plants have been removed have more squirrels than plots where
invasive plants are not removed, but does not want to model the actual
number of squirrels.</p>
<p>The <strong>Mann-Whitney <em>U</em> test</strong>, also known as the <strong>Wilcoxon test</strong>, is a
nonparametric alternative to the two-sample <em>t</em>-test. <strong>Nonparametric</strong>
means that the test does not make restrictive assumptions about the
distribution of the data or a test statistic. Nonparametric tests are
often based on the <strong>ranks</strong> of the data: the smallest value is rank 1,
the next smallest is rank 2, and so on. Because of this, nonparametric
tests allow you make statements about how consistent a pattern is,
rather than about the actual values.</p>
<p>This test has two common use cases:</p>
<ul>
<li><p>The researcher is only interested in whether values in one group are
consistently larger than those in another group.</p></li>
<li><p>The researcher needs to perform a <em>t</em>-test, but the data do not meet
the assumptions of the <em>t</em>-test.</p></li>
</ul>
<p>The syntax for the Wilcoxon test is fairly similar to that of a
<em>t</em>-test.</p>
<p>Example Wilcoxon test in R:</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="mod-09.html#cb922-1" tabindex="-1"></a><span class="co"># simulate some data:</span></span>
<span id="cb922-2"><a href="mod-09.html#cb922-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb922-3"><a href="mod-09.html#cb922-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">20</span>, <span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb922-4"><a href="mod-09.html#cb922-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">20</span>, <span class="dv">4</span>, <span class="dv">9</span>)</span>
<span id="cb922-5"><a href="mod-09.html#cb922-5" tabindex="-1"></a></span>
<span id="cb922-6"><a href="mod-09.html#cb922-6" tabindex="-1"></a><span class="co"># run the test</span></span>
<span id="cb922-7"><a href="mod-09.html#cb922-7" tabindex="-1"></a><span class="fu">wilcox.test</span>(a,b)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  a and b
## W = 35, p-value = 1.126e-06
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Here is the same kind of test, but using the formula interface. You can
use the formula interface for <em>t</em>-tests as well.</p>
<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb924-1"><a href="mod-09.html#cb924-1" tabindex="-1"></a><span class="co"># pull some data from example dataset iris</span></span>
<span id="cb924-2"><a href="mod-09.html#cb924-2" tabindex="-1"></a><span class="co"># i.e., make a new data frame with only 2 groups</span></span>
<span id="cb924-3"><a href="mod-09.html#cb924-3" tabindex="-1"></a>use.species <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;versicolor&quot;</span>, <span class="st">&quot;virginica&quot;</span>)</span>
<span id="cb924-4"><a href="mod-09.html#cb924-4" tabindex="-1"></a>dx <span class="ot">&lt;-</span> iris[<span class="fu">which</span>(iris<span class="sc">$</span>Species <span class="sc">%in%</span> use.species),]</span>
<span id="cb924-5"><a href="mod-09.html#cb924-5" tabindex="-1"></a>dx<span class="sc">$</span>Species <span class="ot">&lt;-</span> <span class="fu">factor</span>(dx<span class="sc">$</span>Species, <span class="at">levels=</span>use.species)</span>
<span id="cb924-6"><a href="mod-09.html#cb924-6" tabindex="-1"></a></span>
<span id="cb924-7"><a href="mod-09.html#cb924-7" tabindex="-1"></a><span class="co"># perform the test.</span></span>
<span id="cb924-8"><a href="mod-09.html#cb924-8" tabindex="-1"></a><span class="fu">wilcox.test</span>(Petal.Length<span class="sc">~</span>Species, <span class="at">data=</span>dx)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  Petal.Length by Species
## W = 44.5, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Interpretation: The researcher can say that values in one group are
consistently greater than values in the other group. Which way that goes
should be obvious from looking at the sample medians or a boxplot.</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb926-1"><a href="mod-09.html#cb926-1" tabindex="-1"></a><span class="co"># follow up the test with a boxplot</span></span>
<span id="cb926-2"><a href="mod-09.html#cb926-2" tabindex="-1"></a><span class="co"># note that the formula interface is used here as well!</span></span>
<span id="cb926-3"><a href="mod-09.html#cb926-3" tabindex="-1"></a><span class="fu">boxplot</span>(Petal.Length<span class="sc">~</span>Species, <span class="at">data=</span>dx)</span></code></pre></div>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-536-1.png" width="672" /></p>
<p>When sample sizes or effect sizes are large, the Wilcoxon test and the
<em>t</em>-test will often give the same answers in terms of significance vs.
nonsignificance.</p>
<p>Because the <em>t</em>-test tests for a difference in means, and the Wilcoxon
is its rank-based alternative, some people think that the Wilcoxon test
is a test for a difference in medians…but this is <strong>not correct</strong>. It’s
more accurate to say that the Wilcoxon test tests for a median
difference.</p>
</div>
</div>
<div id="aside-whats-with-all-these-t-tests" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Aside: what’s with all these <em>t</em>-tests?<a href="mod-09.html#aside-whats-with-all-these-t-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If it’s not obvious by now, the <em>t</em>-test was a very important
development in modern statistics. This is because its test statistic,
<em>t</em>, is very flexible and inclusive. It starts with the quantity of
interest–a difference between a group mean, and either another group
mean and a target value–and scales it by sample size and uncertainty.
Thus, there is a <em>t</em>-test for many situations. A summary is below:</p>
<table style="width:57%;">
<colgroup>
<col width="23%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>One-sample
<em>t</em>-test,
one-tailed</td>
<td>Tests whether
population mean of a
single set of values
is less than, or
greater than, a
specified values
<span class="math inline">\(\mu_{0}\)</span>. Only
appropriate when
differences in one
direction only are of
interest.</td>
</tr>
<tr class="even">
<td>One-sample
<em>t</em>-test,
two-tailed</td>
<td>Tests whether
population mean is
equal to some
specified value
<span class="math inline">\(\mu_{0}\)</span> (null
hypothesis) or not
(alternative
hypothesis).</td>
</tr>
<tr class="odd">
<td>Two-sample
<em>t</em>-test,
one-tailed</td>
<td>Tests whether
population mean of a
one group (<span class="math inline">\(\mu_{1}\)</span>)
is less than, or
greater than, the
mean of another group
(<span class="math inline">\(\mu_{2}\)</span>). Only
appropriate when
differences in one
direction only are of
interest.</td>
</tr>
<tr class="even">
<td>Two-sample
<em>t</em>-test,
two-tailed</td>
<td>Tests whether the
means in two groups
(<span class="math inline">\(\mu_{1}\)</span> and
<span class="math inline">\(\mu_{2}\)</span>) are equal.
Can also be thought
of testing whether
difference in means
is equal to 0. Both
equal variance
(original) and
unequal variance
(Welch-Satterthwaite)
versions
exist…biologists
should use the
unequal variance
version. This is what
R does by default.</td>
</tr>
<tr class="odd">
<td>Paired
<em>t</em>-test,
one-tailed</td>
<td>Tests whether the
mean of differences
between paired
samples is less than,
or greater than, some
specified mean
difference <span class="math inline">\(D_{0}\)</span>.
Only appropriate when
differences in one
direction only are of
interest.</td>
</tr>
<tr class="even">
<td>Paired
<em>t</em>-test,
one-tailed</td>
<td>Tests whether the
mean of differences
between paired
samples is equal to
0.</td>
</tr>
</tbody>
</table>
<p>The number of “samples” in a <em>t</em>-test refers to how many sets of
observations are being compared. There can be either 1 sample (all
observations compared to some value) or 2 samples (2 sets of
observations compared to each other).</p>
<p>The number of “tails” refers to how sides (left (negative) and/or right
(positive)) are of interest in the test statistic distribution. In a
two-tailed test, or two-sided test, an extreme test statistic could fall
in either the right or the left tail of the distribution. The figure
below shows the range of <em>t</em> values which would be declared significant
for each kind of test. Notice that for a one-tailed test, the
significance region extends closer to the center, because its area must
be equal to the same <span class="math inline">\(\alpha\)</span> as a two-tailed test.</p>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-537-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>The <em>t</em>-test is usually one of the first methods taught because its use
cases are simple, and its test statistic is pretty easy to understand.
Consider the original <em>t</em> statistic introduced to the English statistics
literature by Gosset and popularized as “Student’s <em>t</em>” by Fisher:</p>
<p><span class="math display">\[
t=\frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{s_{x_1}^2+s_{x_2}^2}{2}}\sqrt{\frac{2}{n}}}
\]</span></p>
<p>The numerator is the quantity of interest: the difference in sample
means. It should make sense that the larger the difference in sample
means, the greater the probability that the difference will be
statistically significant. However, considering only the sample means
<span class="math inline">\(\bar{x_1}\)</span> and <span class="math inline">\(\bar{x_2}\)</span> is not enough. We also need to account for
variation within each sample, and for the total sample size. Variance
should <em>decrease</em> <em>t</em>, while sample size should increase it. Both terms
are put in the denominator so that the difference in means is relative
to them. Notice that in the denominator, the variances <span class="math inline">\(s_{x_1}^1\)</span> and
<span class="math inline">\(s_{x_2}^1\)</span> are in a numerator. Thus, increasing them will decrease <em>t</em>.
On the other hand, the sample size <em>n</em> is in a numerator in the
denominator (which is tantamount to being in the overall numerator).
Thus, increasing sample size will increase <em>t</em>.</p>
<p>But as elegant as Student’s <em>t</em> is, it has some drawbacks. It can
produce biased estimates and <em>p</em>-values when the two groups have unqual
sample sizes, or unequal variances. For these reasons, a more modern
version of the <em>t</em> statistic was developed by Welch in the 1940s. This
is sometimes called the “Welch’s <em>t</em>-test”, or less precisely the
“Welch-Satterthwaite <em>t</em>-test”. Satterthwaite’s contribution was not to
the <em>t</em> statistic, but rather an equation to better calculate the
degrees of freedom to use for the test<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a>.</p>
<p><span class="math display">\[
t=\frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{s_1}{\sqrt{n_1}}+\frac{s_2}{\sqrt{n2}}}}
\]</span> This newer version has the same numerator, but instead of pooling the
variances and sample sizes, keeps them separate. This allows for more
flexibility when dealing with unequal sample sizes or (mildly)
heteroscedastic data.</p>
</div>
<div id="comparing-3-or-more-groups" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Comparing 3 or more groups<a href="mod-09.html#comparing-3-or-more-groups" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="mod-09-anova" class="section level4 hasAnchor" number="9.4.4.1">
<h4><span class="header-section-number">9.4.4.1</span> Analysis of variance (ANOVA)<a href="mod-09.html#mod-09-anova" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Do the means differ between groups (with number of groups
<span class="math inline">\(\geq2\)</span>.</p>
<p><strong>Null hypothesis:</strong> All group means are equal.</p>
<p><strong>Alternative hypothesis:</strong> At least one group mean is different from
the others.</p>
<p><strong>Example use case:</strong> Marge needs to test whether cucumber mass is
greater in plants grown in potting mix, sandy soil, or clayey soil.</p>
<p><strong>Analysis of variance (ANOVA)</strong> is a special case of the linear model.
When people say “ANOVA” they usually mean the specific linear model that
compares the means of 3 or more groups. In this way it can be thought of
as an extension of the <em>t</em>-test from 2 groups to &gt;2, but this is
incorrect from a frequentist perspective because of how differently the
<em>t</em>-test and ANOVA arrive at a <em>P</em>-value<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a>. It’s better to
think of the frequentist <em>t</em>-test and ANOVA as cousins. Frequentist
ANOVA assesses statistical significance using the statistic <em>F</em>, which
is related to the amount of variance associated with different sources
of variance, relative to the number of degrees of freedom. This is
different from <em>t</em>, which is a difference in means scaled by variances
and sample sizes, <em>F</em>. Either way, <em>t</em> and <em>F</em> (and all test statistics)
are just contrivances to obtain a <em>P</em>-value.</p>
<p>The nature of the <em>F</em> statistic belies the other meaning of ANOVA: the
partitioning of variation into its different sources. In other words,
what proportion of variation in the <em>Y</em> variable does each factor
explain? How much variation is due to random chance (i.e., residual
variation)? ANOVA in this sense can be applied to many models, including
all linear models.</p>
<p>From the state-space linear modelling perspective (or Bayesian
perspective), the differences between the <em>t</em>-test and ANOVA are
trivial. Consider this model:</p>
<p><span class="math display">\[
y_i\sim Normal\left(\mu_{x_i},\ \sigma_{res.}\right)
\]</span></p>
<p>If <span class="math inline">\(\mu_{x_i}\)</span> is the mean for the group defined by <span class="math inline">\(x_i\)</span>, then what is
this model saying? It’s saying that the value of <em>y</em> for observation <em>i</em>
is drawn from a normal distribution with a mean that depends on group of
observation <em>i</em>, and some residual variation . This single equation
covers a lot of situations:</p>
<ul>
<li><p>If the number of groups is 1, then this is a one-sample <em>t</em>-test</p></li>
<li><p>If the number of groups is 2, then this is a two-sample <em>t</em>-test.</p></li>
<li><p>If the number of groups is ≥3, then this is a one-way ANOVA with 3
levels to its 1 factor (“way”).</p>
<ul>
<li>For example, if the number of groups is 4, then it could be a
two-way ANOVA, with two factors with 2 levels each.</li>
</ul></li>
<li><p>If the number of groups is 1, but the values represented paired
differences, then this is a paired <em>t</em>-test.</p></li>
</ul>
<p>In a typical frequentist ANOVA, one first conducts the <strong>omnibus test</strong>,
which assesses whether <u><strong><em>any</em></strong></u> of the group means differ
from each other. Then, a <strong>post-hoc test</strong> is performed to check
<u><strong><em>which</em></strong></u> groups differ from each other. The code below
illustrates this process in R using the built-in <code>iris</code> dataset.</p>
<div class="sourceCode" id="cb927"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb927-1"><a href="mod-09.html#cb927-1" tabindex="-1"></a><span class="co"># fit the linear model, which is what</span></span>
<span id="cb927-2"><a href="mod-09.html#cb927-2" tabindex="-1"></a><span class="co"># ANOVA really is.</span></span>
<span id="cb927-3"><a href="mod-09.html#cb927-3" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Species, <span class="at">data=</span>iris)</span>
<span id="cb927-4"><a href="mod-09.html#cb927-4" tabindex="-1"></a></span>
<span id="cb927-5"><a href="mod-09.html#cb927-5" tabindex="-1"></a><span class="co"># omnibus test:</span></span>
<span id="cb927-6"><a href="mod-09.html#cb927-6" tabindex="-1"></a><span class="fu">anova</span>(mod1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Petal.Length
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Species     2 437.10 218.551  1180.2 &lt; 2.2e-16 ***
## Residuals 147  27.22   0.185                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVA table shows us that the factor species uses 2 degrees of
freedom (variously abbreviated as DF, Df, or d.f.) to account for 437.10
of the squared errors; the residuals use 147 d.f. to account for 27.22
of the squared errors. The ratio of the mean squared error per d.f.
associated with each source, 218.551 / 0.185 = 1180.2, is the test
statistic <em>F</em>. This is used to calculate the <em>P</em>-value (much like <em>t</em>,
<em>F</em> follows a particular distribution defined by DF, which allows <span class="math inline">\(p(F)\)</span>
to be calculated). We can also see that about 94.1% of variation (437.10
/ (437.10+27.22)) is associated with species. This is the model’s
coefficient of determination, or <span class="math inline">\(R^2\)</span>. The <span class="math inline">\(R^2\)</span> tells you the
proportion of variation in <em>Y</em> explained by the model.</p>
<p>Remember that the all that the omnibus test <em>P</em>-value tells you is that
the means of at least one pair of species differ. To find out which pair
or pairs, we need to use a post-hoc test. My favorite is the Tukey’s
honest significant difference (HSD) test because it automatically
adjusts for multiple comparisons.</p>
<div class="sourceCode" id="cb929"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb929-1"><a href="mod-09.html#cb929-1" tabindex="-1"></a><span class="fu">TukeyHSD</span>(<span class="fu">aov</span>(mod1))</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = mod1)
## 
## $Species
##                       diff     lwr     upr p adj
## versicolor-setosa    2.798 2.59422 3.00178     0
## virginica-setosa     4.090 3.88622 4.29378     0
## virginica-versicolor 1.292 1.08822 1.49578     0</code></pre>
<p>This result shows us that all of the species differ from each other. We
can see this because all of the <em>P</em>-values (<code>p adj</code>) for the pairwise
comparisons are <span class="math inline">\(&lt;0.05\)</span> (they are printed by R as <code>0</code>, but are really
just approximated to very tiny value. Report them as “&lt;0.001” or
something like that). We can also see this because the 95% CI for the
pairwise differences do not include 0. For example, the difference in
means for species <code>versicolor</code> and <code>setosa</code> is <code>2.798</code>, with 95% CI =
[<code>2.59</code>, <code>3.00</code>]. The value <code>diff</code> is literally the mean of <code>versicolor</code>
minus the mean of <code>setosa</code>, as printed on the left of the table.</p>
</div>
<div id="way-2-way-and-more-ways-anova" class="section level4 hasAnchor" number="9.4.4.2">
<h4><span class="header-section-number">9.4.4.2</span> 1-way, 2-way, and more ways ANOVA<a href="mod-09.html#way-2-way-and-more-ways-anova" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the literature you often see ANOVA with a prefix like “1-way”,
“2-way”, “3-way”, etc. E.g., “two-way ANOVA”. The number of “ways”
indicates the number of <strong>factors</strong>, or <strong>grouping variables</strong>, being
tested in the analysis. This is independent of how many <strong>levels</strong>, or
<strong>unique values</strong>, each factor has. Factors may have different numbers
of levels. In a <strong>balanced</strong> design, each group (or combination of
groups) has the same number of observations. Balanced designs tend to be
more powerful than unbalanced designs, but unless the difference in the
number of observations is extreme it doesn’t matter very much.</p>
<p>The 2- or more-way ANOVAs may include interactions, where the effect of
one variable modifies, or interacts with, the effect of another
variable. These can be coded in R much more easily than they are
sometimes interpreted. For example, you can code a 2-way interaction
(interaction between 2 variables), 3-way interaction, or more-way, but
3-way and above interactions can be very difficult to interpret.</p>
<p>The example above (crossref) showed a 1-way ANOVA. Let’s add another
factor to <code>iris</code> and try a 2-way ANOVA.</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="mod-09.html#cb931-1" tabindex="-1"></a><span class="co"># spare copy of data frame</span></span>
<span id="cb931-2"><a href="mod-09.html#cb931-2" tabindex="-1"></a>dx <span class="ot">&lt;-</span> iris</span>
<span id="cb931-3"><a href="mod-09.html#cb931-3" tabindex="-1"></a></span>
<span id="cb931-4"><a href="mod-09.html#cb931-4" tabindex="-1"></a><span class="co"># add color variable with no effect</span></span>
<span id="cb931-5"><a href="mod-09.html#cb931-5" tabindex="-1"></a><span class="co"># repeats purple white purple white...for entire data frame</span></span>
<span id="cb931-6"><a href="mod-09.html#cb931-6" tabindex="-1"></a>dx<span class="sc">$</span>Color <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;purple&quot;</span>, <span class="st">&quot;white&quot;</span>)</span>
<span id="cb931-7"><a href="mod-09.html#cb931-7" tabindex="-1"></a></span>
<span id="cb931-8"><a href="mod-09.html#cb931-8" tabindex="-1"></a><span class="co"># 2-way ANOVA for effects of Species and Color</span></span>
<span id="cb931-9"><a href="mod-09.html#cb931-9" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Species<span class="sc">+</span>Color, <span class="at">data=</span>dx)</span>
<span id="cb931-10"><a href="mod-09.html#cb931-10" tabindex="-1"></a><span class="fu">anova</span>(mod2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Petal.Length
##            Df Sum Sq Mean Sq   F value Pr(&gt;F)    
## Species     2 437.10 218.551 1174.2292 &lt;2e-16 ***
## Color       1   0.05   0.049    0.2611 0.6101    
## Residuals 146  27.17   0.186                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVA table shows that color had no significant effect on petal
length. It is often worth checking for an interaction, whether you
suspect one from a biological perspective or not. The quickest way is
with an <strong>interaction plot</strong>.</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb933-1"><a href="mod-09.html#cb933-1" tabindex="-1"></a><span class="fu">interaction.plot</span>(dx<span class="sc">$</span>Color, dx<span class="sc">$</span>Species, dx<span class="sc">$</span>Petal.Length)</span></code></pre></div>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-541-1.png" width="672" /></p>
<p>Because the lines are essentially parallel, this plot shows no evidence
of interaction between color and species.</p>
<p>Let’s add another factor to dx, which does interact with species.</p>
<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb934-1"><a href="mod-09.html#cb934-1" tabindex="-1"></a><span class="co"># order by petal length within species</span></span>
<span id="cb934-2"><a href="mod-09.html#cb934-2" tabindex="-1"></a>dx <span class="ot">&lt;-</span> dx[<span class="fu">order</span>(dx<span class="sc">$</span>Species, dx<span class="sc">$</span>Petal.Length),]</span>
<span id="cb934-3"><a href="mod-09.html#cb934-3" tabindex="-1"></a>dx<span class="sc">$</span>rank.wi.spp <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span></span>
<span id="cb934-4"><a href="mod-09.html#cb934-4" tabindex="-1"></a></span>
<span id="cb934-5"><a href="mod-09.html#cb934-5" tabindex="-1"></a><span class="co"># for each species, make 25 smallest petalled plants</span></span>
<span id="cb934-6"><a href="mod-09.html#cb934-6" tabindex="-1"></a><span class="co"># in no fertilizer group, and 25 largest plants in </span></span>
<span id="cb934-7"><a href="mod-09.html#cb934-7" tabindex="-1"></a><span class="co"># fertilized group</span></span>
<span id="cb934-8"><a href="mod-09.html#cb934-8" tabindex="-1"></a>dx<span class="sc">$</span>Fertilizer <span class="ot">&lt;-</span> <span class="st">&quot;no&quot;</span></span>
<span id="cb934-9"><a href="mod-09.html#cb934-9" tabindex="-1"></a>dx<span class="sc">$</span>Fertilizer[<span class="fu">which</span>(dx<span class="sc">$</span>rank.wi.spp <span class="sc">&gt;=</span><span class="dv">26</span>)] <span class="ot">&lt;-</span> <span class="st">&quot;yes&quot;</span></span>
<span id="cb934-10"><a href="mod-09.html#cb934-10" tabindex="-1"></a></span>
<span id="cb934-11"><a href="mod-09.html#cb934-11" tabindex="-1"></a><span class="co"># reverse that pattern for one species</span></span>
<span id="cb934-12"><a href="mod-09.html#cb934-12" tabindex="-1"></a>flag <span class="ot">&lt;-</span> <span class="fu">which</span>(dx<span class="sc">$</span>Species <span class="sc">==</span> <span class="st">&quot;virginica&quot;</span> <span class="sc">&amp;</span></span>
<span id="cb934-13"><a href="mod-09.html#cb934-13" tabindex="-1"></a>    dx<span class="sc">$</span>rank.wi.spp <span class="sc">&lt;=</span> <span class="dv">25</span>)</span>
<span id="cb934-14"><a href="mod-09.html#cb934-14" tabindex="-1"></a>dx<span class="sc">$</span>Fertilizer[flag] <span class="ot">&lt;-</span> <span class="st">&quot;yes&quot;</span></span>
<span id="cb934-15"><a href="mod-09.html#cb934-15" tabindex="-1"></a></span>
<span id="cb934-16"><a href="mod-09.html#cb934-16" tabindex="-1"></a>flag <span class="ot">&lt;-</span> <span class="fu">which</span>(dx<span class="sc">$</span>Species <span class="sc">==</span> <span class="st">&quot;virginica&quot;</span> <span class="sc">&amp;</span></span>
<span id="cb934-17"><a href="mod-09.html#cb934-17" tabindex="-1"></a>    dx<span class="sc">$</span>rank.wi.spp <span class="sc">&gt;=</span> <span class="dv">26</span>)</span>
<span id="cb934-18"><a href="mod-09.html#cb934-18" tabindex="-1"></a>dx<span class="sc">$</span>Fertilizer[flag] <span class="ot">&lt;-</span> <span class="st">&quot;no&quot;</span></span>
<span id="cb934-19"><a href="mod-09.html#cb934-19" tabindex="-1"></a></span>
<span id="cb934-20"><a href="mod-09.html#cb934-20" tabindex="-1"></a><span class="co"># check for an interaction graphically</span></span>
<span id="cb934-21"><a href="mod-09.html#cb934-21" tabindex="-1"></a><span class="fu">interaction.plot</span>(dx<span class="sc">$</span>Fertilizer, dx<span class="sc">$</span>Species, dx<span class="sc">$</span>Petal.Length)</span></code></pre></div>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-542-1.png" width="672" /></p>
<p>In this plot, the line for one species, <code>virginica</code>, is decidedly not
parallel to the others. An intersection, or opposite sign slope, in an
interaction plot suggests that an interaction may be present. So, we
should test for the interaction. In the R formula system, an interaction
between two variables is specified with <code>*</code> instead of <code>+</code>.</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb935-1"><a href="mod-09.html#cb935-1" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length <span class="sc">~</span> Species <span class="sc">*</span> Fertilizer, <span class="at">data=</span>dx)</span>
<span id="cb935-2"><a href="mod-09.html#cb935-2" tabindex="-1"></a><span class="fu">anova</span>(mod3)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Petal.Length
##                     Df Sum Sq Mean Sq   F value Pr(&gt;F)    
## Species              2 437.10 218.551 3268.4656 &lt;2e-16 ***
## Fertilizer           1   0.07   0.073    1.0857 0.2992    
## Species:Fertilizer   2  17.52   8.761  131.0160 &lt;2e-16 ***
## Residuals          144   9.63   0.067                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The output shows that species and the interaction between species and
fertilizer significantly affected petal length. Even though the factor
Fertilizer appears nonsignificant, this does NOT mean that it can be
dropped from the model. Because of the way that interactions work, any
factor that included in an interaction term must be included by itself
as well. What the table is really telling us is that while fertilizer by
itself is not a significant factor, it <strong><em>is</em></strong> a significant factor
when the effect of species is considered too.</p>
<p>To interpret the interaction, we need to examine the coefficients of the
model:</p>
<div class="sourceCode" id="cb937"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb937-1"><a href="mod-09.html#cb937-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(mod3)<span class="sc">$</span>coefficients,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##                                 Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                        1.332      0.052  25.755    0.000
## Speciesversicolor                  2.552      0.073  34.892    0.000
## Speciesvirginica                   4.660      0.073  63.714    0.000
## Fertilizeryes                      0.260      0.073   3.555    0.001
## Speciesversicolor:Fertilizeryes    0.492      0.103   4.757    0.000
## Speciesvirginica:Fertilizeryes    -1.140      0.103 -11.021    0.000</code></pre>
<p>Here is how to find the group-level effects:</p>
<p><strong>Species = setosa, fertilizer = no:</strong> <code>1.332</code></p>
<p><strong>Species = versicolor, fertilizer = no:</strong> <code>1.332 + 2.552</code> = <code>3.884</code></p>
<p><strong>Species = virginica, fertilizer = no:</strong> <code>1.332 + 4.660</code> = <code>5.992</code></p>
<p><strong>Species = setosa, fertilizer = yes:</strong> <code>1.332 + 0.260</code> = <code>1.592</code></p>
<p><strong>Species = versicolor, fertilizer = yes:</strong>
<code>(1.332 + 2.552) + (0.260 + 0.492)</code> = <code>4.636</code></p>
<p><strong>Species = virginica, fertilizer = yes:</strong>
<code>(1.332 + 4.660) + (0.260 - 1.140)</code> = <code>5.112</code></p>
<p>The figure below shows what the interaction is really doing.</p>
<div class="sourceCode" id="cb939"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb939-1"><a href="mod-09.html#cb939-1" tabindex="-1"></a><span class="fu">boxplot</span>(Petal.Length <span class="sc">~</span> Fertilizer<span class="sc">*</span>Species, <span class="at">data=</span>dx,</span>
<span id="cb939-2"><a href="mod-09.html#cb939-2" tabindex="-1"></a>    <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;grey70&quot;</span>, <span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-545-1.png" width="864" /></p>
<p>This figure shows that in species <code>setosa</code> and <code>versicolor</code>, the effect
of fertilizer on petal length is positive. However, the reverse is true
for <code>virginica</code>.</p>
<p>We can add as many factors as we like to an ANOVA, within reason. Just
remember that <code>+</code> codes an additive effect (no interaction), while <code>*</code>
codes an interaction (one variable affects the effect of another
variable). Here are some additional examples:</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb940-1"><a href="mod-09.html#cb940-1" tabindex="-1"></a><span class="co"># 3-way ANOVA with no interactions</span></span>
<span id="cb940-2"><a href="mod-09.html#cb940-2" tabindex="-1"></a>mod5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length <span class="sc">~</span> Fertilizer <span class="sc">+</span> Species <span class="sc">+</span> Color, <span class="at">data=</span>dx)</span>
<span id="cb940-3"><a href="mod-09.html#cb940-3" tabindex="-1"></a></span>
<span id="cb940-4"><a href="mod-09.html#cb940-4" tabindex="-1"></a><span class="co"># 3-way ANOVA with 1 2-way interaction:</span></span>
<span id="cb940-5"><a href="mod-09.html#cb940-5" tabindex="-1"></a>mod6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length <span class="sc">~</span> Fertilizer <span class="sc">*</span> Species <span class="sc">+</span> Color, <span class="at">data=</span>dx)</span>
<span id="cb940-6"><a href="mod-09.html#cb940-6" tabindex="-1"></a></span>
<span id="cb940-7"><a href="mod-09.html#cb940-7" tabindex="-1"></a><span class="co"># 3-way ANOVA with 2 2-way interactions:</span></span>
<span id="cb940-8"><a href="mod-09.html#cb940-8" tabindex="-1"></a>mod7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length <span class="sc">~</span> Fertilizer <span class="sc">*</span> Species <span class="sc">+</span> </span>
<span id="cb940-9"><a href="mod-09.html#cb940-9" tabindex="-1"></a>    Color <span class="sc">*</span> Species, <span class="at">data=</span>dx)</span>
<span id="cb940-10"><a href="mod-09.html#cb940-10" tabindex="-1"></a></span>
<span id="cb940-11"><a href="mod-09.html#cb940-11" tabindex="-1"></a><span class="co"># 3-way ANOVA with a 3-way interaction (hard to make sense of):</span></span>
<span id="cb940-12"><a href="mod-09.html#cb940-12" tabindex="-1"></a>mod8 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length <span class="sc">~</span> Fertilizer <span class="sc">*</span> Species <span class="sc">*</span> Color, <span class="at">data=</span>dx)</span></code></pre></div>
</div>
<div id="mod-09-kwtest" class="section level4 hasAnchor" number="9.4.4.3">
<h4><span class="header-section-number">9.4.4.3</span> Kruskal-Wallis test<a href="mod-09.html#mod-09-kwtest" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Are values in one or more groups consistently greater or
less than values in another group? (# of groups <span class="math inline">\(\geq3\)</span>); extends
Wilcoxon test to more groups)</p>
<p><strong>Null hypothesis:</strong> For randomly selected values <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> from any 2
groups, <span class="math inline">\(p\left(x&gt;y\right)=p\left(y&gt;x\right)\)</span>.</p>
<p><strong>Alternative hypothesis:</strong> For randomly selected values <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>
from any 2 groups, <span class="math inline">\(p\left(x&gt;y\right)\neq p\left(y&gt;x\right)\)</span></p>
<p><strong>Example use case:</strong> Bart is studying whether tomato yields tend to be
greater in plants treated with no pesticide, pesticide A, or pesticide
B. He is not interested in predicting the actual tomato yield.</p>
<p>Because ANOVA is a linear model, the data used in ANOVA must meet the
<a href="mod-09.html#mod-09-linear-assume">assumptions of the linear model</a>. If the data do
not meet the assumptions, or cannot be transformed to do so, then a
nonparametric alternative to ANOVA is needed. That alternative is the
<strong>Kruskal-Wallis test</strong>. Some texts refer to this as a “nonparametric
ANOVA”, but this is not correct because at no point is variance of any
actually partitioned. Like the Wilcoxon test, the Kruskal-Wallis test is
based on ranks of the data. A significant Kruskal-Wallis test indicates
that values in at least one group are consistently greater than values
in at least one other group. As with ANOVA, a post-hoc test is needed to
determine which groups differ significantly from each other.</p>
<p><strong>Example Kruskal-Wallis test in R</strong></p>
<div class="sourceCode" id="cb941"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb941-1"><a href="mod-09.html#cb941-1" tabindex="-1"></a><span class="fu">kruskal.test</span>(Petal.Length<span class="sc">~</span>Species, <span class="at">data=</span>iris)</span></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  Petal.Length by Species
## Kruskal-Wallis chi-squared = 130.41, df = 2, p-value &lt; 2.2e-16</code></pre>
<p>The significant result means that petal length is consistently greater
for at least species compared to the other. This is analogous to the
omnibus ANOVA test. If we want to know which groups differ, we need a
post-hoc test. There are 2 good options: Dunn’s test, and a series of
pairwise Wilcoxon tests.</p>
<p><strong>Kruskal-Wallis post-hoc option 1: Dunn’s test</strong></p>
<div class="sourceCode" id="cb943"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb943-1"><a href="mod-09.html#cb943-1" tabindex="-1"></a><span class="fu">library</span>(dunn.test)</span></code></pre></div>
<pre><code>## Warning: package &#39;dunn.test&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb945"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb945-1"><a href="mod-09.html#cb945-1" tabindex="-1"></a><span class="fu">dunn.test</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Species, <span class="at">method=</span><span class="st">&quot;bonferroni&quot;</span>)</span></code></pre></div>
<pre><code>##   Kruskal-Wallis rank sum test
## 
## data: x and group
## Kruskal-Wallis chi-squared = 130.411, df = 2, p-value = 0
## 
## 
##                            Comparison of x by group                            
##                                  (Bonferroni)                                  
## Col Mean-|
## Row Mean |     setosa   versicol
## ---------+----------------------
## versicol |  -5.862996
##          |    0.0000*
##          |
## virginic |  -11.41838  -5.555388
##          |    0.0000*    0.0000*
## 
## alpha = 0.05
## Reject Ho if p &lt;= alpha/2</code></pre>
<p>Interpretation: A posthoc Dunn’s test showed that all three species
differed from each other in terms of the rank order of petal lengths.</p>
<p><strong>Kruskal-Wallis post-hoc option 2: Pairwise Wilcoxon tests</strong></p>
<p>I’m not aware of an “off-the-shelf” solution for conducting post-hoc
pairwise Wilcoxon tests, but fortunately it’s pretty straightforward to
do in a <code>for()</code> loop:</p>
<div class="sourceCode" id="cb947"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb947-1"><a href="mod-09.html#cb947-1" tabindex="-1"></a>x.levs <span class="ot">&lt;-</span> <span class="fu">levels</span>(iris<span class="sc">$</span>Species)</span>
<span id="cb947-2"><a href="mod-09.html#cb947-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">t</span>(<span class="fu">combn</span>(x.levs, <span class="dv">2</span>)))</span>
<span id="cb947-3"><a href="mod-09.html#cb947-3" tabindex="-1"></a>res<span class="sc">$</span>diff <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb947-4"><a href="mod-09.html#cb947-4" tabindex="-1"></a>res<span class="sc">$</span>lwr <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb947-5"><a href="mod-09.html#cb947-5" tabindex="-1"></a>res<span class="sc">$</span>upr <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb947-6"><a href="mod-09.html#cb947-6" tabindex="-1"></a>res<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb947-7"><a href="mod-09.html#cb947-7" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(res)){</span>
<span id="cb947-8"><a href="mod-09.html#cb947-8" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> iris<span class="sc">$</span>Petal.Length[<span class="fu">which</span>(iris<span class="sc">$</span>Species <span class="sc">==</span> res<span class="sc">$</span>X2[i])]</span>
<span id="cb947-9"><a href="mod-09.html#cb947-9" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> iris<span class="sc">$</span>Petal.Length[<span class="fu">which</span>(iris<span class="sc">$</span>Species <span class="sc">==</span> res<span class="sc">$</span>X1[i])]</span>
<span id="cb947-10"><a href="mod-09.html#cb947-10" tabindex="-1"></a>    wtest <span class="ot">&lt;-</span> <span class="fu">wilcox.test</span>(x1,x2,<span class="at">conf.int=</span><span class="cn">TRUE</span>)</span>
<span id="cb947-11"><a href="mod-09.html#cb947-11" tabindex="-1"></a>    res<span class="sc">$</span>diff[i] <span class="ot">&lt;-</span> wtest<span class="sc">$</span>estimate</span>
<span id="cb947-12"><a href="mod-09.html#cb947-12" tabindex="-1"></a>    res<span class="sc">$</span>lwr[i] <span class="ot">&lt;-</span> wtest<span class="sc">$</span>conf.int[<span class="dv">1</span>]</span>
<span id="cb947-13"><a href="mod-09.html#cb947-13" tabindex="-1"></a>    res<span class="sc">$</span>upr[i] <span class="ot">&lt;-</span> wtest<span class="sc">$</span>conf.int[<span class="dv">2</span>]</span>
<span id="cb947-14"><a href="mod-09.html#cb947-14" tabindex="-1"></a>    res<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> wtest<span class="sc">$</span>p.value</span>
<span id="cb947-15"><a href="mod-09.html#cb947-15" tabindex="-1"></a>}<span class="co">#i</span></span>
<span id="cb947-16"><a href="mod-09.html#cb947-16" tabindex="-1"></a><span class="co"># mimics a TukeyHSD result:</span></span>
<span id="cb947-17"><a href="mod-09.html#cb947-17" tabindex="-1"></a>res</span></code></pre></div>
<pre><code>##           X1         X2     diff      lwr      upr            p
## 1     setosa versicolor 2.899993 2.699949 2.999986 5.651012e-18
## 2     setosa  virginica 4.099992 3.899975 4.200058 5.665214e-18
## 3 versicolor  virginica 1.200054 1.000018 1.499973 9.133545e-17</code></pre>
<p>Because we conducted 3 tests using the same variables—i.e., in the same
“family”—we need to adjust the significance level α that we use for
significance. The most common adjustment is the <strong>Bonferroni
correction</strong>: dividing α by the number of tests in the family. In this
case, that means dividing α by 3, such that only <span class="math inline">\(P&lt;0.05/3=0.0166\)</span> are
considered significant. Some people argue that the Bonferroni correction
is too conservative—i.e., makes significant results less likely than
they should be—but it works well enough for enough people that it’s
worth considering.</p>
</div>
</div>
<div id="ordered-factors-and-polynomial-patterns-in-grouped-data" class="section level3 hasAnchor" number="9.4.5">
<h3><span class="header-section-number">9.4.5</span> Ordered factors and polynomial patterns in grouped data<a href="mod-09.html#ordered-factors-and-polynomial-patterns-in-grouped-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="ordering-factors" class="section level4 hasAnchor" number="9.4.5.1">
<h4><span class="header-section-number">9.4.5.1</span> Ordering factors<a href="mod-09.html#ordering-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We saw <a href="mod-09.html#mod-09-anova">above</a> how ANOVA test for differences in means between groups; i.e., between <strong>levels</strong> of a <strong>factor</strong>. In the examples we used, the levels of the factors were just group identities (“species A”, “species B”, and so on; or “herbivore” vs. “carnivore” vs. “omnivore”). These group identities have no inherent ordering to them, and your biological conclusions should be the same no matter which group comes “first” (or, leftmost on the plot, or whatever). R will, by default, sort factor levels automatically, and that’s okay. This is why, in the ANOVA examples above, species <code>setosa</code> was treated as the baseline and labeled as the <code>(Intercept)</code>.</p>
<p>We can reorder the levels of a factor to force R to treat the right level as the baseline. For example, consider a dataset with 1 factor, with levels <code>control</code>, <code>chemical A</code>, and <code>chemical B</code>. R would treat <code>chemical a</code> as the baseline, because it is first alphabetically. You can override this by setting <code>control</code> as the first level:</p>
<div class="sourceCode" id="cb949"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb949-1"><a href="mod-09.html#cb949-1" tabindex="-1"></a><span class="co"># spare copy</span></span>
<span id="cb949-2"><a href="mod-09.html#cb949-2" tabindex="-1"></a>iris2 <span class="ot">&lt;-</span> iris</span>
<span id="cb949-3"><a href="mod-09.html#cb949-3" tabindex="-1"></a></span>
<span id="cb949-4"><a href="mod-09.html#cb949-4" tabindex="-1"></a><span class="co"># assign new factor</span></span>
<span id="cb949-5"><a href="mod-09.html#cb949-5" tabindex="-1"></a>iris2<span class="sc">$</span>chemical.expose <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;control&quot;</span>, <span class="st">&quot;chemicala&quot;</span>, <span class="st">&quot;chemicalb&quot;</span>)</span>
<span id="cb949-6"><a href="mod-09.html#cb949-6" tabindex="-1"></a></span>
<span id="cb949-7"><a href="mod-09.html#cb949-7" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb949-8"><a href="mod-09.html#cb949-8" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>chemical.expose, <span class="at">data=</span>iris2)</span>
<span id="cb949-9"><a href="mod-09.html#cb949-9" tabindex="-1"></a></span>
<span id="cb949-10"><a href="mod-09.html#cb949-10" tabindex="-1"></a><span class="co"># view coefficients...note that &quot;control&quot; is not the baseline!</span></span>
<span id="cb949-11"><a href="mod-09.html#cb949-11" tabindex="-1"></a><span class="fu">summary</span>(mod1)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                          Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)                 3.732  0.2512478 14.85385995 4.634369e-31
## chemical.exposechemicalb    0.094  0.3553181  0.26455169 7.917254e-01
## chemical.exposecontrol     -0.016  0.3553181 -0.04503008 9.641444e-01</code></pre>
<p>The output uses the alphabetically first <code>chemicala</code> as the baseline, which is not as convenient.</p>
<div class="sourceCode" id="cb951"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb951-1"><a href="mod-09.html#cb951-1" tabindex="-1"></a><span class="co"># reorder factor</span></span>
<span id="cb951-2"><a href="mod-09.html#cb951-2" tabindex="-1"></a>iris2<span class="sc">$</span>chemical.expose <span class="ot">&lt;-</span> <span class="fu">factor</span>(iris2<span class="sc">$</span>chemical.expose,</span>
<span id="cb951-3"><a href="mod-09.html#cb951-3" tabindex="-1"></a>                                <span class="at">levels=</span><span class="fu">c</span>(<span class="st">&quot;control&quot;</span>, <span class="st">&quot;chemicala&quot;</span>, <span class="st">&quot;chemicalb&quot;</span>))</span>
<span id="cb951-4"><a href="mod-09.html#cb951-4" tabindex="-1"></a></span>
<span id="cb951-5"><a href="mod-09.html#cb951-5" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb951-6"><a href="mod-09.html#cb951-6" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>chemical.expose, <span class="at">data=</span>iris2)</span>
<span id="cb951-7"><a href="mod-09.html#cb951-7" tabindex="-1"></a></span>
<span id="cb951-8"><a href="mod-09.html#cb951-8" tabindex="-1"></a><span class="co"># view coefficients...note that &quot;control&quot; is not the baseline!</span></span>
<span id="cb951-9"><a href="mod-09.html#cb951-9" tabindex="-1"></a><span class="fu">summary</span>(mod2)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                          Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)                 3.716  0.2512478 14.79017781 6.777276e-31
## chemical.exposechemicala    0.016  0.3553181  0.04503008 9.641444e-01
## chemical.exposechemicalb    0.110  0.3553181  0.30958177 7.573174e-01</code></pre>
</div>
<div id="ordered-factors" class="section level4 hasAnchor" number="9.4.5.2">
<h4><span class="header-section-number">9.4.5.2</span> Ordered factors<a href="mod-09.html#ordered-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Some factors have an inherent ordering to their levels. For example, in a toxicology trial you might have “none”, “low”, “medium”, and “high” as exposure levels. Or, you could have have “larva”, “pupa”, and “adult” as levels in a life history model. When analyzing these factors, we should take into account not only the differences between the groups, but the fact that the groups are in some kind of order. Consider the boxplots below:</p>
<div class="sourceCode" id="cb953"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb953-1"><a href="mod-09.html#cb953-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">52</span>)</span>
<span id="cb953-2"><a href="mod-09.html#cb953-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb953-3"><a href="mod-09.html#cb953-3" tabindex="-1"></a>dx1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb953-4"><a href="mod-09.html#cb953-4" tabindex="-1"></a>  <span class="at">stage=</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Instar 2&quot;</span>, <span class="st">&quot;Instar 5&quot;</span>, <span class="st">&quot;Instar 9&quot;</span>), <span class="at">each=</span>n),</span>
<span id="cb953-5"><a href="mod-09.html#cb953-5" tabindex="-1"></a>  <span class="at">wing=</span><span class="fu">c</span>(<span class="fu">rnorm</span>(n, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n, <span class="dv">8</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n, <span class="dv">12</span>, <span class="dv">1</span>)))</span>
<span id="cb953-6"><a href="mod-09.html#cb953-6" tabindex="-1"></a></span>
<span id="cb953-7"><a href="mod-09.html#cb953-7" tabindex="-1"></a>dx2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">runif</span>(<span class="dv">150</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb953-8"><a href="mod-09.html#cb953-8" tabindex="-1"></a>dx2<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="dv">9</span> <span class="sc">-</span> (<span class="fl">1.42</span><span class="sc">*</span>dx2<span class="sc">$</span>x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">150</span>, <span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb953-9"><a href="mod-09.html#cb953-9" tabindex="-1"></a>dx2<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">round</span>(dx2<span class="sc">$</span>x, <span class="dv">0</span>)</span>
<span id="cb953-10"><a href="mod-09.html#cb953-10" tabindex="-1"></a></span>
<span id="cb953-11"><a href="mod-09.html#cb953-11" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="fl">5.1</span>, <span class="fl">5.1</span>, <span class="fl">1.1</span>, <span class="fl">1.1</span>), <span class="at">lend=</span><span class="dv">1</span>, <span class="at">las=</span><span class="dv">1</span>,</span>
<span id="cb953-12"><a href="mod-09.html#cb953-12" tabindex="-1"></a>    <span class="at">bty=</span><span class="st">&quot;n&quot;</span>, <span class="at">cex.axis=</span><span class="fl">1.2</span>, <span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb953-13"><a href="mod-09.html#cb953-13" tabindex="-1"></a><span class="fu">boxplot</span>(wing<span class="sc">~</span>stage, <span class="at">data=</span>dx1, <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,</span>
<span id="cb953-14"><a href="mod-09.html#cb953-14" tabindex="-1"></a>  <span class="at">xlab=</span><span class="st">&quot;Development stage&quot;</span>,</span>
<span id="cb953-15"><a href="mod-09.html#cb953-15" tabindex="-1"></a>  <span class="at">ylab=</span><span class="st">&quot;Wing length (mm)&quot;</span>)</span>
<span id="cb953-16"><a href="mod-09.html#cb953-16" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">labels=</span><span class="fu">unique</span>(dx1<span class="sc">$</span>stage))</span>
<span id="cb953-17"><a href="mod-09.html#cb953-17" tabindex="-1"></a></span>
<span id="cb953-18"><a href="mod-09.html#cb953-18" tabindex="-1"></a><span class="fu">boxplot</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dx2, <span class="at">ylab=</span><span class="st">&quot;Growth (residual)&quot;</span>,</span>
<span id="cb953-19"><a href="mod-09.html#cb953-19" tabindex="-1"></a>  <span class="at">xlab=</span><span class="st">&quot;Distance bin&quot;</span>)</span></code></pre></div>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-552-1.png" width="768" /></p>
<p>In the left plot, we see that wing length increases roughly linearly from the 2nd through the 9th instar stage. In the right plot, we see that residuals of plant growth tend to be greatest in the distance bins close to 0, and negative in bins farther from 0. In fact, it kind of looks like a parabola (i.e., a quadratic function).</p>
<p>We could just naively test for differences between groups, but that wouldn’t tell us the whole story. If we tell R that the factors are ordered, we can request <strong>polynomial contrasts</strong> to test for these patterns we think we see in the boxplots. These would be a linear pattern in the left figure, and a quadratic pattern in the right figure. With an <strong>ordered factor</strong>, R will automatically test for polynomial patterns between groups. R will test for polynomials of order up to <span class="math inline">\(n-1\)</span>, where <span class="math inline">\(n\)</span> is the number of groups. So, if there are 3 groups, R will test polynomial patterns of order 1 (linear) and 2 (quadratic).</p>
<div class="sourceCode" id="cb954"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb954-1"><a href="mod-09.html#cb954-1" tabindex="-1"></a><span class="co"># for the left plot:</span></span>
<span id="cb954-2"><a href="mod-09.html#cb954-2" tabindex="-1"></a>dx1<span class="sc">$</span>stage <span class="ot">&lt;-</span> <span class="fu">factor</span>(dx1<span class="sc">$</span>stage, <span class="at">ordered=</span><span class="cn">TRUE</span>)</span>
<span id="cb954-3"><a href="mod-09.html#cb954-3" tabindex="-1"></a></span>
<span id="cb954-4"><a href="mod-09.html#cb954-4" tabindex="-1"></a><span class="co"># not run, but look at bottom to see how dx1$stage is treated</span></span>
<span id="cb954-5"><a href="mod-09.html#cb954-5" tabindex="-1"></a><span class="co"># dx1$stage</span></span>
<span id="cb954-6"><a href="mod-09.html#cb954-6" tabindex="-1"></a></span>
<span id="cb954-7"><a href="mod-09.html#cb954-7" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(wing<span class="sc">~</span>stage, <span class="at">data=</span>dx1))<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##               Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)  8.0068921  0.1544031 51.857078 3.321267e-33
## stage.L      5.7424515  0.2674340 21.472409 5.775273e-21
## stage.Q     -0.2734039  0.2674340 -1.022323 3.140644e-01</code></pre>
<p>Here is the test for the right plot:</p>
<div class="sourceCode" id="cb956"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb956-1"><a href="mod-09.html#cb956-1" tabindex="-1"></a><span class="co"># for the right plot:</span></span>
<span id="cb956-2"><a href="mod-09.html#cb956-2" tabindex="-1"></a>dx2<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">factor</span>(dx2<span class="sc">$</span>x, <span class="at">ordered=</span><span class="cn">TRUE</span>)</span>
<span id="cb956-3"><a href="mod-09.html#cb956-3" tabindex="-1"></a></span>
<span id="cb956-4"><a href="mod-09.html#cb956-4" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dx2))<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##               Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)  3.7120832  0.3664558 10.1296885 1.632377e-18
## x.L         -2.3515982  1.1193212 -2.1009145 3.740319e-02
## x.Q         -9.0399262  1.0901508 -8.2923629 7.412107e-14
## x.C         -0.7680379  0.9997669 -0.7682170 4.436250e-01
## x^4          1.6784822  0.8863823  1.8936325 6.029373e-02
## x^5         -0.6478629  0.8638215 -0.7499963 4.544891e-01
## x^6         -0.3048916  0.8169887 -0.3731895 7.095601e-01</code></pre>
<p>Just because R will test for up to <span class="math inline">\(n-1\)</span> contrasts, doesn’t mean that you want them. For example, you might only want test for a linear trend in wing length in <code>dx1</code> above. You can do this by manually setting the number of contrasts for a factor, from the set of polynomial contrasts of proper size.</p>
<div class="sourceCode" id="cb958"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb958-1"><a href="mod-09.html#cb958-1" tabindex="-1"></a><span class="co"># for the wings in dx1:</span></span>
<span id="cb958-2"><a href="mod-09.html#cb958-2" tabindex="-1"></a><span class="do">## how.many=1 --&gt; linear contrast only</span></span>
<span id="cb958-3"><a href="mod-09.html#cb958-3" tabindex="-1"></a><span class="do">## contr.poly(3) is polynomial contrast matrix with 3 levels</span></span>
<span id="cb958-4"><a href="mod-09.html#cb958-4" tabindex="-1"></a><span class="fu">contrasts</span>(dx1<span class="sc">$</span>stage, <span class="at">how.many=</span><span class="dv">1</span>) <span class="ot">&lt;-</span> <span class="fu">contr.poly</span>(<span class="dv">3</span>)</span>
<span id="cb958-5"><a href="mod-09.html#cb958-5" tabindex="-1"></a></span>
<span id="cb958-6"><a href="mod-09.html#cb958-6" tabindex="-1"></a><span class="co"># rerun the test:</span></span>
<span id="cb958-7"><a href="mod-09.html#cb958-7" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(wing<span class="sc">~</span>stage, <span class="at">data=</span>dx1))<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##             Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 8.006892  0.1545055 51.82268 6.083708e-34
## stage.L     5.742451  0.2676115 21.45817 2.413280e-21</code></pre>
<p>And here is the same procedure for the growth residuals in <code>dx2</code>:</p>
<div class="sourceCode" id="cb960"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb960-1"><a href="mod-09.html#cb960-1" tabindex="-1"></a><span class="co"># for the growth residuals in dx2:</span></span>
<span id="cb960-2"><a href="mod-09.html#cb960-2" tabindex="-1"></a><span class="do">## how.many=2 --&gt; up to quadratic pattern only</span></span>
<span id="cb960-3"><a href="mod-09.html#cb960-3" tabindex="-1"></a><span class="do">## contr.poly(7) is polynomial contrast matrix with 7 levels</span></span>
<span id="cb960-4"><a href="mod-09.html#cb960-4" tabindex="-1"></a><span class="fu">contrasts</span>(dx2<span class="sc">$</span>x, <span class="at">how.many=</span><span class="dv">2</span>) <span class="ot">&lt;-</span> <span class="fu">contr.poly</span>(<span class="dv">7</span>)</span>
<span id="cb960-5"><a href="mod-09.html#cb960-5" tabindex="-1"></a></span>
<span id="cb960-6"><a href="mod-09.html#cb960-6" tabindex="-1"></a><span class="co"># rerun the test:</span></span>
<span id="cb960-7"><a href="mod-09.html#cb960-7" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dx2))<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)  3.619456  0.3624434  9.986264 3.020970e-18
## x.L         -2.009843  1.0507597 -1.912752 5.772443e-02
## x.Q         -9.629178  1.0527928 -9.146318 4.496049e-16</code></pre>
</div>
</div>
</div>
<div id="tests-for-patterns-in-continuous-data" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Tests for patterns in continuous data<a href="mod-09.html#tests-for-patterns-in-continuous-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="mod-09-correlation" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Correlation<a href="mod-09.html#mod-09-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Correlation</strong> is a statistical relationship where changes in one
variable are related to, or associated with, changes in another
variable. This relationship need not be causal, hence the famous mantra,
“correlation does not equal causation”. As such, correlation is usually
a <strong>descriptive</strong> technique, rather than an <strong>inferential</strong> one. The
difference is that a descriptive statistic summarizes a pattern without
assuming anything about a casual mechanism; whereas inferential
statistics are designed to let researchers infer or predict the
underlying pattern.</p>
<p><strong>Correlation coefficients</strong> are often calculated when we want to see if
two values are related, without going as far as to infer a causal
relationship. Because science is all about finding explanations (i.e.,
causes) for natural phenomena, a correlation analysis is usually not
presented on its own. However, correlation analysis can be part of a
larger workflow:</p>
<ul>
<li><p>Correlations can indicate if some variables in a dataset are
<em>redundant</em>—that is, any analysis of one would say the same as
analyzing the other.</p></li>
<li><p>Correlations can indicate association without inference—e.g., it
might be sufficient to know that your residuals are correlated with
some other variable, without being able to model the relationship.</p></li>
<li><p>Correlations can help identify proxy variables—variables that can
stand in for one another. E.g., correlation analysis may reveal that
an inexpensive, but widely available variable is a good stand-in for
an expensive, but less available one.</p></li>
<li><p>Correlations can screen large numbers of variables for possible
patterns and inform future investigations. For example, in my
workflow I will often start by testing many variables for
correlations, and only fitting inferential models (e.g., GLMs) for
the variables that show some significant correlation with the
response variable.</p></li>
</ul>
<p>Correlations are typically expressed as a correlation coefficient, which
is usually scaled such that 0 indicates no correlation; positive values
approaching 1 indicate a positive correlation; and negative values
approaching -1 indicate a negative correlation. The sign of a
correlation coefficient is related to the slope of a line of best fit.</p>
<ul>
<li><p><strong>Positive correlation:</strong> as one variable gets larger, the other
variable does too.</p></li>
<li><p><strong>Negative correlation:</strong> as one variable gets larger, the other
variable gets smaller.</p></li>
</ul>
<p>The magnitude of the correlation coefficient says something about its
strength or consistency. Small values close to 0 indicate a weak or
inconsistent correlation; values closer to -1 or 1 indicate a strong or
consistent correlation.</p>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-557-1.png" width="672" /></p>
<div id="mod-09-linear-correl" class="section level4 hasAnchor" number="9.5.1.1">
<h4><span class="header-section-number">9.5.1.1</span> Linear correlation (Pearson’s <em>r</em>)<a href="mod-09.html#mod-09-linear-correl" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> As values in one set change, do values in another set
change at a constant rate?</p>
<p><strong>Null hypothesis:</strong> N/A (descriptive statistic)</p>
<p><strong>Alternative hypothesis:</strong> N/A (descriptive statistic)</p>
<p><strong>Example use case:</strong> Lisa is studying whether the size of a mammal’s
cecum is related to the proportion of plant material in its diet, and
suspects that the relationship is linear.</p>
<p>The most common correlation coefficient is probably <strong>Pearson’s product
moment coefficient</strong>, or <strong>Pearson’s <em>r</em></strong>. This describes the strength
and direction of a <strong>linear correlation</strong> between two variables. It can
be calculated in <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#For_a_sample">many
ways</a>;
my favorite is shown below:</p>
<p><span class="math display">\[
r_{xy}=\frac{\sum{x_iy_i}-n\bar{x}\bar{y}}{\left(n-1\right)s_xs_y}
\]</span></p>
<p>In this expression, <em>r<sub>xy</sub></em> is the correlation between vectors <em>x</em> and
<em>y</em>; <em>n</em> is the total number of observations; <em>x<sub>i</sub></em> and <em>y<sub>i</sub></em> are the
<em>i</em>-th value of <em>x</em> and <em>y</em>, respectively; <em>x̅</em> and <em>y̅</em> are the sample
means of <em>x</em> and <em>y</em>; and <em>s<sub>x</sub></em> and <em>s<sub>y</sub></em> are the sample SD of <em>x</em> and
<em>y</em>. This expression shows that <em>r</em> gets larger as the slope of the
relationship between <em>x</em> and <em>y</em> gets larger, regardless of sign; and
that increasing variance decreases <em>r</em>.</p>
<p><strong>Example linear correlations in R:</strong></p>
<div class="sourceCode" id="cb962"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb962-1"><a href="mod-09.html#cb962-1" tabindex="-1"></a><span class="co"># correlation between two variables</span></span>
<span id="cb962-2"><a href="mod-09.html#cb962-2" tabindex="-1"></a><span class="do">## just the coefficient:</span></span>
<span id="cb962-3"><a href="mod-09.html#cb962-3" tabindex="-1"></a><span class="fu">cor</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width)</span></code></pre></div>
<pre><code>## [1] 0.9628654</code></pre>
<p>You can also request a significance test on the coefficient.</p>
<div class="sourceCode" id="cb964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb964-1"><a href="mod-09.html#cb964-1" tabindex="-1"></a><span class="do">## significance test on the coefficient</span></span>
<span id="cb964-2"><a href="mod-09.html#cb964-2" tabindex="-1"></a><span class="do">## i.e., is the coefficient diff. from 0?</span></span>
<span id="cb964-3"><a href="mod-09.html#cb964-3" tabindex="-1"></a><span class="fu">cor.test</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  iris$Petal.Length and iris$Petal.Width
## t = 43.387, df = 148, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.9490525 0.9729853
## sample estimates:
##       cor 
## 0.9628654</code></pre>
<p>You can also get correlations between a set of variables. I.e., a
correlation matrix.</p>
<div class="sourceCode" id="cb966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb966-1"><a href="mod-09.html#cb966-1" tabindex="-1"></a><span class="co"># correlations between several variables at once:</span></span>
<span id="cb966-2"><a href="mod-09.html#cb966-2" tabindex="-1"></a><span class="co"># i.e., a correlation matrix</span></span>
<span id="cb966-3"><a href="mod-09.html#cb966-3" tabindex="-1"></a><span class="fu">cor</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000</code></pre>
<p>There isn’t a built-in way to check for <u>significant</u>
correlations between many pairs of variables at once. This is because
<code>cor.test()</code> only works with one pair at a time. Fortunately, it’s not
hard to do in a loop:</p>
<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb968-1"><a href="mod-09.html#cb968-1" tabindex="-1"></a><span class="co"># data frame we need</span></span>
<span id="cb968-2"><a href="mod-09.html#cb968-2" tabindex="-1"></a>dx <span class="ot">&lt;-</span> iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb968-3"><a href="mod-09.html#cb968-3" tabindex="-1"></a></span>
<span id="cb968-4"><a href="mod-09.html#cb968-4" tabindex="-1"></a><span class="co"># set up data frame to hold results</span></span>
<span id="cb968-5"><a href="mod-09.html#cb968-5" tabindex="-1"></a>cor.df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">combn</span>(<span class="fu">names</span>(dx), <span class="dv">2</span>)))</span>
<span id="cb968-6"><a href="mod-09.html#cb968-6" tabindex="-1"></a>cor.df<span class="sc">$</span>r <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb968-7"><a href="mod-09.html#cb968-7" tabindex="-1"></a>cor.df<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb968-8"><a href="mod-09.html#cb968-8" tabindex="-1"></a></span>
<span id="cb968-9"><a href="mod-09.html#cb968-9" tabindex="-1"></a><span class="co"># find the r and p values in a loop</span></span>
<span id="cb968-10"><a href="mod-09.html#cb968-10" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(cor.df)){</span>
<span id="cb968-11"><a href="mod-09.html#cb968-11" tabindex="-1"></a>  <span class="co"># test once per iteration  </span></span>
<span id="cb968-12"><a href="mod-09.html#cb968-12" tabindex="-1"></a>  curr <span class="ot">&lt;-</span> <span class="fu">cor.test</span>(dx[,cor.df<span class="sc">$</span>V1[i]], dx[,cor.df<span class="sc">$</span>V2[i]])</span>
<span id="cb968-13"><a href="mod-09.html#cb968-13" tabindex="-1"></a>  <span class="co"># pull out values we need from correlation test result</span></span>
<span id="cb968-14"><a href="mod-09.html#cb968-14" tabindex="-1"></a>  cor.df<span class="sc">$</span>r[i] <span class="ot">&lt;-</span> curr<span class="sc">$</span>estimate</span>
<span id="cb968-15"><a href="mod-09.html#cb968-15" tabindex="-1"></a>  cor.df<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> curr<span class="sc">$</span>p.value</span>
<span id="cb968-16"><a href="mod-09.html#cb968-16" tabindex="-1"></a>}<span class="co">#close i loop</span></span>
<span id="cb968-17"><a href="mod-09.html#cb968-17" tabindex="-1"></a></span>
<span id="cb968-18"><a href="mod-09.html#cb968-18" tabindex="-1"></a><span class="co"># print result</span></span>
<span id="cb968-19"><a href="mod-09.html#cb968-19" tabindex="-1"></a>cor.df</span></code></pre></div>
<pre><code>##             V1           V2          r            p
## 1 Sepal.Length  Sepal.Width -0.1175698 1.518983e-01
## 2 Sepal.Length Petal.Length  0.8717538 1.038667e-47
## 3 Sepal.Length  Petal.Width  0.8179411 2.325498e-37
## 4  Sepal.Width Petal.Length -0.4284401 4.513314e-08
## 5  Sepal.Width  Petal.Width -0.3661259 4.073229e-06
## 6 Petal.Length  Petal.Width  0.9628654 4.675004e-86</code></pre>
<p>It’s not pretty, but we can see right away that every pair is
significantly correlated except sepal width and sepal length.</p>
</div>
<div id="mod-09-nonlinear" class="section level4 hasAnchor" number="9.5.1.2">
<h4><span class="header-section-number">9.5.1.2</span> Nonlinear (rank) correlation (Spearman’s <span class="math inline">\(\rho\)</span>)<a href="mod-09.html#mod-09-nonlinear" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> As values in one set change, do values in another set
change consistently in one direction or another?</p>
<p><strong>Null hypothesis:</strong> N/A (descriptive statistic)</p>
<p><strong>Alternative hypothesis:</strong> N/A (descriptive statistic)</p>
<p><strong>Example use case:</strong> Maggie is studying whether the size of a mammal’s
cecum is related to the proportion of plant material in its diet, and is
not interested in the shape of the relationship.</p>
<p>Processes and relationships in biology are not always linear. Consider
the right figure below. There is clearly a relationship between the <em>Y</em>
and the <em>X</em> variable, but the relationship is probably not linear. In
other words, the slope of <em>Y</em> with respect to <em>X</em> is not constant. So,
using Pearson’s <em>r</em> to say that there is a linear correlation would not
be appropriate. However, we could say that as <em>X</em> increases, <em>Y</em> also
tends to increase. Such a relationship is called <strong>monotonic</strong>: the
<strong>sign</strong> of the slope is always the same, even if its magnitude changes.</p>
<p><img src="applied_biol_data_analysis_20220816_files/figure-html/unnamed-chunk-562-1.png" width="672" /></p>
<p>However, if you rank-transform the data, the relationship turns into
something more linear. A “rank-transform” assigns the smallest value
value “0”, the second smallest value to “1”, and so on.
Rank-transforming these data lets us calculate a nonparametric,
nonlinear correlation. The nonparametric equivalent of Pearson’s <em>r</em> is
Spearman’s <span class="math inline">\(\rho\)</span> (“rho”). It can be obtained using code similar to what
was done for linear correlation.</p>
<div class="sourceCode" id="cb970"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb970-1"><a href="mod-09.html#cb970-1" tabindex="-1"></a><span class="fu">cor</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Sepal.Width, <span class="at">method=</span><span class="st">&quot;spearman&quot;</span>)</span></code></pre></div>
<pre><code>## [1] -0.3096351</code></pre>
<div class="sourceCode" id="cb972"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb972-1"><a href="mod-09.html#cb972-1" tabindex="-1"></a><span class="co"># can also get a significance test for rho</span></span>
<span id="cb972-2"><a href="mod-09.html#cb972-2" tabindex="-1"></a><span class="fu">cor.test</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Sepal.Width, <span class="at">method=</span><span class="st">&quot;spearman&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in cor.test.default(iris$Petal.Length, iris$Sepal.Width, method =
## &quot;spearman&quot;): Cannot compute exact p-value with ties</code></pre>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  iris$Petal.Length and iris$Sepal.Width
## S = 736637, p-value = 0.0001154
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.3096351</code></pre>
<p>When you run a nonlinear correlation test, you will often get the
warning
<code>Warning in cor.test.default ... : Cannot compute exact p-value with ties.</code>
All this means is that some of the rankings are tied. This warning can
be safely ignored.</p>
<pre><code></code></pre>
<p>There is another nonparametric rank-based correlation coefficient,
Kendall’s <span class="math inline">\(\tau\)</span> (“tau”), but it is very similar to <span class="math inline">\(\rho\)</span> and less well
known. So, I suggest you use <span class="math inline">\(\rho\)</span> unless you have a compelling reason
not to.</p>
</div>
</div>
<div id="mod-09-linear-mods" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Linear models<a href="mod-09.html#mod-09-linear-mods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="mod-09-whatis-linear" class="section level4 hasAnchor" number="9.5.2.1">
<h4><span class="header-section-number">9.5.2.1</span> What is a linear model?<a href="mod-09.html#mod-09-whatis-linear" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Linear models</strong> are called <em>linear</em> models because they can be
expressed as a set of linear equations. This means that a change in some
response variable <em>y</em> is described by an additive change in some
predictor <em>x</em> multiplied by a constant. We usually describe that
constant as the <strong>slope</strong>: the change in in <em>y</em> resulting from adding 1
to <em>x</em>. As a result, all linear models can be written very compactly in
matrix notation:</p>
<p><span class="math display">\[
\mathbf{Y=X\beta+U}
\]</span></p>
<p>The boldface variables indicate that each term is really a matrix, not a
variable in the usual sense. In most biological analyses, <strong><em>Y</em></strong>,
<span class="math inline">\(\beta\)</span>, and <strong><em>U</em></strong> are row or column vectors, or matrices with 1 row
or 1 column. The matrix <strong><em>X</em></strong> is the <strong>design matrix</strong>, which contains
the explanatory variables. This notation isn’t very helpful for us
biologists, but it describes what the computer is doing when it fits a
model<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a>:</p>
<p><span class="math display">\[
\left[\begin{matrix}y_1\\y_2\\\vdots\\y_n\\\end{matrix}\right]=\left[\begin{matrix}1&amp;x_{11}&amp;\cdots&amp;x_{1p}\\1&amp;x_{21}&amp;\cdots&amp;x_{2p}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\1&amp;x_{n1}&amp;\cdots&amp;x_{np}\\\end{matrix}\right]\left[\begin{matrix}\beta_0&amp;\beta_1&amp;...&amp;\beta_p\\\end{matrix}\right]+\left[\begin{matrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\\\end{matrix}\right]
\]</span></p>
<p>where <em>n</em> is the number of observations; <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, …, <span class="math inline">\(y_n\)</span> is a
column vector of observed values of the dependent variable (aka:
response variable); <em>p</em> is the number of linear predictors; <span class="math inline">\(x_{i,j}\)</span> is
the <em>i</em>-th value of predictor <em>j</em>; <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, …, <span class="math inline">\(\beta_p\)</span> is
the vector of regression coefficients; and <span class="math inline">\(\varepsilon_1\)</span>,
<span class="math inline">\(\varepsilon_2\)</span>, …, <span class="math inline">\(\varepsilon_n\)</span> is a vector of <em>i</em>.<em>i</em>.<em>d</em>. normal
residuals. This matrix notation illustrates how linear regression can be
extended to models with multiple predictor variables or even multiple
response variables.</p>
<p><strong>The most important meaning of the name “linear” refers to the fact
that the change in the response variable <em>Y</em> per unit change in some
predictor variable <em>X</em> is constant</strong>. This means that a given change in
<em>X</em> will always produce the same change in <em>X</em>. Changing <em>X</em> from 1 to 2
will have the same effect on <em>Y</em> as changing <em>X</em> from 101 to 102, or
1,000,001 to 1,000,002, which is decidedly <em>not</em> the case with nonlinear
functions. In other words, the plot of <em>Y</em> vs. <em>X</em> will be a straight
line with a constant slope. If it sounds like I’m belaboring this point,
it’s because I am…it’s really important!</p>
<p>Linear models also include those where <em>y</em>, <em>x</em>, or both have been
transformed by some function.</p>
<p><strong>Function</strong>: <span class="math inline">\(y=\beta_0+\beta_1x\)</span></p>
<ul>
<li><strong>Why it’s linear:</strong> Rate of change <span class="math inline">\(dy/dx\)</span> is a scalar (<span class="math inline">\(\beta_1\)</span>).</li>
</ul>
<p><strong>Function:</strong> <span class="math inline">\(y=\beta_0+\beta_1x+\beta_2x^2\)</span></p>
<ul>
<li><strong>Why it’s linear:</strong> Although the rate of change <span class="math inline">\(dy/dx\)</span> of a
polynomial is a function of <span class="math inline">\(x\)</span> and not a scalar, the rate of change
with respect to any <span class="math inline">\(x\)</span> term is a scalar (e.g.,
<span class="math inline">\(dy/\left(dx^2\right)=\beta_2\)</span>).</li>
</ul>
<p><strong>Function:</strong> <span class="math inline">\(y=ae^{bx}\)</span></p>
<ul>
<li><strong>Why it’s linear:</strong> This is an exponential model. While the
function itself is nonlinear because <span class="math inline">\(dy/dx\)</span> is a function of <span class="math inline">\(x\)</span>,
the function can be made linear by log-transforming both sides.</li>
</ul>
<p><strong>Function:</strong> <span class="math inline">\(log\left(y\right)=\beta_0+\beta_1x\)</span></p>
<ul>
<li><strong>Why it’s linear:</strong> This is a log-linear model. This is what you
get if you log-transform the exponential model above.</li>
</ul>
<p><strong>Function:</strong> <span class="math inline">\(y=ax^b\)</span></p>
<ul>
<li><strong>Why it’s linear:</strong> This is a <strong>power law</strong>, because <span class="math inline">\(x\)</span> is raised
to a power. If both sides are log-transformed, then the relationship
becomes linear:
<span class="math inline">\(\log\left(y\right)=\log\left(a\right)+b\log\left(x\right)\)</span></li>
</ul>
<p>This notion of linearity is what makes many analyses with <strong>factors</strong>
(categories or grouping variables) as predictors “linear”. For example,
the <em>t</em>-test is a linear model, but it doesn’t really describe a line on
a plot of <em>y</em> vs. <em>x</em> the way that linear regression does. However, if
you encode the factor (grouping variable) as 0s and 1s and think about
the underlying linear algebra, it becomes clear what the difference in
group means really means:</p>
<p><span class="math display">\[
\left[\begin{matrix}y_1\\y_2\\\vdots\\y_n\\\end{matrix}\right]=\left[\begin{matrix}1&amp;x_1\\1&amp;x_2\\\vdots&amp;\vdots\\1&amp;x_n\\\end{matrix}\right]\left[\begin{matrix}\beta_0&amp;\beta_1\\\end{matrix}\right]+\left[\begin{matrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\\\end{matrix}\right]
\]</span></p>
<p>For any observation <em>Y<sub>i</sub></em>, the expected value (without any error term)
is:</p>
<p><span class="math display">\[
E\left(y_i\right)=\left(1\times\beta_0\right)+\left(x_i\times\beta_1\right)
\]</span></p>
<p>For group 1, <span class="math inline">\(x_1\)</span> = 0, so the expected value is <span class="math inline">\(\beta_0\)</span>. For group 2,
<span class="math inline">\(x_i=1\)</span>, so the <span class="math inline">\(\beta_1\)</span> term comes into play as the difference in
means between groups 1 and 2. This is what R, SAS, and other programs
are doing internally when you analyze data with a factor variable. Here
is what that looks like visually:</p>
<p><img src="09_04.jpg" width="60%" style="display: block; margin: auto;" /></p>
<p>Similarly, a one-way ANOVA with 3 levels can be thought of this way:</p>
<p><span class="math display">\[
E\left(y_i\right)=\left(1\times\beta_0\right)+\left(x_1,i\times\beta_1\right)+\left(x_{2,i}\times\beta_2\right)
\]</span></p>
<p>And visualized this way:</p>
<p><img src="09_05.jpg" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="why-do-we-care-about-linear-models" class="section level4 hasAnchor" number="9.5.2.2">
<h4><span class="header-section-number">9.5.2.2</span> Why do we care about linear models?<a href="mod-09.html#why-do-we-care-about-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Finally, it’s worth asking, “Why do we care so much about linear
models?”. There are two reasons. First, as shown above, linear models
can be written very compactly in the language of linear algebra (i.e.,
the branch of mathematics that deals with certain types of operations on
matrices and vectors). From a practical standpoint, this forces us to
organize our datasets sensibly, into matrices with rows that describe
observations and columns that describe variables. From a computational
standpoint, this makes the computer’s job much more efficient because of
the architecture of modern computers.</p>
<p>Second, and less obvious, is that the model coefficients of a linear
model can be <strong>solved for</strong>, rather than approximated. When we fit a
linear model to data, we are really after the values in the coefficients
matrix. A modern statistical program can solve for these values rather
than having to estimate them. For example, a simple linear regression
with 1 predictor variable <em>X</em> can be solved by finding the intercept
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that together minimize the sum of squared
residuals <span class="math inline">\(SS_{res}\)</span>. These coefficients are:</p>
<p><span class="math display">\[
{\hat{\beta}}_1=\frac{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}
\]</span></p>
<p><span class="math display">\[
{\hat{\beta}}_0=\bar{y}-{\hat{\beta}}_1\bar{x}
\]</span></p>
<p>The caret symbol (“^”) indicates that a value is an “estimate”, even
when it is solved for from data (e.g., <span class="math inline">\(\hat{\beta_1}\)</span> is pronounced
“beta-one-hat” and is the estimated slope). Similarly, the bar above a
letter indicates the sample mean (e.g., <span class="math inline">\(\bar{x}\)</span>” is pronounced “x bar”
and is the mean of all observed <em>x</em>).</p>
</div>
<div id="mod-09-linear-assume" class="section level4 hasAnchor" number="9.5.2.3">
<h4><span class="header-section-number">9.5.2.3</span> Assumptions of linear models<a href="mod-09.html#mod-09-linear-assume" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Linear models have some other features and assumptions that you need to
be aware of. If your data or variables violate the assumptions below,
you may need to use something other than linear models to analyze your
data.</p>
<p><strong>Assumption 1: Normal residuals</strong></p>
<p>The linear model assumes that model residuals, the random errors
representing the differences between observed and predicted values,
follow a normal distribution. Models for data that do not have normal
residuals exist, but they are not linear models. We’ll explore those in
a later module.</p>
<p>The assumption of normal residuals explains the last term in the linear
model equation, <span class="math inline">\(\varepsilon_i\)</span>:</p>
<p><span class="math display">\[
y_i=\beta_0+\beta_1x_i+\varepsilon_i
\]</span></p>
<p>In this equation, <span class="math inline">\(y_i\)</span> is observation <em>i</em> of the response variable <em>y</em>;
<span class="math inline">\(\beta_0\)</span> is the intercept; <span class="math inline">\(\beta_1\)</span> is the slope; <span class="math inline">\(x_i\)</span> is the
predictor variable <em>x</em> for observation <em>i</em>, and <span class="math inline">\(\varepsilon_i\)</span> is the
residual for observation <em>i</em> (<span class="math inline">\(\varepsilon\)</span> is the Greek letter
“epsilon” and usually stands for error or residual terms). The residual
term can be expanded like this:</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+Normal\left(0,\sigma_{res}\right)\]</span></p>
<p>This expansion makes it clear that each residual <span class="math inline">\(\varepsilon_i\)</span> comes
independently from a normal distribution with mean 0 and standard
deviation (SD) <span class="math inline">\(\sigma_{res}\)</span>. For convenience, we sometimes separate
the deterministic part and the stochastic part of the model and write
the <strong>state space</strong> form of the linear regression model:</p>
<p><span class="math display">\[
y_i\sim Normal\left(\eta_i,\ \sigma_{res}\right)
\]</span></p>
<p><span class="math display">\[
\eta_i=\beta_0+\beta_1x_i
\]</span></p>
<p>In this version, the symbol ~ means “distributed as” and denotes a
<strong>stochastic</strong> relationship—a relationship that contains some element of
randomness. This is in contrast to a <strong>deterministic</strong> relationship,
which does not (equality, aka: identity, = is the most famous kind of
deterministic relationship).</p>
<p><strong>Assumption 2: Homoscedasticity</strong></p>
<p>Linear models assume <strong>homoscedasticity</strong>, or constant variance. This
means that the residual variance does not depend on either <em>x</em> or
<em>y</em>.This is part of the definition of a variable or a residual being
independent and identically distributed (<em>i</em>.<em>i</em>.<em>d</em>.). If the variance
in the response variable depends on some predictor variable, then this
should be incorporated into the model (resulting in something other than
a linear model). If the variance appears to depend on the response
variable, then there is probably an issue with the assumed response
distribution (more on this below).</p>
<p>The figure below shows two datasets with a linear relationship between
<em>x</em> and <em>y</em>. In the left panel, the variance is the same everywhere
(homoscedastic). The right panel shows a heteroscedastic relationship
where the variance increases at larger <em>x</em>.</p>
<p><img src="09_06.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>The figure below shows two datasets with a categorical predictor,
suitable for a <em>t</em>-test. The left panel shows a relationship with equal
variances in each group, while the right panel shows a situation with
unequal variances.</p>
<p><img src="09_07.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>Heteroscedasticity is a serious problem for linear models because it
leads to biased parameter estimates and standard errors (SE) of those
estimates. The latter issue means that significance tests on parameters
will be incorrect. Bayesian statistics does not involve significance
tests, but heteroscedasticity that is unaccounted for will still lead to
biased estimates. The usual solution is to either apply a
variance-stabilizing transformation (like the log-transform),
incorporate variance parameters that can be estimated, or be clever
about choosing a response distribution.</p>
<p><strong>Assumption 3: Fixed and independent predictors</strong></p>
<p>Linear models assume that predictor values are precisely known (“fixed”)
and independent of each other (i.e., not autocorrelated). If there is
uncertainty in the predictor variables, this adds uncertainty to the
response values that linear models cannot account for. Simply put,
linear models have a term for uncertainty in <em>y</em>, <span class="math inline">\(\sigma_{res}\)</span>, but no
such term for uncertainty in <em>x</em>.</p>
<p><strong>Assumption 4: Independently and identically distributed errors
(<em>i</em>.<em>i</em>.<em>d</em>.)</strong></p>
<p>The assumption of independently and identically distributed (<em>i.i.d.</em>)
errors is <u><strong><em>very</em></strong></u> important. It means that the
residual, or predictive error, for each observation depends only on the
parameters of the residual distribution and <u><strong><em>not</em></strong></u> on
predictor variables or other observations. When the assumption of
independence is violated, the degrees of freedom in the analysis is
artificially inflated because the number of unique pieces of information
is smaller than the nominal number of samples. In frequentist analyses,
this deflates the <em>P</em>-value and increases the chance of a type I error
(false positive). There are methods to deal with errors that are not
independent, but linear models are not among them.</p>
<p><strong><em>Final note about assumptions:</em></strong></p>
<p>Linear models can be robust to <u><em>mild</em></u> violations of these
assumptions, but you need to be careful. Many of these assumptions can
be tested directly—for example, testing residuals for normality or
homoscedasticity—and if there is any doubt about whether your data meet
the assumptions, you should perform those tests. Actually, you should
perform the tests anyway just to be safe.</p>
</div>
<div id="mod-09-lin-reg" class="section level4 hasAnchor" number="9.5.2.4">
<h4><span class="header-section-number">9.5.2.4</span> Simple linear regression<a href="mod-09.html#mod-09-lin-reg" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One of the most basic and well-known of all statistical analyses, the
linear regression model describes a pattern where some response variable
<em>y</em> varies as a linear function of a predictor variable <em>x</em>. The
equation is the same as the equation for a line that you learned in your
first algebra class:</p>
<p><span class="math display">\[y=mx+b\]</span> where <span class="math inline">\(m\)</span> is the slope and <span class="math inline">\(b\)</span> is the <em>y</em>-intercept. However,
we statistics we usually write the model parameters as the Greek letter
“beta” (<span class="math inline">\(\beta\)</span>) with subscripts starting at 0; and we reorder the terms
so that the subscripts increase. Thus, the linear model also looks like
this:</p>
<p><span class="math display">\[
y=\beta_0+\beta_1x+\varepsilon
\]</span></p>
<p>where:</p>
<ul>
<li><p><em>y</em> is the response or dependent variable</p></li>
<li><p><em>x</em> is the explanatory, predictor, or independent variable</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> is the <em>y</em>-intercept (i.e., the value of <em>y</em> when <em>x</em> = 0)
(“beta zero” or “beta naught”). If <span class="math inline">\(x=0\)</span>, then <span class="math inline">\(y=\beta_0\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> is the slope or regression coefficient (i.e., the change
in <em>y</em> per unit change in <em>x</em>) (“beta one”). If <em>x</em> increases by 1,
then <em>y</em> increases by <span class="math inline">\(\beta_1\)</span>.</p></li>
<li><p><span class="math inline">\(\varepsilon\)</span> is a random error term that describes residual
variation not explained by the model (“epsilon”).</p></li>
</ul>
<p>For reasons that will become more clear in <a href="mod-10.html#mod-10">Module 10</a>, some
people prefer to write this in an <strong>observation-wise</strong>, <strong>state-space</strong>
notation.</p>
<p>This format makes it clear that each <em>y</em> value, <span class="math inline">\(y_i\)</span>, has its own
expected value <span class="math inline">\(\eta_i\)</span> (“eta sub i”), dependent on the value of <em>x</em> for
observation <em>i,</em> <span class="math inline">\(x_i\)</span>. This relates to a key assumption of the linear
model, that residuals are independently and identically distributed
(<em>i</em>.<em>i</em>.<em>d</em>.). The “state-space” part of the formulation above refers
to the fact that the model equation explicitly spearates the true,
unobserved, “state” of the system from the observed values which occur
in a “space” defined by the state and the uncertainty about the
observation process. Understanding how to represent models in
state-space format is the key to understanding more advanced methods
like the GLM.</p>
<p>Linear regression in R uses the same <code>lm()</code> function as ANOVA that we
saw above, because both linear regression and ANOVA are linear models.
The formula and syntax are the same, with the only difference being that
the predictor variable is a continuous variable rather than a factor.</p>
<div class="sourceCode" id="cb976"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb976-1"><a href="mod-09.html#cb976-1" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width, <span class="at">data=</span>iris)</span>
<span id="cb976-2"><a href="mod-09.html#cb976-2" tabindex="-1"></a><span class="co"># access parameter coefficients and other info</span></span>
<span id="cb976-3"><a href="mod-09.html#cb976-3" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Petal.Length ~ Petal.Width, data = iris)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.33542 -0.30347 -0.02955  0.25776  1.39453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.08356    0.07297   14.85   &lt;2e-16 ***
## Petal.Width  2.22994    0.05140   43.39   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4782 on 148 degrees of freedom
## Multiple R-squared:  0.9271, Adjusted R-squared:  0.9266 
## F-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb978"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb978-1"><a href="mod-09.html#cb978-1" tabindex="-1"></a><span class="co"># perform an anova on the model</span></span>
<span id="cb978-2"><a href="mod-09.html#cb978-2" tabindex="-1"></a><span class="fu">anova</span>(mod1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Petal.Length
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Petal.Width   1 430.48  430.48  1882.5 &lt; 2.2e-16 ***
## Residuals   148  33.84    0.23                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The R output tells you a lot about the regression: the coefficients (and
their <em>P</em>-values), the omnibus ANOVA for the model (the <em>F</em> test at the
bottom), and the coefficient of determination (Adjusted <em>R</em>-squared).
Coefficients are identified as the y-intercept <span class="math inline">\(\beta_0\)</span> (<code>Intercept</code>)
and by the variable names (e.g., the slope with respect to petal width
is <code>Petal.Width</code>). Each coefficient is presented as its estimate and SE
(e.g., the intercept is <code>1.08 ± 0.07</code>). The test statistic <em>t</em> for each
coefficient is the ratio of the estimate to the SE, and the <em>P</em>-value
for <em>t</em> calculated from a <em>t</em>-distribution (much as in a <em>t</em>-test).</p>
</div>
<div id="mod-09-mult-reg" class="section level4 hasAnchor" number="9.5.2.5">
<h4><span class="header-section-number">9.5.2.5</span> Multiple linear regression<a href="mod-09.html#mod-09-mult-reg" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Linear regression is easily extended to multiple linear regression,
where there are &gt;1 continuous predictors. Just as with ANOVA (see
crossref), additional predictor variables are added to the model formula
using <code>+</code>.</p>
<div class="sourceCode" id="cb980"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb980-1"><a href="mod-09.html#cb980-1" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width <span class="sc">+</span> Sepal.Width, <span class="at">data=</span>iris)</span>
<span id="cb980-2"><a href="mod-09.html#cb980-2" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Petal.Length ~ Petal.Width + Sepal.Width, data = iris)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.33753 -0.29251 -0.00989  0.21447  1.24707 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.25816    0.31352   7.203 2.84e-11 ***
## Petal.Width  2.15561    0.05283  40.804  &lt; 2e-16 ***
## Sepal.Width -0.35503    0.09239  -3.843  0.00018 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4574 on 147 degrees of freedom
## Multiple R-squared:  0.9338, Adjusted R-squared:  0.9329 
## F-statistic:  1036 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Adding predictors to a multiple regression model will almost always
increase the model fit (e.g., proportion of variance explained, <span class="math inline">\(R^2\)</span>),
but this does not mean you should keep adding predictors. Multiple
regression models are extremely vulnerable to <strong>overfitting</strong>, a
statistical mistake where random noise (i.e., residual variation) is
modeled as if it was part of the deterministic part of the model.
Multiple regression models are also highly vulnerable to
<strong>collinearity</strong>, which is when the predictors are correlated with each
other. When collinear predictors are included in a model, it is
impossible for the model to unambiguously partition the sums of squares
associated with each predictor. Compare the 2 models below. The first is
the same as <code>mod1</code> above. The second includes sepal length as a second
predictor, which is highly correlated with the first predictor petal
width.</p>
<div class="sourceCode" id="cb982"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb982-1"><a href="mod-09.html#cb982-1" tabindex="-1"></a><span class="co"># model coefficients</span></span>
<span id="cb982-2"><a href="mod-09.html#cb982-2" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width, <span class="at">data=</span>iris))</span></code></pre></div>
<pre><code>## (Intercept) Petal.Width 
##    1.083558    2.229940</code></pre>
<div class="sourceCode" id="cb984"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb984-1"><a href="mod-09.html#cb984-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width<span class="sc">+</span>Sepal.Length, <span class="at">data=</span>iris))</span></code></pre></div>
<pre><code>##  (Intercept)  Petal.Width Sepal.Length 
##   -1.5071384    1.7481029    0.5422556</code></pre>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="mod-09.html#cb986-1" tabindex="-1"></a><span class="co"># model R^2</span></span>
<span id="cb986-2"><a href="mod-09.html#cb986-2" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width, <span class="at">data=</span>iris))<span class="sc">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.9266173</code></pre>
<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb988-1"><a href="mod-09.html#cb988-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width<span class="sc">+</span>Sepal.Length,</span>
<span id="cb988-2"><a href="mod-09.html#cb988-2" tabindex="-1"></a>    <span class="at">data=</span>iris))<span class="sc">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.9478233</code></pre>
<p>Notice that while adding sepal length improved the model fit, it also
changed the estimated slope for petal width from 2.23 to 1.75, a shift
of over 20%! Even worse, the effect of sepal length by itself affected
by the inclusion of petal width:</p>
<div class="sourceCode" id="cb990"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb990-1"><a href="mod-09.html#cb990-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Sepal.Length, <span class="at">data=</span>iris))</span></code></pre></div>
<pre><code>##  (Intercept) Sepal.Length 
##    -7.101443     1.858433</code></pre>
<p>Adding the collinear predictor petal width to the model above changes
the effect of sepal length from 1.86 to 0.54, a change of over 70%!
What’s happening here is that the collinearity between petal width and
sepal length confuses the model fitting algorithm. To help understand
this, imagine you wanted to measure the effect of two variables, but you
never knew their actual values—only their product. It’s the same
problem.</p>
<p>To further illustrate the point, consider the example below where a
totally uncorrelated variable <code>x5</code> is added to the model. The
coefficient of petal width in this model is essentially the same as in
the model with petal width as the sole predictor.</p>
<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb992-1"><a href="mod-09.html#cb992-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb992-2"><a href="mod-09.html#cb992-2" tabindex="-1"></a>iris<span class="sc">$</span>x5 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(iris))</span>
<span id="cb992-3"><a href="mod-09.html#cb992-3" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Petal.Length<span class="sc">~</span>Petal.Width<span class="sc">+</span>x5, <span class="at">data=</span>iris))</span></code></pre></div>
<pre><code>## (Intercept) Petal.Width          x5 
##  1.08471526  2.22874451 -0.01137603</code></pre>
</div>
<div id="mod-09-ancova" class="section level4 hasAnchor" number="9.5.2.6">
<h4><span class="header-section-number">9.5.2.6</span> Analysis of covariance (ANCOVA)<a href="mod-09.html#mod-09-ancova" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In <strong>analysis of covariance (ANCOVA)</strong>, a continuous response variable
is modeled using a continuous explanatory variable
<u><strong><em>and</em></strong></u> a factor (grouping variable). You can think of
it as a mix of linear regression and ANOVA, although all three methods
are just different expressions of the linear model. You’ve probably
heard of ANCOVA before without realizing it. When scientific studies are
described on the news, the reporter might say that the researchers
“controlled for” the effects of additional variables. That often means
that the researchers used a type of ANCOVA in their analysis.</p>
<p>The figure below shows two examples of what ANCOVA looks like in
practice. Both types of ANCOVA are extremely common in biology.</p>
<p><img src="09_08.jpg" width="80%" style="display: block; margin: auto;" /></p>
<ul>
<li><p><strong>Left:</strong> Continuous response <em>Y</em> increases as a linear function of
continuous predictor <em>X</em>, and values in group 2 tend to be larger
than values in group 1. This is called “main effects” ANCOVA because
groups 1 and 2 have different intercepts but the same slope; i.e.,
there is a “main effect” of group on <em>Y</em>.</p></li>
<li><p><strong>Right:</strong> Continuous response <em>Y</em> varies as a function of
continuous predictor <em>X</em>, and the effect of <em>X</em> on <em>Y</em> different in
each group (i.e., the slopes are different). This is an “ANCOVA with
interaction”. Here, “group” interacts with (changes the effect of)
<em>X</em>.</p>
<ul>
<li><p>An “interaction” means that the effect of one variable affects
the effect of another variableBecause the trendlines are
non-parallel, they must intersect.</p></li>
<li><p>Thus, there will be a region where group 1 &gt; group 2, and a
region where group 2 &gt; group 1. I.e., you cannot discuss a
“main effect” of group on <em>Y</em>. …just like an interaction in
ANOVA!</p></li>
</ul></li>
</ul>
<p><strong>Example ANCOVA in R</strong></p>
<p>Use the <code>ToothGrowth</code> dataset. This contains data on tooth length after
a lengthy feeding trial in 60 guinea pigs. The variables are tooth
length in mm (<code>len</code>), vitamin C dose in mg/day (<code>dose</code>), and vitamin C
delivery method (<code>supp</code>).</p>
<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb994-1"><a href="mod-09.html#cb994-1" tabindex="-1"></a><span class="co"># name is too long</span></span>
<span id="cb994-2"><a href="mod-09.html#cb994-2" tabindex="-1"></a>tg <span class="ot">&lt;-</span> ToothGrowth</span>
<span id="cb994-3"><a href="mod-09.html#cb994-3" tabindex="-1"></a></span>
<span id="cb994-4"><a href="mod-09.html#cb994-4" tabindex="-1"></a><span class="co"># fit the ANCOVA with interaction</span></span>
<span id="cb994-5"><a href="mod-09.html#cb994-5" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(len<span class="sc">~</span>dose<span class="sc">*</span>supp, <span class="at">data=</span>tg)</span>
<span id="cb994-6"><a href="mod-09.html#cb994-6" tabindex="-1"></a><span class="fu">anova</span>(mod1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: len
##           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
## dose       1 2224.30 2224.30 133.4151 &lt; 2.2e-16 ***
## supp       1  205.35  205.35  12.3170 0.0008936 ***
## dose:supp  1   88.92   88.92   5.3335 0.0246314 *  
## Residuals 56  933.63   16.67                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVA table tells us that dose, supplement, and the interaction of
dose and supplement are significant predictors of tooth growth, with
dose having most of the explanatory power.</p>
<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb996-1"><a href="mod-09.html#cb996-1" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = len ~ dose * supp, data = tg)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.2264 -2.8462  0.0504  2.2893  7.9386 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   11.550      1.581   7.304 1.09e-09 ***
## dose           7.811      1.195   6.534 2.03e-08 ***
## suppVC        -8.255      2.236  -3.691 0.000507 ***
## dose:suppVC    3.904      1.691   2.309 0.024631 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.083 on 56 degrees of freedom
## Multiple R-squared:  0.7296, Adjusted R-squared:  0.7151 
## F-statistic: 50.36 on 3 and 56 DF,  p-value: 6.521e-16</code></pre>
<p>The coefficients table tells us:</p>
<ul>
<li><p>When dose = 0 and supplement = orange juice (OJ), mean tooth growth
was 11.55 mm.</p></li>
<li><p>In animals fed orange juice, each additional mg/day increased tooth
growth by 7.81 mm.</p></li>
<li><p>In animals fed ascorbic acid (VC) instead of orange juice, each
additional mg/day of vitamin C increased tooth length by (7.81 +
3.90) = 11.71 mm. So, vitamin C had a greater effect when given as
ascorbic acid than when given in orange juice.</p></li>
<li><p>Interestingly, minimum tooth growth was smaller in the ascorbic acid
group (11.55 – 8.26 = 3.29 mm), but this is an artifact of the fact
that the slopes are unequal and greater in the VC group.</p></li>
</ul>
</div>
</div>
<div id="linear-models-wrap-up-for-now" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Linear models wrap-up (for now)<a href="mod-09.html#linear-models-wrap-up-for-now" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By now you’ve seen some of the variety and power of linear models. Many
fundamental statistics methods–regression, correlation, ANOVA,
<em>t</em>-tests, and so on–are really just linear models in disguise. That
ought to suggest how powerful of a framework linear models can be. There
are other methods we’ll encounter in this course that can be thought of
as being derived from linear models, usually by <strong>generalizing</strong> or
relaxing certain parts of the definition (although, as we’ll see, this
is backwards–linear models are the special case of a broader class).
What they all have in common, however, a linear part somewhere.</p>
<p>These relatives include:</p>
<ul>
<li><p><strong>Generalized linear models (GLM):</strong> use a link function and an
alternative response distribution. See <a href="mod-10.html#mod-10">module 10</a>.</p></li>
<li><p><strong>(Generalized) linear mixed models ((G)LMM):</strong> include
randomly-varying parameters to account for unknown variation. See
<a href="mod-12.html#mod-12">Module 12</a>.</p></li>
<li><p><strong>Generalized or Ordinary Least Squares (GLS or OLS):</strong> includes a
covariance structure to account for non-independence of residuals.
One important class of variant is <strong>phylogenetic least squares</strong>,
which uses genetic relatedness to account for unmodeled similarities
between species in an analysis.</p></li>
<li><p><strong>Multivariate analysis of variance (MANOVA):</strong> Extension of ANOVA
to data matrices, where the “response variable” is a set of response
variables. See <a href="mod-13.html#mod-13-manova">Module 13</a>.</p></li>
<li><p><strong>Principal components analysis (PCA)</strong> and <strong>Redundancy Analysis
(RA)</strong>: Multivariate ordination techniques which rely on much of the
same underlying math (e.g., minimizing least squares) to describe
variation in multiple dimensions. See <a href="mod-13.html#mod-13-pca">Module 13</a>.</p></li>
</ul>
</div>
</div>
<div id="mod-09-contingency" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Tests for proportions and contingency<a href="mod-09.html#mod-09-contingency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="mod-09-ctables" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Contingency tables<a href="mod-09.html#mod-09-ctables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes biological data do not come with neatly-defined numeric
responses. Often, the response “variable” is whether or not an
observation belongs to a particular category or not—or a combination of
categories. These can be called <strong>categorical data</strong>, where the
information of interest is not a numeric value, but identity or
classification. Consider the (made up) data below:</p>
<p><img src="09_09.jpg" width="60%" style="display: block; margin: auto;" /></p>
<p>This tables tallies the fates of 48 plants that were exposed or not
exposed to an experimental pesticide. The outcomes were classified as
“no pollen production”, “reduced pollen production”, and “normal pollen
production”. Rather than asking a question about the mean pollen
production in each group, the researchers might ask whether the fate of
a plant (in terms of pollen production) is independent of whether or not
it was treated. This is known as a <strong>test of independence</strong>. A good clue
for whether or not you need to do a test for independence is whether or
not your data are best summarized in a <strong>contingency table</strong> such as the
table above. A contingency table shows how often observations fall into
each category within a dataset.</p>
</div>
<div id="tests-on-contingency-tables" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Tests on contingency tables<a href="mod-09.html#tests-on-contingency-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="mod-09-fishers" class="section level4 hasAnchor" number="9.6.2.1">
<h4><span class="header-section-number">9.6.2.1</span> Fisher’s exact test<a href="mod-09.html#mod-09-fishers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Are the proportions of data described by two or more categorical variables random or nonrandom?</p>
<p><strong>Null hypothesis:</strong> Membership in one of one set of categories is independent of membership in one of another set of categories.</p>
<p><strong>Alternative hypothesis:</strong> Membership in one of one set of categories is not independent of membership in one of another set of categories.</p>
<p><strong>Example use case:</strong> Rick is studying if whether a fish spawning or not spawning is related to whether it resides in main channel or off-channel habitat, and he has dozens of fish in his dataset.</p>
<p><strong>Fisher’s exact test</strong> is a test for independence in contingency tables
that is usually only used when sample sizes are small. This is because
the <em>P</em>-value calculation involves a numerous factorials, which can
quickly overwhelm even modern desktop computers. Consequently, most
modern programs use an approximation to the true <em>P</em>-value, obviating
the purpose of an exact test. For any situation where a Fisher’s exact
test is appropriate, a chi-square (<span class="math inline">\(\chi^2\)</span>) test will usually give the
same answer, but faster (as long as the sample size is large enough).</p>
<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb998-1"><a href="mod-09.html#cb998-1" tabindex="-1"></a><span class="co"># make a matrix from the table above</span></span>
<span id="cb998-2"><a href="mod-09.html#cb998-2" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">17</span>,<span class="dv">6</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">14</span>), <span class="at">byrow=</span><span class="cn">TRUE</span>, <span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb998-3"><a href="mod-09.html#cb998-3" tabindex="-1"></a>a</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   17    6
## [2,]    3    4
## [3,]    4   14</code></pre>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="mod-09.html#cb1000-1" tabindex="-1"></a><span class="fu">fisher.test</span>(a)</span></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  a
## p-value = 0.002849
## alternative hypothesis: two.sided</code></pre>
<p>Because Fisher’s is an exact test (i.e., the <em>P</em>-value is an direct,
exact calculation), there is no test statistic and no degrees of freedom
to report.</p>
</div>
<div id="mod-09-chisq" class="section level4 hasAnchor" number="9.6.2.2">
<h4><span class="header-section-number">9.6.2.2</span> Chi-squared (<span class="math inline">\(\chi^{2}\)</span>) test<a href="mod-09.html#mod-09-chisq" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Are the proportions of data described by two or more categorical variables random or nonrandom?</p>
<p><strong>Null hypothesis:</strong> Membership in one of one set of categories is independent of membership in one of another set of categories.</p>
<p><strong>Alternative hypothesis:</strong> Membership in one of one set of categories is not independent of membership in one of another set of categories.</p>
<p><strong>Example use case:</strong> Morty is studying if whether a fish spawning or not spawning is related to whether it resides in main channel or off-channel habitat, and he has hundreds of fish in his dataset.</p>
<p>The chi-square (<span class="math inline">\(\chi^2\)</span>) test is a more powerful, flexible, and
computationally efficient cousin of the Fisher’s exact test. It works by
comparing the input contingency table to what the contingency table
would be expected to look like under the assumption of independence.
That is, if the null hypothesis were true. The test statistic, <span class="math inline">\(\chi^2\)</span>
measures how different the two contingency tables are. The <span class="math inline">\(\chi^2\)</span> test
works better with large sample sizes; with small sample sizes, the
approximations use to estimate the <em>P</em>-value can break down. A good rule
of thumb is that every cell (or most cells) of the hypothetical null
contingency table should have <span class="math inline">\(\geq5\)</span> observations.</p>
<p>The test statistic <span class="math inline">\(\chi^2\)</span> (“chi squared”) is calculated as:</p>
<p><span class="math display">\[
\chi^2=\sum_{i=1}^{n}\frac{\left(O_i-E_i\right)^2}{E_i}
\]</span></p>
<p>where <em>n</em> is the number of categories (e.g., cells in the contingency
table); <em>O<sub>i</sub></em> is the number of observations in category <em>i</em>, and <em>E<sub>i</sub></em>
is the number of expected outcomes in category <em>i</em>. Calculating the
number of expected values for each cell can be tricky, but boils down to
partitioning up the marginal totals (row or column totals) of the table.
The example below illustrates how to get expected values from the same
flat contingency table we saw earlier:statistic is then compared to a
<span class="math inline">\(\chi^2\)</span> distribution with a number of degrees of freedom equal to:</p>
<p><span class="math display">\[
DF=\left(number~of~rows-1\right)\left(number~of~columns-1\right)
\]</span></p>
<p>Coming up with the number of expected values for each cell can be
tricky, but boils down to partitioning up the marginal totals (row or
column totals) of the table. The example below illustrates how to get
expected values from the same flat contingency table we saw earlier:</p>
<p><img src="09_10.jpg" width="60%" style="display: block; margin: auto;" /></p>
<p>If the categories were independent, then the probability of being in
both the “None” pollen group and the “Untreated” pesticide group would
the product of the marginal probabilities of being in those groups:</p>
<p><img src="09_11.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>For the table above:</p>
<ul>
<li><p>23 observations were in the “None” pollen group.</p></li>
<li><p>Of those 23 observations, if pesticide treatment has no effect on
pollen production, we should expect half (24/48) of the “None”
observations to be in the “Untreated” group and the other half to be
in the “Treated” group.</p></li>
<li><p>Thus, the expected number of None—Untreated is
<span class="math inline">\(23\times p\left(Untreated\right)=11.5\)</span>.</p></li>
</ul>
<p><strong>Example</strong> <span class="math inline">\(\chi^2\)</span> <strong>test in R:</strong></p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="mod-09.html#cb1002-1" tabindex="-1"></a><span class="co"># make a matrix from the table above</span></span>
<span id="cb1002-2"><a href="mod-09.html#cb1002-2" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">17</span>,<span class="dv">6</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">14</span>), <span class="at">byrow=</span><span class="cn">TRUE</span>, <span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb1002-3"><a href="mod-09.html#cb1002-3" tabindex="-1"></a></span>
<span id="cb1002-4"><a href="mod-09.html#cb1002-4" tabindex="-1"></a><span class="co"># run the test</span></span>
<span id="cb1002-5"><a href="mod-09.html#cb1002-5" tabindex="-1"></a><span class="fu">chisq.test</span>(a)</span></code></pre></div>
<pre><code>## Warning in chisq.test(a): Chi-squared approximation may be incorrect</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  a
## X-squared = 10.959, df = 2, p-value = 0.004171</code></pre>
<p>One advantage of the <span class="math inline">\(\chi^2\)</span> test in R vs. the Fisher’s test is that it
returns the expected contingency table, for comparision. This can help
you see which categories are the most over- or underrepresented.</p>
<div class="sourceCode" id="cb1005"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1005-1"><a href="mod-09.html#cb1005-1" tabindex="-1"></a>cq1 <span class="ot">&lt;-</span> <span class="fu">chisq.test</span>(a)</span></code></pre></div>
<pre><code>## Warning in chisq.test(a): Chi-squared approximation may be incorrect</code></pre>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="mod-09.html#cb1007-1" tabindex="-1"></a>cq1<span class="sc">$</span>observed</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   17    6
## [2,]    3    4
## [3,]    4   14</code></pre>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="mod-09.html#cb1009-1" tabindex="-1"></a>cq1<span class="sc">$</span>expected</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 11.5 11.5
## [2,]  3.5  3.5
## [3,]  9.0  9.0</code></pre>
<p>This helps us see that the apparent lack of independence is likely
driven by the relative numbers of “none” and “normal” pollen producers.</p>
</div>
<div id="mod-09-gtest" class="section level4 hasAnchor" number="9.6.2.3">
<h4><span class="header-section-number">9.6.2.3</span> <em>G</em>-test<a href="mod-09.html#mod-09-gtest" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Question:</strong> Are the proportions of data described by two or more categorical variables random or nonrandom?</p>
<p><strong>Null hypothesis:</strong> Membership in one of one set of categories is independent of membership in one of another set of categories.</p>
<p><strong>Alternative hypothesis:</strong> Membership in one of one set of categories is not independent of membership in one of another set of categories.</p>
<p><strong>Example use case:</strong> Beth is studying if whether a fish spawning or not spawning is related to whether it resides in main channel or off-channel habitat, and she has thousands of fish in her dataset.</p>
<p>The <em>G</em>-test is another test for independence that can be used in
situations where the <span class="math inline">\(\chi^2\)</span> test was classically recommended. Its test
statistic is a bit different:</p>
<p><span class="math display">\[
G=2\sum_{i}^{n}{O_ilog{\left(\frac{O_i}{E_i}\right)}}
\]</span></p>
<p>Where the terms are the same as in the equation for <span class="math inline">\(\chi^2\)</span> and “log”
is the natural logarithm function. Further, the total observed count
must equal the total expected count:</p>
<p><span class="math display">\[
\sum_{i}^{n} O_i=\sum_{i}^{n} E_i
\]</span></p>
<p>The <em>G</em>-test should be used when sample sizes are large, because it is
more computationally efficient than <span class="math inline">\(\chi^2\)</span>, and in situations where
<span class="math inline">\(O_i&gt;2E_i\)</span> for at least some cells.</p>
<p>Despite that fact that is has been recommended in literature for over 3
decades, the <em>G</em>-test is not available in the base R <code>stats</code> package. It
is implemented in several add-on packages; the example below shows the
<code>g.test()</code> function in package <code>AMR</code>.</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="mod-09.html#cb1011-1" tabindex="-1"></a><span class="fu">library</span>(AMR)</span></code></pre></div>
<pre><code>## Warning: package &#39;AMR&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="mod-09.html#cb1013-1" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">17</span>,<span class="dv">6</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">14</span>), <span class="at">byrow=</span><span class="cn">TRUE</span>, <span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb1013-2"><a href="mod-09.html#cb1013-2" tabindex="-1"></a></span>
<span id="cb1013-3"><a href="mod-09.html#cb1013-3" tabindex="-1"></a><span class="co"># bump up sample size from example table</span></span>
<span id="cb1013-4"><a href="mod-09.html#cb1013-4" tabindex="-1"></a><span class="co"># because G-test is meant for large samples</span></span>
<span id="cb1013-5"><a href="mod-09.html#cb1013-5" tabindex="-1"></a>a <span class="ot">&lt;-</span> a<span class="sc">*</span><span class="dv">20</span></span>
<span id="cb1013-6"><a href="mod-09.html#cb1013-6" tabindex="-1"></a></span>
<span id="cb1013-7"><a href="mod-09.html#cb1013-7" tabindex="-1"></a><span class="co"># run the test:</span></span>
<span id="cb1013-8"><a href="mod-09.html#cb1013-8" tabindex="-1"></a><span class="fu">g.test</span>(a)</span></code></pre></div>
<pre><code>## 
##  G-test of independence
## 
## data:  a
## X-squared = 230.19, p-value &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="mod-09-oddsratios" class="section level3 hasAnchor" number="9.6.3">
<h3><span class="header-section-number">9.6.3</span> Contingency tests and odds ratios<a href="mod-09.html#mod-09-oddsratios" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is an interesting connection between contingency tests and a
famous generalized linear model, <a href="mod-10.html#mod-10-logistic">logistic
regression</a>. Briefly, logistic regression models a
binary outcome (can take 1 of 2 values). One of the key outputs of a
logistic regression is the odds ratio (OR), which is defined as Euler’s
constant <em>e</em> raised to the power of the effect (i.e., slope or <span class="math inline">\(\beta\)</span>)
for some predictor. An OR of 2 indicates that if x increases by 1 (or
you go from baseline category to some other category), the odds of <span class="math inline">\(y=1\)</span>
will double. In other words, a success becomes <span class="math inline">\(2\times\)</span> as likely.
Similarly, an OR of 0.25 means that <span class="math inline">\(y=1\)</span> becaomes 1/4 as likely.
Consider our contingency table from earlier:</p>
<p><img src="09_09.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>When a plan is untreated, its <strong>probability</strong> of producing a normal
pollen count is <span class="math inline">\(\left(4\right)/\left(17+3+4\right)=0.1667\)</span>. The
<strong>odds</strong> are a related quantity, being the ratio of the probability to
its 1-complement:</p>
<p><span class="math display">\[
O(Normal|Untreated)=\frac{p(Normal|Untreated)}{1-p(Normal|Untreated)}
\]</span></p>
<p>In this case, the odds are “4 to 20” or
<span class="math inline">\(\left(4/24\right)/\left(1-\left(4/24\right)\right)=0.2222\)</span>. In the
treated group, however, the odds become
<span class="math inline">\(\left(14/24\right)/\left(1-\left(14/24\right)\right)=1.4\)</span>, or “14 to
10”. The ratio of these two odds is the “odds ratio” or effect of
treatment on the likelihood of normal pollen production:</p>
<p><span class="math display">\[
OR=\frac{1.4}{0.2222}=6.3
\]</span></p>
<p>In other words, a treated plant is 6.3 times more likely to experience
normal pollen production than an untreated plant.</p>
<p>Why am I bringing this up? It should be obvious at this point that many
analyses conceived as <span class="math inline">\(\chi^2\)</span> tests could be reimagined as logistic
regressions. The obvious advantage of doing so is that a logistic
regression is a predictive model, whereas a <span class="math inline">\(\chi^2\)</span> test is not. So,
analyzing the data using logistic regression may offer some advantages.</p>
</div>
</div>
<div id="mod-09-classification" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Classification<a href="mod-09.html#mod-09-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The fourth class of common biological data analysis, classification, is
not commonly covered in introductory statistics courses and so is not
covered in detail here. Maybe I’ll build a module on classification if
there’s interest.</p>
<p>Briefly, <strong>classification</strong> methods use independent (explanatory)
variables to “classify”, or predict which of &gt;=2 categories a response
will fall into. Classification methods can be predictive or descriptive.
The most famous method is logistic regression, which we’ll cover in
<a href="mod-10.html#mod-10-logistic">Module 10</a>. Other methods include multinomial
regression (which generalizes from the binary classification of logistic
regression to <span class="math inline">\(\geq2\)</span> possible outcomes); cluster analysis,
classification and regression trees (CART), and many models in the
machine learning and data science space.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="64">
<li id="fn64"><p>Gosset published his work under the pen name “Student”
because his employer at the time, the Guinness Brewery, didn’t want
their competitors to know that they used the <em>t</em>-test in their
quality control procedures<a href="mod-09.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>This is why <em>t</em>-tests in R often report non-integer
degrees of freedom.<a href="mod-09.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>From a Bayesian perspective, where no <em>P</em>-values or test
statistic distributions are involved, the <em>t</em>-test and ANOVA are
exactly the same model.<a href="mod-09.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>Technically, <strong>β</strong> is a row vector (matrix with 1 row)
because of the way that matrix multiplication is defined. However,
we usually present it as a column vector to save space. Most
biologists don’t know or care about linear algebra anyway.<a href="mod-09.html#fnref67" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-08.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/09-basic.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["applied_biol_data_analysis_20220816.pdf", "applied_biol_data_analysis_20220816.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
