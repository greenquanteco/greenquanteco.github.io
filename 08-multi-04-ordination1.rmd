---
title: "Multivariate statistics part 4: Ordination"
author: "Nick Green, Kennesaw State University"
output:
  html_document: 
    toc: yes
    toc_float: true
    number_sections: yes
    keep_md: yes
font_size: 16pt
bibliography: stat_refs.bib
csl: ecology.csl
---

# Introduction

In the [last section](https://greenquanteco.github.io/08-multi-03-analyzing_sim.html) we saw an application of **ordination**: representing high-dimensional relationships between objects in a 2-d space. This is done in such a way as to represent important patterns in the data in few enough dimensions for our 3-d brains to handle.
 
Ordination is literally **ordering observations along two or more axes**. That is, coming up with a new coordinate system that shows relationships between observations. How that ordering is done varies wildly among techniques. One broad class of techniques, collectively called **eigenvector** based methods, use the power of linear algebra to place the data into a new coordinate system that better captures the patterns in the data. The other class uses **Monte Carlo sampling** to find arrangements of samples that retain the important patterns in reduced dimension space.

No matter how they work, all ordination methods have the same goal: representing patterns found in many dimensions in fewer dimensions. For this course our focus will be on interpretation rather than calculation. The applications of ordination overlap with those of the multivariate techniques that we have already seen:

- **Cluster identification**: observations that are closer to each other in the ordination space are more similar to each other
- **Dimension reduction**: the axes of an ordination are estimates of synthetic variables that combine information about many variables at once

# Principal components analysis (PCA)

## PCA intro

Principal components analysis (PCA) is a method for extracting synthetic gradients from a multivariate dataset that capture most of the variation in that dataset. These gradients are calculated by finding linear combinations of the variables that minimize sums of squared deviations from the gradient. This means that PCA has a lot in common with linear regression, and many of the same assumptions apply.
If you’ve never heard of principal components analysis or ordination, it might be worth watching a video that explains and shows the basic ideas. Here is one that only takes 5 minutes and has a nice theme song (accessed 2021-08-10) .
Imagine a dataset with 2 variables, *x* and *y*. You could capture and display all the information about this dataset in a 2-d scatterplot, by simply plotting *y* vs. *x*. Likewise, you could capture and display all of the information about a 3-dimensional dataset with a 3-d plot. For 4 or more dimensions, a true scatterplot can’t be rendered sensibly or even understood by our pathetic meat-based brains.

```{r, echo=FALSE, fig.width=6, fig.height=6}
set.seed(123)
n <- 50
x <- runif(n, 1, 20)
y <- 1.2 + 1.7*x + rnorm(n, 0, 15)
x.orig <- x
y.orig <- y

par(mfrow=c(1,1), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.3, cex.axis=1.3)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(0, 20), ylim=c(-30, 50), pch=16, cex=1.2)
``` 

One way to describe the variation in the dataset above is to think about how the samples vary along each axis. The figure below shows how the variation among samples can be broken down into the variation in Variable 1 and the variation in Variable 2. When we say “variation in variable 1”, we mean “deviation between the values of variable 1 and the mean of variable 1”. That is what is shown in the figure below.

- Each point, or sample, has a Variable 1 coordinate and a Variable 2 coordinate.
- Each point’s value for Variable 1 can be thought of as the difference between that value and the mean of Variable 1.
- Likewise, each point’s value for Variable 2 can be thought of as the difference between that value and the mean of Variable 2.
- The position of each point can thus be reframed as its deviation with respect to the Variable 1, and its deviation with respect to Variable 2. This reframing is the same as centering each variable (i.e., subtracting the mean). 
- The total variance among the samples is equal to the variance with respect to Variable 1 plus the variance with respect to Variable 2.

```{r, echo=FALSE, fig.width=9, fig.height=4.5}
ymin <- min(y)
ymax <- max(y)
ymu <- mean(y)
xmin <- min(x)
xmax <- max(x)
xmu <- mean(x)

par(mfrow=c(1,2), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.1, cex.axis=1.1)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(0, 20), ylim=c(-30, 60),
    type="n")
segments(xmu, ymin, xmu, ymax, lwd=3, col="red", lty=2)
segments(xmu, y, x, y, lwd=2, col="red")
points(x, y, pch=16, cex=1.1)
text(12, -5, "Variation in variable 1", adj=0, col="red", cex=1.1, xpd=NA)
text(12, -18, expression(sum((x[i]-bar(x))^2, italic(i)==1, italic(n))==1531.9),
    adj=0, col="red", cex=1.1, xpd=NA)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(0, 20), ylim=c(-30, 60),
    type="n")
segments(xmin, ymu, xmax, ymu, lwd=3, col="blue", lty=2)
segments(x, ymu, x, y, lwd=2, col="blue")
points(x, y, pch=16, cex=1.1)
text(9, -5, "Variation in variable 2", adj=0, col="blue", cex=1.1)
text(9, -18, expression(sum((y[i]-bar(y))^2, italic(i)==1, italic(n))==15686.5),
    adj=0, col="blue", cex=1.1, xpd=NA)
```

The figure above shows how the total variation in the dataset is split up (“partitioned”) into variation in Variable 1 and variation in Variable 2. 

Describing each observation as its deviation from the mean of each variable has the effect of **centering** the points at the origin. Notice that the variation, expressed as sums of squared deviations, is unchanged.

```{r, echo=FALSE, fig.width=9, fig.height=4.5}

x <- as.numeric(scale(x, scale=FALSE))
y <- as.numeric(scale(y, scale=FALSE))

ymin <- min(y)
ymax <- max(y)
ymu <- mean(y)
xmin <- min(x)
xmax <- max(x)
xmu <- mean(x)

par(mfrow=c(1,2), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.1, cex.axis=1.1)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(-10, 10), ylim=c(-50, 50),
    type="n")
segments(xmu, ymin, xmu, ymax, lwd=3, col="red", lty=2)
segments(xmu, y, x, y, lwd=2, col="red")
points(x, y, pch=16, cex=1.1)
text(1, -20, "Variation in variable 1", adj=0, col="red", cex=1.1, xpd=NA)
text(1, -35, expression(sum((x[i]-bar(x))^2, italic(i)==1, italic(n))==1531.9),
    adj=0, col="red", cex=1.1, xpd=NA)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(-10, 10), ylim=c(-50, 50),
    type="n")
segments(xmin, ymu, xmax, ymu, lwd=3, col="blue", lty=2)
segments(x, ymu, x, y, lwd=2, col="blue")
points(x, y, pch=16, cex=1.1)
text(-1, -20, "Variation in variable 2", adj=0, col="blue", cex=1.1)
text(-1, -35, expression(sum((y[i]-bar(y))^2, italic(i)==1, italic(n))==15686.5),
    adj=0, col="blue", cex=1.1, xpd=NA)
```

If variables have different ranges, it is a good idea to scale them as well as center them (aka: standardizing or *Z*-scaling). This means that the original values are converted to *Z*-scores by subtracting the mean and dividing by the SD. Trying to use PCA or other eigenvector-based methods without standardizing variables will distort the results. Standardization puts variation along any axis on equal footing.
 
The figure below shows the data as deviations from variable means after standardization. Note that the sums of squares are equal in both directions now.

```{r, echo=FALSE, fig.width=9, fig.height=4.5}
x <- as.numeric(scale(x.orig))
y <- as.numeric(scale(y.orig))

ymin <- min(y)
ymax <- max(y)
ymu <- mean(y)
xmin <- min(x)
xmax <- max(x)
xmu <- mean(x)

par(mfrow=c(1,2), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.1, cex.axis=1.1)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(-2, 2), ylim=c(-2.5, 2.5),
    type="n")
segments(xmu, ymin, xmu, ymax, lwd=3, col="red", lty=2)
segments(xmu, y, x, y, lwd=2, col="red")
points(x, y, pch=16, cex=1.1)
text(0.2, -1, "Variation in variable 1", adj=0, col="red", cex=1.1, xpd=NA)
text(0.2, -1.75, expression(sum((x[i]-bar(x))^2, italic(i)==1, italic(n))==49),
    adj=0, col="red", cex=1.3, xpd=NA)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(-2, 2), ylim=c(-2.5, 2.5),
    type="n")
segments(xmin, ymu, xmax, ymu, lwd=3, col="blue", lty=2)
segments(x, ymu, x, y, lwd=2, col="blue")
points(x, y, pch=16, cex=1.1)
text(-0.2, -1, "Variation in variable 2", adj=0, col="blue", cex=1.1)
text(-0.2, -1.75, expression(sum((y[i]-bar(y))^2, italic(i)==1, italic(n))==49),
    adj=0, col="blue", cex=1.1, xpd=NA)
```
If our goal is to describe variation in the data as succinctly as possible, then using Variable 1 and Variable 2 as axes might not be the best approach. Notice that most of the variation in the points doesn’t line up exactly with either of the variables, but along the red arrow shown below. The rest of the variation is along the direction perpendicular to the red arrow, illustrated by the blue arrow. (Note that the arrows may only appear perpendicular if the plot is precisely square).

```{r, echo=FALSE, fig.width=6, fig.height=6}
########### illustrate rotation to PCs
fit1 <- lm(y~x)
mod1 <- coef(fit1)

# get coefficients for model
beta0 <- mod1[1]
beta1 <- mod1[2]
alpha1 <- (-1)/beta1
alpha0 <- 0-alpha1*0

# plot with arrows
# note: will not be perpendicular unless axes are the same!
par(mfrow=c(1,1), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.1, cex.axis=1.1)
plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(-2, 2), ylim=c(-2.5, 2.5),
    type="n")
points(x, y, pch=16, cex=1.1)
arrows(xmin, beta0+beta1*(xmin), 
    xmax, beta0+beta1*xmax, 
    lwd=3, col="red", code=3)
arrows(xmin, alpha0 + alpha1*xmin,
         xmax, alpha0 + alpha1*xmax,
         lwd=3, col="blue", code=3)
		 
```

The red arrow is a vector that contains information about both Variable 1 and Variable 2. It is tricky, but not too hard, to calculate each observation’s deviations from the mean of that vector (left panel below). We can also calculate the deviations along a perpendicular vector, because even after describing the variation in the “red” direction, there is still some variation in the “blue” direction (right panel).

```{r, echo=FALSE, fig.width=10, fig.height=5}
## for each point, need dy, height between PC1 and point
dy <- y-(beta0 + beta1 * x)

## each point intersects with a line perpendicular to PC1:
## y = alpha0 + alpha1*x = (dy + beta0) + beta1 * x
## let slope of perpendicular line = alpha1 = -1/beta1
alpha1 <- -1/beta1
## let intercept of perpendicular line = alpha0
alpha0 <- (dy+beta0) + beta1*x - alpha1*x

# the lines defined by alpha0 and alpha1 intersect with PC1 at
# x coordinates intx and y coordinates inty
intx <- (beta0-alpha0)/(alpha1-beta1) 
inty <- beta0 + beta1*intx

# intercepts for line parallel to PC1 (intersect PC1)
b0.vec <- dy + beta0

# points where parallel lines and PC2 intercept
## intercept of PC2
a0dot <- (beta0+beta1*x)/(alpha1*x)


###for(i in 1:n){abline(b0.vec[i], beta1, col="green")}
## x coordinates of intersections
xint1 <- (alpha0-b0.vec)/(beta1-alpha1)
yint1 <- alpha0 + alpha1*xint1
###points(xint, yint, pch=17, col="purple")

# add variation along PC2
## intercepts for lines passing though each point
a0.vec <- y-alpha1*x
###for(i in 1:n){abline(a0.vec[i], alpha1, col="green")}

## x coordinates of intersections
xint2 <- (beta0-a0.vec)/(alpha1-beta1)
yint2 <- beta0+beta1*xint2


# put it all together
dy <- y-(beta0 + beta1 * x)
alpha1 <- -1/beta1
alpha0 <- (dy+beta0) + beta1*x - alpha1*x
intx <- (beta0-alpha0)/(alpha1-beta1) 
inty <- beta0 + beta1*intx

intx2 <- (dy+beta0)/(alpha1-beta1)
inty2 <- alpha1*intx2

sum(((x-intx)^2) + ((y-inty)^2))
sum(((x-intx2)^2) + ((y-inty2)^2))

xred <- c(-2, 2)
yred <- c(-3, 3)

use.cex <- 0.9
par(mfrow=c(1,2), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, bty="n",
    cex.lab=1.1, cex.axis=1.1)
	plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(xred[1], xred[2]), ylim=c(yred[1], yred[2]),
    type="n")
arrows(xred[1], beta0+beta1*(xred[1]), 
    xred[2], beta0+beta1*xred[2], 
    lwd=3, col="red", code=3)
arrows(-1.5, alpha1*(-1.5),
    1.5, alpha1*1.5,
    lwd=3, col="blue", code=3)
segments(x, y, intx2, inty2, lwd=2, col="red", lty=2)
points(x, y, pch=16, cex=1.2)
text(xred[1], -2.8, "Variation in red direction", col="red", adj=0, cex=use.cex)
text(xred[2], -2.8,
    expression(sum((italic(red[i])-bar(italic(red)))^2, italic(i)==1, italic(n))==76.6),
    adj=1, col="red", cex=use.cex)

plot(x, y, xlab="Variable 1", ylab="Variable 2",
    xlim=c(xred[1], xred[2]), ylim=c(yred[1], yred[2]),
    type="n")
arrows(xred[1], beta0+beta1*(xred[1]), 
    xred[2], beta0+beta1*xred[2], 
    lwd=3, col="red", code=3)
arrows(-1.5, alpha1*(-1.5),
    1.5, alpha1*1.5,
    lwd=3, col="blue", code=3)
segments(x, y, intx, inty, lwd=2, col="blue", lty=2)
points(x, y, pch=16, cex=1.2)
text(xred[1], -2.8, "Variation in blue direction", col="blue", adj=0, cex=use.cex)
text(xred[2], -2.8,
    expression(sum((italic(blue[i])-bar(italic(blue)))^2, italic(i)==1, italic(n))==21.4),
    adj=1, col="blue", cex=use.cex)
```

We know that the variation on the red and blue axes is the same as the variation on the *X* and *Y* axes, because the sums of squares are the same. The red and blue axes are just different ways of orienting the data to express the same patterns. All we really did was **rotate** the original coordinate system (defined by Variables 1 and 2) to a new coordinate system (defined by red and blue).

You may have guessed that the red and blue axes have special names: they are the **principal components** of this dataset. Principal components (PCs) are **synthetic** or **composite axes** that capture most of the variation. A PC is a **linear combination** of each of the original variables. This is easy to see in the figure above, because PC1 is defined by its coordinates. The same logic is true with more variables; it’s just harder to visualize. 

## PCA by hand

The procedure above illustrates the geometric interpretation of PCA. The general procedure is:

1. Begin with the samples as a cloud of n points in a p-dimensional space.
2. Center (and usually scale) the axes in the point cloud. This will place the origin of the new coordinate system at the p-dimensional centroid of the cloud.
3. Rotate the axes to maximize the variance along the axes. As the angle of rotation $\theta$ changes, the variance $\sigma^{2}$ will also change.
4. Continue to rotate the axes until the variance along each axis is maximized. Because the data are changed only by rotation, the Euclidean distances between them are preserved.

The other way PCA can be understood is in the language of linear algebra. The procedure is roughly given by:

1.	Calculate the variance-covariance matrix **S** of the data matrix **A**. This is a *p* $\times$ *p* square matrix with the variances of each variable on the diagonal, and covariances between pairs of variables in the upper and lower triangles.
2.	Find the eigenvalues $\lambda$ of **S**. Each eigenvalue represents a portion of the original total variance--the proportion corresponding to a particular principal component.
3.	Find the eigenvectors. For each eigenvalue $\lambda$, there is an eigenvector that contains the coefficients of the linear equation for that principal component. Together the eigenvectors form a *p* $\times$ *p* matrix, **Y**.
4.	Find the scores for the original samples on each principal component as $\textbf{X} = \textbf{AY}$. 
The linear algebra method of PCA in R is illustrated below.

```{r}
# generate some random data for PCA
set.seed(123)
n <- 50
x <- runif(n, 1, 20)
y <- 1.2 + 1.7*x + rnorm(n, 0, 15)

# produces data with a linear relationship between x and y
# data matrix: n rows * p columns
A <- cbind(x,y)

# standardize each variable
A <- apply(A, 2, scale)

# calculate variance-covariance matrix S
S <- cov(A)
```

The variance-covariance matrix **S** contains the variances of the variables on the diagonal. Both variances are 1 because we scaled the variables (compare to `cov(A)` to see the difference). This matrix **S** is symmetric because the covariance function is reflexive; i.e., Cov(x,y) = Cov(y,x). The variance-covariance matrix is useful because it contains information about both the spread (variance) and orientation (covariance) in the data. For a dataset like ours with 2 variables, the variance-covariance matrix has 2 dimensions (one for each variable).

$$ \textbf{S}= \begin{bmatrix}Var(x) & Cov(x,y) \\ Cov(y,x) & Var(y) \end{bmatrix} $$

```{r}
# calculate eigenvalues and eigenvectors
eigens <- eigen(S)
evals <- eigens$values
evecs <- eigens$vectors
```

PCA is all about defining a new coordinate system for the data that preserves Euclidean distances, but maximizes the variance captured on the axes of the new system. The axes of the new system are found as linear combinations of the original variables. This means that the new coordinate system will have as many dimensions as the original coordinate system. 

The data in our example can be thought of as a matrix with two columns; each sample is defined by a vector of length two (one for each dimension). That vector simply contains the *x* and *y* coordinates of the sample. If the dataset is matrix **X**, then each point is a vector $v_{i}$ where *i* = 1, 2, …, *n* (*n* = number of samples). For example, the vector [1, 2] corresponds to an observation with *x* = 1 and *y* = 2 (this vector is also a row of **X**). 

To transform the points in the original coordinate system to a new coordinate system, we multiply each vector $v_{i}$ by a *p* $\times$ *p* transformation matrix  **T** (recall that *p* is the number of dimensions) to get the transformed coordinates $b_{i}$.

$$\textbf{T}v_{i}=b_{i}$$

Or written out fully:

$$\begin{bmatrix} \textbf{T}_{1,1} & \textbf{T}_{1,2} \\ \textbf{T}_{2,1} & \textbf{T}_{2,2} \end{bmatrix} \begin{bmatrix} x_{i} \\ y_{i} \end{bmatrix} =\begin{bmatrix}\textbf{T}_{1,1}x_{i} & \textbf{T}_{1,2}y_{i} \\ \textbf{T}_{2,1}x_{i} & \textbf{T}_{2,2}y_{i}\end{bmatrix}$$

It turns out that some vectors $v_{i}$ have a very interesting property: when transformed by **T**, they change length but not direction. These vectors are called **eigenvectors**. The **scalar** (aka: constant) that defines the change in their length (aka: magnitude) is called an **eigenvalue**. This property is expressed compactly as:

$$\textbf{T}v_{i}=\lambda b_{i} $$


The interpretation of this expression is that transforming a coordinate $v_{i}$ into a new coordinate system with matrix **T** is the same as multiplying $v_{i}$ by the eigenvalue $\lambda$. If the eigenvectors are collected into a new matrix **V**, and the eigenvalues are collected into a vector **L**, then

$$\textbf{S}\textbf{V}=\textbf{L}\textbf{V} $$

Put in terms of our 2-d example,

$$\begin{bmatrix}Var(x) & Cov(x,y) \\ Cov(y,x) & Var(y) \end{bmatrix} \begin{bmatrix} v_{1}\\v_{2} \end{bmatrix}=
\begin{bmatrix}\lambda_{1}\\\lambda_{2} \end{bmatrix}
\begin{bmatrix}v_{1}\\v_{2} \end{bmatrix} $$

Once the terms are solved (by the computer!), we can put the eigenvectors in descending order of their eigenvalues to get the principal components. 

```{r}
# calculate scores by matrix operations
pc <- A %*% evecs

# check that variances = eigenvalues and 
# covariances = 0 (must be, by definition of PCA)
## close enough (<1e-16), probably a numerical limit involved
cov(pc)

# variance explained by each PC
round(evals/sum(evals),3)
```

The result shows us that about 81% of the variation is explained by PC1 alone, with the remainder explained by PC2. Notice that all of the variation in the dataset is associated with 1 and only 1 PC. This is part of the definition of PCA--the total variation among the samples does not change. All that changes is how we describe the variation (i.e., how we orient the axes used to describe the samples).

Why go through all of this trouble? Besides the benefit of understanding what PCA is doing, we can use the linear algebra to translate between the coordinate system of the original data, and the coordinates system defined by the PCs. The algebra is complicated, but it boils down to multiplying the matrix of PC scores by the transpose of the eigenvector matrix:

```{r}
backxy <- pc %*% t(evecs)
range(A-backxy)
```

The “back-calculated” coordinates are off by at most $4.5\times10^{-16}$…the differences should technically be 0, but they are approximated to a very small number by R.

## PCA in R (package `vegan`)

There are two methods for PCA in base R: `prcomp()` and `princomp()`. Both of these methods produce similar outputs, and the only major difference between them is the syntax for extracting results. For this course we are going to use the ordination functions in package `vegan`. Although designed for community ecology, `vegan` is widely used for ordination and multivariate analysis in many fields . Importantly, `vegan` is actively maintained and updated with new techniques as they are developed. Also importantly, `vegan` offers a common interface for many kinds of ordination.

The example below uses the bat brain dataset that we have used before. Load the dataset into R and take a look. @hutcheon2002 reported data on brain morphology and lifestyle from 63 species of bats. Their dataset contains the following variables:

|Variable|Meaning|
|----|----|
|`species`|	Species|
|`family`|Taxonomic family|
|`diet`|Herbivore, gleaner, hawker, or vampire|
|`bow`|Body weight (g)|
|`brw`|Brain weight ($\mu$g)|
|`aud`|Auditory nuclei volume (mm^3^)|
|`mob`|Main olfactory bulb volume (mm^3^)|
|`hip`|Hippcampus volume (mm^3^)|

Import the dataset `bat_data_example.csv`. You can download it [here](https://greenquanteco.github.io/bat_data_example.csv
). The code below requires that you have the file in your R working directory.

```{r}
library(vegan)
library(rgl)
in.name <- "bat_data_example.csv"
dat <- read.csv(in.name, header=TRUE)
```

We are going to explore how brain morphology, measured as volumes of various brain parts, varies with taxonomy and lifestyle. First, we need to make sure that our data are suitable for PCA. We can inspect distributions of the variables with histograms.

```{r, fig.width=8, fig.height=4}
pc.cols <- 4:ncol(dat)
par(mfrow=c(2,3))
for(i in 1:length(pc.cols)){hist(dat[,pc.cols[i]])}
```

The histograms suggest that a log transform would be appropriate, because PCA requires multivariate normality.

```{r, fig.width=8, fig.height=4}
dat2 <- dat
dat2[,pc.cols] <- apply(dat2[,pc.cols], 2, log)
par(mfrow=c(2,3))
for(i in 1:length(pc.cols)){
    hist(dat2[,pc.cols[i]],
        main=names(dat2)[pc.cols[i]])
}
```

Much better. We should also check to see if any of the variables are related to each other. It’s okay if they are, but such relationships need to be kept in mind when interpreting the PCA (or any ordination, for that matter). To explore relationships between the variables, we will use a `pairs()` plot and a function borrowed from the `pairs()` help page.

```{r, fig.width=8, fig.height=8}
# borrowed from ?pairs examples
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

# make our plot:
pairs(dat2[,pc.cols],
    lower.panel = panel.smooth,
    upper.panel = panel.cor,
    gap=0)
```

The scatterplot matrix suggests that every variable is closely related to every other variable. This makes sense for morphological data: the sizes of different body parts tend to scale with overall body size. We are interested in brain morphology independent of size, we can factor out size by dividing the volumes of each brain part by the body weight. Because the data are already on the log scale, the division is accomplished by subtracting the log-transformed body weight. The new values of `brw`, `aud`, `mob`, and `hip` are the sizes of those brain components with body size factored out. Neglecting to factor out body size or overall size prior to an ordination will result in an ordination dominated by body size.

```{r, fig.width=8, fig.height=8}
dat3 <- dat2
dat3[,pc.cols[-1]] <- apply(dat3[,pc.cols], 1,
    function(x){x[-1]-x[1]})
pairs(dat3[,pc.cols],
    lower.panel = panel.smooth,
    upper.panel = panel.cor,
    gap=0)
```

Next, use the function `rda()` from `vegan` to calculate the PCA. In this PCA we scale the variables (aka: standardize) by subtracting the mean and dividing by the SD. This has the effect of making each variable have mean = 0 and SD = 1. Scaling is not required for PCA but is ***highly*** recommended. If values are not scaled, then the PCA will be dominated by the variables with the greatest values or by the observations with extreme values.

```{r}
p1 <- rda(dat3[,pc.cols], scale=TRUE)
summary(p1)
```

The summary of the output contains most of what we need. Of greatest interest are the relative contributions of each PC. That is, how much of the overall variation is associated with each PC.

The overall variation is the sum of the variances of the original variables. The PCA was fit using centered and scaled values, so we need to scale the original data to see the variances that were used in the PCA. The scaling meant that each variable ended up with mean = 0 and variance = 1, so the total variance was equal to 5.

```{r}
apply(scale(dat3[,pc.cols]), 2, var)
```
 
That total variance of 5 was partitioned into 5 PCs. In PCA, data are ordinated on one PC for each of the original variables. Each PC is a combination of the original variables. The summary table for the PCA shows how the total variance was split up. 

- The first row, `Eigenvalue`, is the variance associated with each PC. The sum of the eigenvalues equals the total variance, 5. 
- The `Proportion Explained` row shows the proportion of total variance captured on each PC. PC1 captures 29.8%, PC2 captured 26.9%, and so on. For example, the proportion of variance explained by PC1, 0.2977, is equal to the eigenvalue of PC1 divided by the total variance (1.4884 / 5).
- The `Cumulative Proportion` row is the running total of proportion of variance explained, starting with PC1. The rule of thumb for PCA is that you should present and interpret enough PCs to capture ≥80% of the variation. In this example, it takes 3 PCs to get up to about 80% of the variation explained). Two PCs is easier to deal with, but sometimes you need 3. 

```{r}
summary(p1)$cont$importance
```

An alternative strategy for deciding how many PCs to interpret is to look at a screeplot, which shows the relative contributions of each PC to the overall variance. The variance is expressed as “Inertia”--the eigenvalues of the PCs. The proportion explained by each axis (seen in the table above), is simply the eigenvalues of the axes divided by the total of all eigenvalues. Some people prefer to present a screeplot that shows proportion of variance explained rather than the eigenvalues.

```{r, fig.width=6, fig.height=6}
par(mfrow=c(1,2))
screeplot(p1, main="Eigenvalues")

# alternative version with proportion of variance explained
# instead of eigenvalues (variances)
prx <- 100*summary(p1)$cont$importance[2,]
barplot(prx, ylim=c(0, 30),
    main="%Variance",
    ylab="Proportion of variance explained")
```

The **loadings** of the variables express how much each variable is associated with each PC. These values have two interpretations:

- First, they are the correlations between the variables and the PCs. 
- Second, they are the coordinates for the biplot vectors (see below), which help us see the relationships between the ordination and the variables. 
  - Note: the biplot vectors implied by these coordinates are sometimes rescaled to more faithfully represent the relationships between variables. See @legendre2012numerical for a thorough explanation.

```{r}
scores(p1, choices = 1:4, display = "species", scaling = 0)
```

PC1 is most strongly correlated with `aud` (*r* = -0.55) and `hip` (*r* = 0.52). PC2 is most strongly correlated with `mob` (*r* = 0.59) and `brw` (*r* = -0.55). Ideally, each PC would have a few variables strongly correlated with it (*r* > 0.7), but that isn’t the case here. Interestingly, most of the variables are moderately correlated with first three axes. This suggests that none of the variables is strongly driving any of the PCs. We can check this with a biplot.

An ordination **biplot** is probably the most important tool for interpreting the relationships in the data captured by the ordination. It is called a biplot because it presents two kinds of data: *similarity between the samples* indicated by proximity in the ordination space; and *relationships between some set of quantitative variables and the ordination axes*. The samples are plotted as points; the variables are plotted as vectors radiating from the origin.

```{r, fig.width=6, fig.height=6} 
par(mfrow=c(1,1))
biplot(p1)
```

The biplot shows the points (by row name/number) and the five numeric variables that went into the PCA. Each vector shows the projection of a variable into the ordination space. 

- The direction of a vector shows the direction (in ordination space) in which a variable increases. For example, samples in the upper left have increased aud; samples in the upper right have increased mob, and so on. A variable decreases in the direction opposite its vector: samples in the lower right have decreased aud.
  - Compare the variable loadings with the biplot arrows. Do these values make sense?
- Relative angles of vectors reflect the correlation between the underlying variables.
  - Variables whose vectors point in the same direction are positively correlated with each other; the smaller the angle between two vectors, the stronger the correlation (*r* approaching 1).
  - Vectors perpendicular to each other are uncorrelated (*r* close to 0).
  - Vectors pointing in opposite directions are negatively correlated with each other (*r* approaching -1).
  - Correlation coefficients cannot be inferred directly from angles because of how the coordinates on the plot are scaled, but the angles do give a *rough* idea.
- The length of a vector indicates the strength of the correlation with the ordination space. Longer vectors indicate stronger correlations (|*r* |). 
- Each variable vectors represents an axis of the original coordinate system.
  - Shorter vectors have most of their length off the plane of the biplot.
  - Longer vectors have more of their length near the plane of the biplot.
  - The biplot is really a plane within the original data coordinate system defined by the PCs.

Because it took 3 PCs to get up to about 80% of the variation, we should present and interpret the first 3 PCs. We can plot other variables with the `biplot()` command.

```{r, fig.width=8, fig.height=8}
par(mfrow=c(2,2))
biplot(p1, choices=c(1,2))
biplot(p1, choices=c(1,3))
biplot(p1, choices=c(2,3))
```

Even better, we can make a 3-d plot using the `rgl` package. Note that the code block below was not run to make this page; try running it on your machine. The plots made by `plot3d()` are cool because you can rotate them with the mouse. Exporting `rgl` figures to static image formats like `.jpg` can be tricky because you must specify the rotation angles, which can pretty much only be done by trial and error.

```{r, eval=FALSE}
library(rgl)
par(mfrow=c(1,1))
px <- scores(p1, choices=1:3)$sites
plot3d(px[,1], px[,2], px[,3],
    xlab="PC1", ylab="PC2", zlab="PC3")
```

![Screen capture of a 3-d plot produced by `rgl::plot3d()`](C:/Users/ngreen62/OneDrive - Kennesaw State University/biol 6490/_stat/fig/08_14.jpg)


We can add more information to the biplots to help us make sense of the data. Let’s color-code the diets.

```{r, eval=FALSE}
diets <- sort(unique(dat3$diet))
cols <- rainbow(length(diets))
use.cols <- cols[match(dat3$diet, diets)]

par(mfrow=c(1,1))
px <- scores(p1, choices=1:3)$sites
vx <- scores(p1, choices=1:3)$species
plot3d(px[,1], px[,2], px[,3],
    xlab="PC1", ylab="PC2", zlab="PC3",
    col=use.cols, size=50)
for(i in 1:nrow(vx)){
    segments3d(c(0, vx[i,1]), c(0, vx[i,2]), c(0, vx[i,3]), col="red")
    text3d(vx[i,1], vx[i,2], vx[i,3], rownames(vx)[i], col="red")
}
```

![Screen capture of an improved 3-d plot produced by `rgl::plot3d()`](C:/Users/ngreen62/OneDrive - Kennesaw State University/biol 6490/_stat/fig/08_15.jpg)

The `biplot()` command in `vegan` isn’t very flexible, so we if we want a nicer-looking plot we will need to construct it manually.

```{r, fig.width=9, fig.height=9}
px <- scores(p1, choices=1:3)$sites
vx <- scores(p1, choices=1:3)$species

diets <- sort(unique(dat3$diet))
cols <- rainbow(length(diets))
use.cols <- cols[match(dat3$diet, diets)]

par(mfrow=c(2,2))
plot(px[,1], px[,2], pch=16, cex=1.4, col=use.cols,
    xlab="PC1", ylab="PC2", xlim=c(-1.5, 1.5), ylim=c(-1.5, 1.5))
segments(0, 0, vx[,1], vx[,2], col="red")
text(vx[,1], vx[,2], rownames(vx), col="red")
plot(px[,1], px[,3], pch=16, cex=1.4, col=use.cols,
    xlab="PC1", ylab="PC3", xlim=c(-1.5, 1.5), ylim=c(-1.5, 1.5))

segments(0, 0, vx[,1], vx[,3], col="red")
text(vx[,1], vx[,3], rownames(vx), col="red")
plot(px[,2], px[,3], pch=16, cex=1.4, col=use.cols,
    xlab="PC2", ylab="PC3", xlim=c(-1.5, 1.5), ylim=c(-1.5, 1.5))
segments(0, 0, vx[,2], vx[,3], col="red")
text(vx[,2], vx[,3], rownames(vx), col="red")
```

## Application of PCA: PC regression

Because PCs capture information about multiple variables at once, they can be used to represent those variables *in other statistical methods*. For example, a PC that represents many measurements of body parts or tree species composition or gene expression can be used as a predictor variable in a logistic regression or as a response variable. This practice is sometimes called **PC regression**. The example below uses the `iris` dataset to illustrate using principal components of flower morphology to predict species identity.

```{r, fig.width=7, fig.height=7}
library(vegan)
# grab numeric variables
dat <- iris[,1:4]
# standardize each variable
dat <- apply(dat, 2, scale)
# fit PCA with vegan::rda()
p1 <- rda(dat)

# examine output
summary(p1)

# make some colors to label species
use.col <- rainbow(3)[match(iris$Species,levels(iris$Species))]

# extract scores of samples (px) and biplot vectors (vx)
px <- scores(p1, display="sites")
vx <- scores(p1,display="species")
 
# make a plot of the flowers in PCA space
plot(px[,1], px[,2], col=use.col, pch=16,
    xlim=c(-3, 3), ylim=c(-3, 3),
    xlab="PC1", ylab="PC2")
segments(0, 0, vx[,1], vx[,2], col="red")
text(vx[,1], vx[,2], rownames(vx), col="red")
legend("topleft", legend=levels(iris$Species),
    pch=16, col=rainbow(3))
```

The ordination reveals that 73% of variation is explained by PC1. The figure shows that the species fall out very cleanly along PC1, with is associated with petal morphology. Let’s use PC1 as a predictor for species in a logistic regression. 

```{r}
dat2 <- data.frame(y=iris$Species, pc1=px[,1])
dat2$z <- ifelse(dat2$y == "virginica", 1, 0)

mod1 <- glm(z~pc1, data=dat2, family=binomial)
summary(mod1)
```

The logistic regression results suggest that for every unit increase in PC1, the odds ratio of a flower being *Iris virginica* increases by 23. That’s a very strong signal. Just for fun, below are the model predictions of probability of being *I*. *virginica* and the ROC curve. Both confirm visually what the coefficients table above suggested, that PC1 is a very reliable predictor of *Iris* species (at least in this dataset).

```{r, fig.width=7, fig.height=7}
n <- 50
prx <- seq(min(dat2$pc1), max(dat2$pc1), length=n)
dx <- data.frame(pc1=prx)
pred <- predict(mod1, newdata=data.frame(dx),
    type="link", se.fit=TRUE)

mn <- plogis(pred$fit)
ll <- plogis(qnorm(0.025, pred$fit, pred$se.fit))
uu <- plogis(qnorm(0.975, pred$fit, pred$se.fit))

par(mfrow=c(1,1), mar=c(5.1, 5.1, 1.1, 1.1),
    lend=1, las=1, cex.axis=1.3, cex.lab=1.3,
    bty="n")
plot(prx, mn, type="n", xlab="PC1",
    ylab=expression(P(italic(virginica))),
    ylim=c(-0.1,1.1))
points(prx, ll, type="l", lty=2)
points(prx, uu, type="l", lty=2)
points(prx, mn, type="l", lwd=2)
points(dat2$pc1, jitter(dat2$z, amount=0.05), xpd=NA)

library(pROC)
p1 <- predict(mod1, type="response")
roc1 <- roc(dat2$z ~ p1, plot = TRUE, print.auc = TRUE)
roc1
```

Going the other way, modeling PC scores in response to predictor variables, is tricky but it can be done. Usually this is only acceptable when there is a ***clear*** relationship between the modeled PC and several of the original variables that went into the PCA. Treating a PC as a *dependent variable* is an elegant way to get around the problem of having multiple collinear response variables. Conversely, treating a PC as a predictor variable is a way of dealing with multiple collinear predictor variables. Axes from other ordination techniques can be used in this manner, but require ***careful*** biological and statistical justification as well as very cautious interpretation.

[**Go back to main page**](https://greenquanteco.github.io/index.html)

# References

<div id="refs"></div>

# Legal notice

This site is for educational purposes only. This work and its content is released under the [Creative Commons Attribution-ShareAlike 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license. Inclusion of third-party data falls under guidelines of fair use as defined in [section 107 of the US Copyright Act of 1976](https://www.law.cornell.edu/uscode/text/17/107). 